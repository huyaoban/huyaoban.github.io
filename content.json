{"meta":{"title":"胡尔摩斯","subtitle":"胡尔摩斯","description":"不会写诗的程序员不是一个合格的码农","author":"胡尔摩斯","url":"http://huermosi.xyz","root":"/"},"pages":[{"title":"联系我","date":"2020-01-04T13:00:33.000Z","updated":"2021-04-01T07:00:06.030Z","comments":true,"path":"about/index.html","permalink":"http://huermosi.xyz/about/index.html","excerpt":"","text":""}],"posts":[{"title":"SqlSessionTemplate是如何保证MyBatis中SqlSession的线程安全的","slug":"mybatis/mybatis-sqlsession-01","date":"2021-04-15T09:43:20.000Z","updated":"2021-04-26T12:40:52.021Z","comments":true,"path":"2021/9527888015/","link":"","permalink":"http://huermosi.xyz/2021/9527888015/","excerpt":"","text":"一、DefaultSqlSession的线程不安全性在MyBatis架构中SqlSession是提供给外层调用的顶层接口，实现类有：DefaultSqlSession、SqlSessionManager以及mybatis-spring提供的实现SqlSessionTemplate。默认的实现类为DefaultSqlSession如。类图结构如下所示： 对于MyBatis提供的原生实现类来说，用的最多的就是DefaultSqlSession，但我们知道DefaultSqlSession这个类不是线程安全的！如下： 二、SqlSessionTemplate是如何使用DefaultSqlSession的而在我们开发的时候肯定会用到Spring，也会用到mybatis-spring框架，在使用MyBatis与Spring集成的时候我们会用到了SqlSessionTemplate这个类，例如下边的配置，注入一个单例的SqlSessionTemplate对象： SqlSessionTemplate的源代码注释如下： 通过源码我们何以看到 SqlSessionTemplate实现了SqlSession接口，也就是说我们可以使用SqlSessionTemplate来代理以往的DefaultSqlSession完成对数据库的操作，但是DefaultSqlSession这个类不是线程安全的，所以DefaultSqlSession这个类不可以被设置成单例模式的。 如果是常规开发模式的话，我们每次在使用DefaultSqlSession的时候都从SqlSessionFactory当中获取一个就可以了。但是与Spring集成以后，Spring提供了一个全局唯一的SqlSessionTemplate对象来完成DefaultSqlSession的功能，问题就是：无论是多个Dao使用一个SqlSessionTemplate，还是一个Dao使用一个SqlSessionTemplate，SqlSessionTemplate都是对应一个sqlSession对象，当多个web线程调用同一个Dao时，它们使用的是同一个SqlSessionTemplate，也就是同一个SqlSession，那么它是如何确保线程安全的呢？让我们一起来分析一下： 三、SqlSessionTemplate是如何保证DefaultSqlSession线程安全的（1）首先，通过如下代码创建代理类，表示创建SqlSessionFactory的代理类的实例，该代理类实现SqlSession接口，定义了方法拦截器，如果调用代理类实例中实现SqlSession接口定义的方法，该调用则被导向SqlSessionInterceptor的invoke方法（代理对象的InvocationHandler就是SqlSessionInterceptor，如果把它命名为SqlSessionInvocationHandler则更好理解！） 核心代码就在 SqlSessionInterceptor的invoke方法当中。 在上面的invoke方法当中使用了两个工具方法分别是： 那么这两个方法又是如何与Spring的事物进行关联的呢？ 1、getSqlSession方法如下： 2、closeSqlSession方法如下： 大致的分析到此为止，可能有些许不够顺畅，不过：纸上得来终觉浅，绝知此事要躬行！还希望小伙伴打开自己的编译器，找到此处的代码，认真走一遍流程！ 其实通过上面的代码我们可以看出Mybatis在很多地方都用到了代理模式，代理模式可以说是一种经典模式，其实不紧紧在这个地方用到了代理模式，Spring的事物、AOP、Mybatis数据库连接池技术、MyBatis的核心原理（如何在只有接口没有实现类的情况下完成数据库的操作！）等技术都使用了代理技术。 四、SqlSessionManager又是什么鬼？上述说了SqlSession的实现还有一个SqlSessionManager，那么SqlSessionManager到底是什么个东西哪？且看定义如下： 你可能会发现SqlSessionManager的构造方法竟然是private的，那我们怎么创建这个对象哪？其实SqlSessionManager创建对象是通过newInstance的方法创建对象的，但需要注意的是他虽然有私有的构造方法，并且提供给我们了一个公有的newInstance方法，但它并不是一个单例模式！ newInstance有很多重载的方法，如下所示： SqlSessionManager的openSession方法及其重载的方法是直接通过调用其中底层封装的SqlSessionFactory对象的openSession方法来创建SqlSession对象的，重载方法如下： SqlSessionManager中实现了SqlSession接口中的方法，例如：select、update等，都是直接调用sqlSessionProxy代理对象中相应的方法。在创建该代理对像的时候使用的InvocationHandler对象是SqlSessionInterceptor，他是定义在SqlSessionManager的一个内部类，其定义如下： 五、总结综上所述，我们应该大致了解了DefaultSqlSession和SqlSessionManager之间的区别： 1、DefaultSqlSession的内部没有提供像SqlSessionManager一样通过ThreadLocal的方式来保证线程的安全性； 2、SqlSessionManager是通过localSqlSession这个ThreadLocal变量，记录与当前线程绑定的SqlSession对象，供当前线程循环使用，从而避免在同一个线程多次创建SqlSession对象造成的性能损耗； 3、DefaultSqlSession不是线程安全的，我们在进行原生开发的时候，需要每次为一个操作都创建一个SqlSession对象，其性能可想而知； 在上面的invoke方法当中使用了两个工具方法分别是： 那么这两个方法又是如何与Spring的事物进行关联的呢？ 1、getSqlSession方法如下： 2、closeSqlSession方法如下： 大致的分析到此为止，可能有些许不够顺畅，不过：纸上得来终觉浅，绝知此事要躬行！还希望小伙伴打开自己的编译器，找到此处的代码，认真走一遍流程！ 其实通过上面的代码我们可以看出Mybatis在很多地方都用到了代理模式，代理模式可以说是一种经典模式，其实不紧紧在这个地方用到了代理模式，Spring的事物、AOP、Mybatis数据库连接池技术、MyBatis的核心原理（如何在只有接口没有实现类的情况下完成数据库的操作！）等技术都使用了代理技术。 四、SqlSessionManager又是什么鬼？上述说了SqlSession的实现还有一个SqlSessionManager，那么SqlSessionManager到底是什么个东西哪？且看定义如下： 你可能会发现SqlSessionManager的构造方法竟然是private的，那我们怎么创建这个对象哪？其实SqlSessionManager创建对象是通过newInstance的方法创建对象的，但需要注意的是他虽然有私有的构造方法，并且提供给我们了一个公有的newInstance方法，但它并不是一个单例模式！ newInstance有很多重载的方法，如下所示： SqlSessionManager的openSession方法及其重载的方法是直接通过调用其中底层封装的SqlSessionFactory对象的openSession方法来创建SqlSession对象的，重载方法如下： SqlSessionManager中实现了SqlSession接口中的方法，例如：select、update等，都是直接调用sqlSessionProxy代理对象中相应的方法。在创建该代理对像的时候使用的InvocationHandler对象是SqlSessionInterceptor，他是定义在SqlSessionManager的一个内部类，其定义如下： 五、总结综上所述，我们应该大致了解了DefaultSqlSession和SqlSessionManager之间的区别： 1、DefaultSqlSession的内部没有提供像SqlSessionManager一样通过ThreadLocal的方式来保证线程的安全性； 2、SqlSessionManager是通过localSqlSession这个ThreadLocal变量，记录与当前线程绑定的SqlSession对象，供当前线程循环使用，从而避免在同一个线程多次创建SqlSession对象造成的性能损耗； 3、DefaultSqlSession不是线程安全的，我们在进行原生开发的时候，需要每次为一个操作都创建一个SqlSession对象，其性能可想而知； 本文转载自灯塔大数据头条，原文连接点这里，版权归原作者所有，转载目的是为了避免以后找不到。","categories":[{"name":"MYBATIS","slug":"MYBATIS","permalink":"http://huermosi.xyz/categories/MYBATIS/"}],"tags":[{"name":"MYBATIS","slug":"MYBATIS","permalink":"http://huermosi.xyz/tags/MYBATIS/"},{"name":"SQLSESSION","slug":"SQLSESSION","permalink":"http://huermosi.xyz/tags/SQLSESSION/"}]},{"title":"有点狠有点猛，我用责任链模式重构了业务代码","slug":"design-pattern/design-pattern-chain","date":"2021-04-07T13:28:07.000Z","updated":"2021-04-08T08:49:31.516Z","comments":true,"path":"2021/9527202104222212000/","link":"","permalink":"http://huermosi.xyz/2021/9527202104222212000/","excerpt":"","text":"1. 前言文章开篇，抛出一个老生常谈的问题，学习设计模式有什么作用？ 设计模式主要是为了应对代码的复杂性，让其满足开闭原则，提高代码的扩展性 另外，学习的设计模式 一定要在业务代码中落实，只有理论没有真正实施，是无法真正掌握并且灵活运用设计模式的 这篇文章主要说 责任链设计模式，认识此模式是在读 Mybatis 源码时， Interceptor 拦截器主要使用的就是责任链，当时读过后就留下了很深的印象（内心 OS：还能这样玩） 文章先从基础概念说起，另外分析一波 Mybatis 源码中是如何运用的，最后按照 “习俗”，设计一个真实业务场景上的应用 责任链设计模式大纲如下：什么是责任链模式 完成真实的责任链业务场景设计 Mybatis Interceptor 底层实现 责任链模式总结 2. 什么是责任链模式举个例子，SpringMvc 中可以定义拦截器，并且可以定义多个。当一个用户发起请求时，顺利的话请求会经过所有拦截器，最终到达业务代码逻辑，SpringMvc 拦截器设计就是使用了责任链模式 为什么说顺利的话会经过所有拦截器？因为请求不满足拦截器自定义规则会被打回，但这并不是责任链模式的唯一处理方式，继续往下看 在责任链模式中，多个处理器（参照上述拦截器）依次处理同一个请求。一个请求先经过 A 处理器处理，然后再把请求传递给 B 处理器，B 处理器处理完后再传递给 C 处理器，以此类推，形成一个链条，链条上的每个处理器 各自承担各自的处理职责 责任链模式中多个处理器形成的处理器链在进行处理请求时，有两种处理方式： 请求会被 所有的处理器都处理一遍，不存在中途终止的情况，这里参照 MyBatis 拦截器理解 二则是处理器链执行请求中，某一处理器执行时，如果不符合自制定规则的话，停止流程，并且剩下未执行处理器就不会被执行，大家参照 SpringMvc 拦截器理解 这里通过代码的形式对两种处理方式作出解答，方便读者更好的理解。首先看下第一种，请求会经过所有处理器执行的情况 IHandler 负责抽象处理器行为，handle() 则是不同处理器具体需要执行的方法，HandleA、HandleB 为具体需要执行的处理器类，HandlerChain 则是将处理器串成一条链执行的处理器链 123456789101112public class ChainApplication &#123; public static void main(String[] args) &#123; HandlerChain handlerChain = new HandlerChain(); handlerChain.addHandler(Lists.newArrayList(new HandlerA(), new HandlerB())); handlerChain.handle(); /** * 程序执行结果： * HandlerA打印：执行 HandlerA * HandlerB打印：执行 HandlerB */ &#125;&#125; 这种责任链执行方式会将所有的 处理器全部执行一遍，不会被打断。Mybatis 拦截器用的正是此类型，这种类型 重点在对请求过程中的数据或者行为进行改变 而另外一种责任链模式实现，则是会对请求有阻断作用，阻断产生的前置条件是在处理器中自定义的，代码中的实现较简单，读者可以联想 SpringMvc 拦截器的实现流程 根据代码看得出来，在每一个 IHandler 实现类中会返回一个布尔类型的返回值，如果返回布尔值为 false，那么责任链发起类会中断流程，剩余处理器将不会被执行。就像我们定义在 SpringMvc 中的 Token 拦截器，如果 Token 失效就不能继续访问系统，处理器将请求打回 12345678910public class ChainApplication &#123; public static void main(String[] args) &#123; HandlerChain handlerChain = new HandlerChain(); handlerChain.addHandler(Lists.newArrayList(new HandlerA(), new HandlerB())); boolean resultFlag = handlerChain.handle(); if (!resultFlag) &#123; System.out.println(\"责任链中处理器不满足条件\"); &#125; &#125;&#125; 读者可以自己在 IDEA 中实现两种不同的责任链模式，对比其中的不同，设想下业务中真实的应用场景，再或者可以跑 SpringBoot 项目，创建多个拦截器来佐证文中的说辞 本章节介绍了责任链设计模式的具体语义，以及不同责任链实现类型代码举例，并以 Mybatis、SpringMvc 拦截器为参照点，介绍各自不同的代码实现以及应用场景 3. 责任链业务场景设计趁热打铁，本小节对使用的真实业务场景进行举例说明。假设业务场景是这样的，我们 系统处在一个下游服务，因为业务需求，系统中所使用的 基础数据需要从上游中台同步到系统数据库 基础数据包含了很多类型数据，虽然数据在中台会有一定验证，但是 数据只要是人为录入就极可能存在问题，遵从对上游系统不信任原则，需要对数据接收时进行一系列校验 最初是要进行一系列验证原则才能入库的，后来因为工期问题只放了一套非空验证，趁着春节期间时间还算宽裕，把这套验证规则骨架放进去 从我们系统的接入数据规则而言，个人觉得需要支持以下几套规则 必填项校验，如果数据无法满足业务所必须字段要求，数据一旦落入库中就会产生一系列问题 非法字符校验，因为数据如何录入，上游系统的录入规则是什么样的我们都不清楚，这一项规则也是必须的 长度校验，理由同上，如果系统某字段长度限制 50，但是接入来的数据 500长度，这也会造成问题 为了让读者了解业务嵌入责任链模式的前因，这里列举了三套校验规则，当然真实中可能不止这三套。但是 一旦将责任链模式嵌入数据同步流程，就会 完全符合文初所提的开闭原则，提高代码的扩展性 本案例设计模式中的开闭原则通过 Spring 提供支持，后续添加新的校验规则就可以不必修改原有代码 这里要再强调下，设计模式的应用场景一定要灵活掌握，只有这样才能在合适的业务场景合理运用对象的设计模式 既然设计模式场景说过了，最后说一下需要达成的业务需求。将一个批量数据经过处理器链的处理，返回出符合要求的数据分类 定义顶级验证接口和一系列处理器实现类没什么难度，但是应该如何进行链式调用呢？ 这一块代码需要有一定 Spring 基础才能理解，一起来看下 VerifyHandlerChain 如何将所有处理器串成一条链 VerifyHandlerChain 处理流程如下： 实现自 InitializingBean 接口，在对应实现方法中获取 IOC 容器中类型为 VerifyHandler 的 Bean，也就是 EmptyVerifyHandler、SexyVerifyHandler 将 VerifyHandler 类型的 Bean 添加到处理器链容器中 定义校验方法 verify()，对入参数据展开处理器链的全部调用，如果过程中发现已无需要验证的数据，直接返回 这里使用 SpringBoot 项目中默认测试类，来测试一下如何调用 123456789101112@SpringBootTestclass ChainApplicationTests &#123; @Autowired private VerifyHandlerChain verifyHandlerChain; @Test void contextLoads() &#123; List&lt;Object&gt; verify = verifyHandlerChain.verify(Lists.newArrayList(\"源码兴趣圈\", \"@龙台\")); System.out.println(verify); &#125;&#125; 这样的话，如果客户或者产品提校验相关的需求时，我们只需要实现 VerifyHandler 接口新建个校验规则实现类就 OK 了，这样符合了设计模式的原则：满足开闭原则，提高代码的扩展性 熟悉之前作者写过设计模式的文章应该知道，强调设计模式重语意，而不是具体的实现过程。所以，你看这个咱们这个校验代码，把责任链两种模式结合了使用 上面的代码只是示例代码，实际业务中的实现要比这复杂很多，比如： 如何定义处理器的先后调用顺序。比如说某一个处理器执行时间很长并且过滤数据很少，所以希望把它放到最后面执行 这是为当前业务的所有数据类型进行过滤，如何自定义单个数据类型过滤。比如你接入学生数据，学号有一定校验规则，这种处理器类肯定只适合单一类型 还有很多的业务场景，所以设计模式强调的应该是一种思想，而不是固定的代码写法，需要结合业务场景灵活变通 4. 责任链模式的好处一定要使用责任链模式么？不使用能不能完成业务需求？ 回答是肯定可以，设计模式只是帮助减少代码的复杂性，让其满足开闭原则，提高代码的扩展性。如果不使用同样可以完成需求 如果不使用责任链模式，上面说的真实同步场景面临两个问题 如果把上述说的代码逻辑校验规则写到一起，毫无疑问这个类或者说这个方法函数奇大无比。减少代码复杂性一贯方法是：将大块代码逻辑拆分成函数，将大类拆分成小类，是应对代码复杂性的常用方法。如果此时说：可以把不同的校验规则拆分成不同的函数，不同的类，这样不也可以满足减少代码复杂性的要求么。这样拆分是能解决代码复杂性，但是这样就会面临第二个问题 开闭原则：添加一个新的功能应该是，在已有代码基础上扩展代码，而非修改已有代码。大家设想一下，假设你写了三套校验规则，运行过一段时间，这时候领导让加第四套，是不是要在原有代码上改动 综上所述，在合适的场景运用适合的设计模式，能够让代码设计复杂性降低，变得更为健壮。朝更远的说也能让自己的编码设计能力有所提高，告别被人吐槽的烂代码… 5. Mybatis Interceptor底层实现上面说了那么多，框架底层源码是怎么设计并且使用责任链模式的？之前在看 Mybatis 3.4.x 源码时了解到 Interceptor 底层实现就是责任链模式，这里和读者分享 Interceptor 具体实现 开门见山，直接把视线聚焦到 Mybatis 源码，版本号 3.4.7-SNAPSHOT 熟悉么？是不是和我们上面用到的责任链模式差不太多，有处理器集合 interceptors，有添加处理器方法 Mybatis Interceptor 不仅用到了责任链，还用到了动态代理，服务于 Mybatis 四大 “护教法王”，在创建对象时通过动态代理和责任链相结合组装而成插件模块 ParameterHandler ResultSetHandler StatementHandler Executor 使用过 Mybatis 的读者应该知道，查询 SQL 的分页语句就是使用 Interceptor 实现，比如市场上的 PageHelper、Mybatis-Plus 分页插件再或者我们自实现的分页插件（应该没有项目组使用显示调用多条语句组成分页吧） 拿查询语句举例，如果定义了多个查询相关的拦截器，会先经过拦截器的代码加工，所有的拦截器执行完毕后才会走真正查询数据库操作 扯的话就扯远了，能够知道如何用、在哪用就可以了。通过 Interceptor 也能知道一点，想要读框架源码，需要一定的设计模式基础。如果对责任链、动态代理不清楚，那么就不能理解这一块的精髓 6. 结言文章通过图文并茂的方式帮助大家理解责任链设计模式，在两种类型示例代码以及举例实际业务场景下，相信小伙伴已经掌握了如何在合适的场景使用责任链设计模式 看完文章后可以结合 Mybatis、SpringMvc 拦截器更深入掌握责任链模式的应用场景以及使用手法。另外可以结合项目中实际业务场景灵活使用，相信真正使用后的你会对责任链模式产生更深入的了解 参考文章 《设计模式之美：职责链模式》 本文转载自代码小当家头条，原文连接点这里，版权归原作者所有，转载目的是为了避免以后找不到。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/tags/JAVA/"},{"name":"设计模式","slug":"设计模式","permalink":"http://huermosi.xyz/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"职责链模式","slug":"职责链模式","permalink":"http://huermosi.xyz/tags/%E8%81%8C%E8%B4%A3%E9%93%BE%E6%A8%A1%E5%BC%8F/"},{"name":"MYBATIS","slug":"MYBATIS","permalink":"http://huermosi.xyz/tags/MYBATIS/"},{"name":"SPRING","slug":"SPRING","permalink":"http://huermosi.xyz/tags/SPRING/"}]},{"title":"SpringBoot国际化实战，实现语言的自由切换","slug":"springboot/springboot-thymeleaf-i18n","date":"2021-04-07T13:02:00.000Z","updated":"2021-04-08T08:49:31.745Z","comments":true,"path":"2021/952720210072154/","link":"","permalink":"http://huermosi.xyz/2021/952720210072154/","excerpt":"","text":"国际化是每个大型公司官网或者技术文档都会有的，比如前端UI库element、阿里云等，本节我们利用thymeleaf来实现国际化操作。 1. 新建项目为了方便后续阅读我们新建模块fw-springboot-international，基本的SpringBoot+thymeleaf+国际化信息(message.properties)项目 2. maven配置添加thymeleaf依赖 123456789101112131415&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.yisu.cloud&lt;/groupId&gt; &lt;artifactId&gt;fw-cloud-common&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 3. 国际化配置设置了一个localeResolver，可以采用Cookie来控制国际化的语言，也可以采用Session来控制，两个启用一个即可。还设置一个LocaleChangeInterceptor拦截器来拦截国际化语言的变化，并且将拦截器加入到Spring中。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * 配置信息 * @Author xuyisu * @Date 2019/12/6 */@Configurationpublic class I18nConfig extends WebMvcConfigurationSupport &#123; /** * session区域解析器 * @return */ @Bean public LocaleResolver localeResolver() &#123; SessionLocaleResolver resolver = new SessionLocaleResolver(); resolver.setDefaultLocale(Locale.CHINA); return resolver; &#125; /** * cookie区域解析器 * @return */// @Bean// public LocaleResolver localeResolver() &#123;// CookieLocaleResolver slr = new CookieLocaleResolver();// //设置默认区域,// slr.setDefaultLocale(Locale.CHINA);// slr.setCookieMaxAge(3600);//设置cookie有效期.// return slr;// &#125; @Bean public LocaleChangeInterceptor localeChangeInterceptor() &#123; LocaleChangeInterceptor lci = new LocaleChangeInterceptor(); // 设置请求地址的参数,默认为：locale// lci.setParamName(LocaleChangeInterceptor.DEFAULT_PARAM_NAME); return lci; &#125; @Override public void addInterceptors(InterceptorRegistry registry) &#123; registry.addInterceptor(localeChangeInterceptor()); &#125;&#125; 4. 控制层对于使用thymeleaf的我们可以直接跳转到页面，使用方式和JSP类似。这里我们设置默认页面就是跳转到index.html 12345678910111213/** * 首页 * @Author xuyisu * @Date 2019/12/6 */@Controllerpublic class IndexController &#123; @GetMapping(\"/\") public String index() &#123; return \"/index\"; &#125;&#125; 5. message 信息中文zh_CN 12345login.userId&#x3D;用户名login.noUserId&#x3D;请输入用户名login.password&#x3D;密码login.noPassword&#x3D;密码不能为空login.login&#x3D;登录 英文en_US 12345login.userId &#x3D; Login IDlogin.noUserId &#x3D; Please enter the user IDlogin.password &#x3D; Passwordlogin.noPassword &#x3D; password can not be blanklogin.login &#x3D; Login 6. 页面模拟一个简单的表单登录 12345678910111213141516171819&lt;!DOCTYPE html&gt;&lt;html xmlns=\"http://www.w3.org/1999/xhtml\" xmlns:th=\"http://www.thymeleaf.org\"&gt;&lt;head&gt; &lt;title&gt;SpringBoot-international&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div&gt; &lt;form th:align=\"center\"&gt; &lt;label th:text=\"#&#123;login.userId&#125;\"&gt;Username&lt;/label&gt; &lt;input type=\"text\" th:placeholder=\"#&#123;login.noUserId&#125;\" required=\"\" autofocus=\"\"&gt; &lt;br&gt; &lt;label th:text=\"#&#123;login.password&#125;\"&gt;Password&lt;/label&gt; &lt;input type=\"password\" th:placeholder=\"#&#123;login.noPassword&#125;\" required=\"\"&gt; &lt;br&gt; &lt;button type=\"submit\" th:text=\"#&#123;login.login&#125;\"&gt;Sign in&lt;/button&gt; &lt;/form&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 7. 应用启动并访问浏览器输入http://localhost:8774/，可以看到如下表单，默认是中文的，所以他默认会去messages_zh_CN.properties中找，如果没有就会去messages.properties中找。 如果输入http://localhost:8774/?locale=en_US语言就会切到英文。同样的如果url后参数设置为locale=zh_CH，语言就会切到中文 8. 前后端分离的情况对于如果不是thymeleaf的环境，而是前后端分离的情况，可以使用如下方式，通过接口设置语言环境，默认中文，然后通过key 获取对应的value值。 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * 设置语言环境 * @Author xuyisu * @Date 2019/12/6 */@RestControllerpublic class LanguageController &#123; @Autowired private MessageUtil messageUtil; /** * 设置语言 * @param request * @param response * @param lang * @return */ @GetMapping(\"/setLang\") public FwResult getInfoByLang(HttpServletRequest request, HttpServletResponse response, String lang)&#123; LocaleResolver localeResolver = RequestContextUtils.getLocaleResolver(request); if(\"zh\".equals(lang))&#123; localeResolver.setLocale(request, response, new Locale(\"zh\", \"CN\")); &#125;else if(\"en\".equals(lang))&#123; localeResolver.setLocale(request, response, new Locale(\"en\", \"US\")); &#125; return FwResult.okMsg(\"设置\"+lang+\"成功\"); &#125; /** * 根据key 获取内容 * @param key * @return */ @GetMapping(\"/getValue\") public FwResult getValue(String key) &#123; String welcome = messageUtil.getMessage(key); return FwResult.ok(welcome); &#125;&#125; 获取message 中的国际化配置信息，这里抽取成一个公共方法 1234567891011121314151617181920212223242526272829303132@Componentpublic class MessageUtil &#123; @Resource private MessageSource messageSource; public String getMessage(String code) &#123; return getMessage(code, null); &#125; /** * * @param code ：对应messages配置的key. * @param args : 数组参数. * @return */ public String getMessage(String code, Object[] args)&#123; return getMessage(code, args, \"\"); &#125; /** * * @param code ：对应messages配置的key. * @param args : 数组参数. * @param defaultMessage : 没有设置key的时候的默认值. * @return */ public String getMessage(String code,Object[] args,String defaultMessage)&#123; //这里使用比较方便的方法，不依赖request. Locale locale = LocaleContextHolder.getLocale(); return messageSource.getMessage(code, args, defaultMessage, locale); &#125;&#125; 9. 启动应用测试浏览器或Postman 输入localhost:8774/getValue?key=login.noUserId 修改语言环境localhost:8774/setLang?lang=en浏览器或Postman 再次输入localhost:8774/getValue?key=login.noUserId 10. 乱码处理如果遇到国际化配置文件中存在乱码的情况可以按照下图将标记的部分勾选即可 本文转载自青锋爱编程头条，原文连接点这里，版权归原作者所有，转载目的是为了避免以后找不到。","categories":[{"name":"SPRING","slug":"SPRING","permalink":"http://huermosi.xyz/categories/SPRING/"},{"name":"SPRINGBOOT","slug":"SPRING/SPRINGBOOT","permalink":"http://huermosi.xyz/categories/SPRING/SPRINGBOOT/"}],"tags":[{"name":"SPRING","slug":"SPRING","permalink":"http://huermosi.xyz/tags/SPRING/"},{"name":"SPRINGBOOT","slug":"SPRINGBOOT","permalink":"http://huermosi.xyz/tags/SPRINGBOOT/"}]},{"title":"MySQL的万字总结（缓存，索引，Explain，事务，redo日志等）","slug":"mysql/mysql-summary-cache-index-explain-redo","date":"2021-01-07T13:45:05.000Z","updated":"2021-04-08T08:49:31.676Z","comments":true,"path":"2021/9527202101-07 21:45:05/","link":"","permalink":"http://huermosi.xyz/2021/9527202101-07%2021:45:05/","excerpt":"","text":"1. 开局一张图这张图是重点！！！咱要先对MySQL有一个宏观的了解，知道他的执行流程。 一条SQL语句过来的流程是什么样的？那就follow me。哈哈哈哈，皮一下很开心。 当客户端连接到MySQL服务器时，服务器对其进行认证。可以通过用户名与密码认证，也可以通过SSL证书进行认证。登录认证后，服务器还会验证客户端是否有执行某个查询的操作权限。 在正式查询之前，服务器会检查查询缓存，如果能找到对应的查询，则不必进行查询解析，优化，执行等过程，直接返回缓存中的结果集。 MySQL的解析器会根据查询语句，构造出一个解析树，主要用于根据语法规则来验证语句是否正确，比如SQL的关键字是否正确，关键字的顺序是否正确。 而预处理器主要是进一步校验，比如表名，字段名是否正确等4. 查询优化器将解析树转化为查询计划，一般情况下，一条查询可以有很多种执行方式，最终返回相同的结果，优化器就是根据成本找到这其中最优的执行计划5. 执行计划调用查询执行引擎，而查询引擎通过一系列API接口查询到数据6. 得到数据之后，在返回给客户端的同时，会将数据存在查询缓存中 2. 查询缓存我们先通过show variables like ‘%query_cache%’来看一下默认的数据库配置，此为本地数据库的配置。 2.1 概念have_query_cache:当前的MYSQL版本是否支持“查询缓存”功能。 query_cache_limit:MySQL能够缓存的最大查询结果，查询结果大于该值时不会被缓存。默认值是1048576(1MB) query_cache_min_res_unit:查询缓存分配的最小块（字节）。默认值是4096（4KB）。当查询进行时，MySQL把查询结果保存在query cache，但是如果保存的结果比较大，超过了query_cache_min_res_unit的值，这时候MySQL将一边检索结果，一边进行保存结果。他保存结果也是按默认大小先分配一块空间，如果不够，又要申请新的空间给他。如果查询结果比较小，默认的query_cache_min_res_unit可能造成大量的内存碎片，如果查询结果比较大，默认的query_cache_min_res_unit又不够，导致一直分配块空间，所以可以根据实际需求，调节query_cache_min_res_unit的大小。 注：如果上面说的内容有点弯弯绕，那举个现实生活中的例子，比如咱现在要给运动员送水，默认的是500ml的瓶子，如果过来的是少年运动员，可能500ml太大了，他们喝不完，造成了浪费，那我们就可以选择300ml的瓶子，如果过来的是成年运动员，可能500ml不够，那他们一瓶喝完了，又开一瓶，直接不渴为止。那么那样开瓶子也要时间，我们就可以选择1000ml的瓶子。 query_cache_size:为缓存查询结果分配的总内存。 query_cache_type:默认为on，可以缓存除了以select sql_no_cache开头的所有查询结果。 query_cache_wlock_invalidate:如果该表被锁住，是否返回缓存中的数据，默认是关闭的。 2.2 原理MYSQL的查询缓存实质上是缓存SQL的hash值和该SQL的查询结果，如果运行相同的SQL,服务器直接从缓存中去掉结果，而不再去解析，优化，寻找最低成本的执行计划等一系列操作，大大提升了查询速度。 但是万事有利也有弊。 第一个弊端就是如果表的数据有一条发生变化，那么缓存好的结果将全部不再有效。这对于频繁更新的表，查询缓存是不适合的。 比如一张表里面只有两个字段，分别是id和name，数据有一条为1，张三。我使用select * from 表名 where name=“张三”来进行查询，MySQL发现查询缓存中没有此数据，会进行一系列的解析，优化等操作进行数据的查询，查询结束之后将该SQL的hash和查询结果缓存起来，并将查询结果返回给客户端。但是这个时候我有新增了一条数据2，张三。如果我还用相同的SQL来执行，他会根据该SQL的hash值去查询缓存中，那么结果就错了。所以MySQL对于数据有变化的表来说，会直接清空关于该表的所有缓存。这样其实是效率是很差的。 第二个弊端就是缓存机制是通过对SQL的hash，得出的值为key，查询结果为value来存放的，那么就意味着SQL必须完完全全一模一样，否则就命不中缓存。 我们都知道hash值的规则，就算很小的查询，哈希出来的结果差距是很多的，所以select * from 表名 where name=“张三”和SELECT * FROM 表名 WHERE NAME=“张三”和select * from 表名 where name = “张三”，三个SQL哈希出来的值是不一样的，大小写和空格影响了他们，所以并不能命中缓存，但其实他们搜索结果是完全一样的。 2.3 生产如何设置MySQL Query Cache先来看线上参数： 我们发现将query_cache_type设置为OFF，其实网上资料和各大云厂商提供的云服务器都是将这个功能关闭的，从上面的原理来看，在一般情况下，他的弊端大于优点。 3. 索引3.1 例子创建一个名为user的表，其包括id，name，age，sex等字段信息。此外，id为主键聚簇索引，idx_name为非聚簇索引。 12345678CREATE TABLE `user` ( `id` varchar(10) NOT NULL DEFAULT '', `name` varchar(10) DEFAULT NULL, `age` int(11) DEFAULT NULL, `sex` varchar(10) DEFAULT NULL, PRIMARY KEY (`id`), KEY `idx_name` (`name`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8; 我们将其设置10条数据，便于下面的索引的理解。 12345678910INSERT INTO `user` VALUES ('1', 'andy', '20', '女');INSERT INTO `user` VALUES ('10', 'baby', '12', '女');INSERT INTO `user` VALUES ('2', 'kat', '12', '女');INSERT INTO `user` VALUES ('3', 'lili', '20', '男');INSERT INTO `user` VALUES ('4', 'lucy', '22', '女');INSERT INTO `user` VALUES ('5', 'bill', '20', '男');INSERT INTO `user` VALUES ('6', 'zoe', '20', '男');INSERT INTO `user` VALUES ('7', 'hay', '20', '女');INSERT INTO `user` VALUES ('8', 'tony', '20', '男');INSERT INTO `user` VALUES ('9', 'rose', '21', '男'); 3.2 聚簇索引（主键索引）先来一张图镇楼，接下来就是看图说话。他包含两个特点： 使用记录主键值的大小来进行记录和页的排序。页内的记录是按照主键的大小顺序排成一个单项链表。各个存放用户记录的页也是根据页中用户记录的主键大小顺序排成一个双向链表。 叶子节点存储的是完整的用户记录。 注：聚簇索引不需要我们显示的创建，他是由InnoDB存储引擎自动为我们创建的。如果没有主键，其也会默认创建一个。复制代码 3.3 非聚簇索引（二级索引）上面的聚簇索引只能在搜索条件是主键时才能发挥作用，因为聚簇索引可以根据主键进行排序的。如果搜索条件是name，在刚才的聚簇索引上，我们可能遍历，挨个找到符合条件的记录，但是，这样真的是太蠢了，MySQL不会这样做的。 如果我们想让搜索条件是name的时候，也能使用索引，那可以多创建一个基于name的二叉树。如下图。他与聚簇索引的不同： 叶子节点内部使用name字段排序，叶子节点之间也是使用name字段排序。 叶子节点不再是完整的数据记录，而是name和主键值。 为什么不再是完整信息？ MySQL只让聚簇索引的叶子节点存放完整的记录信息，因为如果有好几个非聚簇索引，他们的叶子节点也存放完整的记录绩效，那就不浪费空间啦。 如果我搜索条件是基于name，需要查询所有字段的信息，那查询过程是啥？ 根据查询条件，采用name的非聚簇索引，先定位到该非聚簇索引某些记录行。 根据记录行找到相应的id，再根据id到聚簇索引中找到相关记录。这个过程叫做回表。 3.4 联合索引图就不画了，简单来说，如果name和age组成一个联合索引，那么先按name排序，如果name一样，就按age排序。 3.5 一些原则 最左前缀原则。一个联合索引（a,b,c）,如果有一个查询条件有a，有b，那么他则走索引，如果有一个查询条件没有a，那么他则不走索引。 使用唯一索引。具有多个重复值的列，其索引效果最差。例如，存放姓名的列具有不同值，很容易区分每行。而用来记录性别的列，只含有“男”，“女”，不管搜索哪个值，都会得出大约一半的行，这样的索引对性能的提升不够高。 不要过度索引。每个额外的索引都要占用额外的磁盘空间，并降低写操作的性能。在修改表的内容时，索引必须进行更新，有时可能需要重构，因此，索引越多，所花的时间越长。 索引列不能参与计算，保持列“干净”，比如from_unixtime(create_time) = ’2014-05-29’就不能使用到索引，原因很简单，b+树中存的都是数据表中的字段值，但进行检索时，需要把所有元素都应用函数才能比较，显然成本太大。所以语句应该写成create_time = unix_timestamp(’2014-05-29’); 一定要设置一个主键。前面聚簇索引说到如果不指定主键，InnoDB会自动为其指定主键，这个我们是看不见的。反正都要生成一个主键的，还不如我们设置，以后在某些搜索条件时还能用到主键的聚簇索引。 主键推荐用自增id，而不是uuid。上面的聚簇索引说到每页数据都是排序的，并且页之间也是排序的，如果是uuid，那么其肯定是随机的，其可能从中间插入，导致页的分裂，产生很多表碎片。如果是自增的，那么其有从小到大自增的，有顺序，那么在插入的时候就添加到当前索引的后续位置。当一页写满，就会自动开辟一个新的页。 注：如果自增id用完了，那将字段类型改为bigint，就算每秒1万条数据，跑100年，也没达到bigint的最大值。 3.6 万年面试题（为什么索引用B+树） B+树的磁盘读写代价更低：B+树的内部节点并没有指向关键字具体信息的指针，因此其内部节点相对B树更小，如果把所有同一内部节点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多，一次性读入内存的需要查找的关键字也就越多，相对IO读写次数就降低了。 由于B+树的数据都存储在叶子结点中，分支结点均为索引，方便扫库，只需要扫一遍叶子结点即可，但是B树因为其分支结点同样存储着数据，我们要找到具体的数据，需要进行一次中序遍历按序来扫，所以B+树更加适合在区间查询的情况，所以通常B+树用于数据库索引。 4. 优化器在开篇的图里面，我们知道了SQL语句从客户端经由网络协议到查询缓存，如果没有命中缓存，再经过解析工作，得到准确的SQL，现在就来到了我们这模块说的优化器。 首先，我们知道每一条SQL都有不同的执行方法，要不通过索引，要不通过全表扫描的方式。 那么问题就来了，MySQL是如何选择时间最短，占用内存最小的执行方法呢？ 4.1 什么是成本？ I/O成本。数据存储在硬盘上，我们想要进行某个操作需要将其加载到内存中，这个过程的时间被称为I/O成本。默认是1。 CPU成本。在内存对结果集进行排序的时间被称为CPU成本。默认是0.2。 4.2 单表查询的成本先来建一个用户表dev_user，里面包括主键id，用户名username，密码password，外键user_info_id，状态status，外键main_station_id，是否外网访问visit，这七个字段。索引有两个，一个是主键的聚簇索引，另一个是显式添加的以username为字段的唯一索引uname_unique。 如果搜索条件是select * from dev_user where username=’XXX’，那么MySQL是如何选择相关索引呢？ 使用所有可能用到的索引我们可以看到搜索条件username，所以可能走uname_unique索引。也可以做聚簇索引，也就是全表扫描。 计算全表扫描代价我们通过show table status like ‘dev_user’命令知道rows和data_length字段，如下图。rows：表示表中的记录条数，但是这个数据不准确，是个估计值。 data_length:表示表占用的存储空间字节数。 data_length=聚簇索引的页面数量X每个页面的大小 反推出页面数量=1589248÷16÷1024=97 I/O成本：97X1=97 CPU成本：6141X0.2=1228 总成本：97+1228=1325 计算使用不同索引执行查询的代价因为要查询出满足条件的所有字段信息，所以要考虑回表成本。I/O成本=1+1X1=2(范围区间的数量+预计二级记录索引条数) CPU成本=1X0.2+1X0.2=0.4(读取二级索引的成本+回表聚簇索引的成本) 总成本=I/O成本+CPU成本=2.4 对比各种执行方案的代价，找出成本最低的那个 上面两个数字一对比，成本是采用uname_unique索引成本最低。 4.3 多表查询的成本对于两表连接查询来说，他的查询成本由下面两个部分构成： 单次查询驱动表的成本 多次查询被驱动表的成本（具体查询多次取决于对驱动表查询的结果集有多少个记录） 4.4 index dive如果前面的搜索条件不是等值，而是区间，如select * from dev_user where username&gt;’admin’ and username&lt;’test’这个时候我们是无法看出需要回表的数量。 步骤1：先根据username&gt;’admin’这个条件找到第一条记录，称为区间最左记录。 步骤2：再根据username&lt;’test’这个条件找到最后一条记录，称为区间最右记录。 步骤3：如果区间最左记录和区间最右记录相差不是很远，可以准确统计出需要回表的数量。如果相差很远，就先计算10页有多少条记录，再乘以页面数量，最终模糊统计出来。 5. Explain5.1 产品来索命产品：为什么这个页面出来这么慢？ 开发：因为你查的数据多呗，他就是这么慢 产品：我不管，我要这个页面快点，你这样，客户怎么用啊 开发：。。。。。。。你行你来 哈哈哈哈，不瞎BB啦，如果有些SQL贼慢，我们需要知道他有没有走索引，走了哪个索引，这个时候我就需要通过explain关键字来深入了解MySQL内部是如何执行的。 5.2 id一般来说一个select一个唯一id，如果是子查询，就有两个select，id是不一样的，但是凡事有例外，有些子查询的，他们id是一样的。 这是为什么呢？ 那是因为MySQL在进行优化的时候已经将子查询改成了连接查询，而连接查询的id是一样的。 5.3 select_type simple：不包括union和子查询的查询都算simple类型。 primary：包括union，union all，其中最左边的查询即为primary。 union：包括union，union all，除了最左边的查询，其他的查询类型都为union。 5.4 table显示这一行是关于哪张表的。 5.5 type：访问方法 ref：普通二级索引与常量进行等值匹配 ref_or_null：普通二级索引与常量进行等值匹配，该索引可能是null const：主键或唯一二级索引列与常量进行等值匹配 range：范围区间的查询 all：全表扫描 5.6 possible_keys对某表进行单表查询时可能用到的索引 5.7 key经过查询优化器计算不同索引的成本，最终选择成本最低的索引 5.8 rows 如果使用全表扫描，那么rows就代表需要扫描的行数 如果使用索引，那么rows就代表预计扫描的行数 5.9 filtered 如果全表扫描，那么filtered就代表满足搜索条件的记录的满分比 如果是索引，那么filtered就代表除去索引对应的搜索，其他搜索条件的百分比 6. redo日志（物理日志）InnoDB存储引擎是以页为单位来管理存储空间的，我们进行的增删改查操作都是将页的数据加载到内存中，然后进行操作，再将数据刷回到硬盘上。 那么问题就来了，如果我要给张三转账100块钱，事务已经提交了，这个时候InnoDB把数据加载到内存中，这个时候还没来得及刷入硬盘，突然停电了，数据库崩了。重启之后，发现我的钱没有转成功，这不是尴尬了吗？ 解决方法很明显，我们在硬盘加载到内存之后，进行一系列操作，一顿操作猛如虎，还未刷新到硬盘之前，先记录下，在XXX位置我的记录中金额减100，在XXX位置张三的记录中金额加100，然后再进行增删改查操作，最后刷入硬盘。如果未刷入硬盘，在重启之后，先加载之前的记录，那么数据就回来了。 这个记录就叫做重做日志，即redo日志。他的目的是想让已经提交的事务对数据的修改是永久的，就算他重启，数据也能恢复出来。 6.1 log buffer（日志缓冲区）为了解决磁盘速度过慢的问题，redo日志不能直接写入磁盘，咱先整一大片连续的内存空间给他放数据。这一大片内存就叫做日志缓冲区，即log buffer。到了合适的时候，再刷入硬盘。至于什么时候是合适的，这个下一章节说。 我们可以通过show VARIABLES like ‘innodb_log_buffer_size’命令来查看当前的日志缓存大小，下图为线上的大小。 6.2 redo日志刷盘时机由于redo日志一直都是增长的，且内存空间有限，数据也不能一直待在缓存中， 我们需要将其刷新至硬盘上。 那什么时候刷新到硬盘呢？ log buffer空间不足。上面有指定缓冲区的内存大小，MySQL认为日志量已经占了 总容量的一半左右，就需要将这些日志刷新到磁盘上。 事务提交时。我们使用redo日志的目的就是将他未刷新到磁盘的记录保存起来，防止 丢失，如果数据提交了，我们是可以不把数据提交到磁盘的，但为了保证持久性，必须 把修改这些页面的redo日志刷新到磁盘。 后台线程不同的刷新 后台有一个线程，大概每秒都会将log buffer里面的redo日志刷新到硬盘上。 checkpoint 下下小节讲 6.3 redo日志文件组我们可以通过show variables like ‘datadir’命令找到相关目录，底下有两个文件， 分别是ib_logfile0和ib_logfile1,如下图所示。 我们将缓冲区log buffer里面的redo日志刷新到这个两个文件里面，他们写入的方式 是循环写入的，先写ib_logfile0,再写ib_logfile1,等ib_logfile1写满了，再写ib_logfile0。 那这样就会存在一个问题，如果ib_logfile1写满了，再写ib_logfile0，之前ib_logfile0的内容 不就被覆盖而丢失了吗？ 这就是checkpoint的工作啦。 6.4 checkpointredo日志是为了系统崩溃后恢复脏页用的，如果这个脏页可以被刷新到磁盘上，那么 他就可以功成身退，被覆盖也就没事啦。 冲突补习 从系统运行开始，就不断的修改页面，会不断的生成redo日志。redo日志是不断 递增的，MySQL为其取了一个名字日志序列号Log Sequence Number，简称lsn。 他的初始化的值为8704，用来记录当前一共生成了多少redo日志。 redo日志是先写入log buffer，之后才会被刷新到磁盘的redo日志文件。MySQL为其 取了一个名字flush_to_disk_lsn。用来说明缓存区中有多少的脏页数据被刷新到磁盘上啦。 他的初始值和lsn一样，后面的差距就有了。 做一次checkpoint分为两步 计算当前系统可以被覆盖的redo日志对应的lsn最大值是多少。redo日志可以被覆盖， 意味着他对应的脏页被刷新到磁盘上，只要我们计算出当前系统中最早被修改的oldest_modification, 只要系统中lsn小于该节点的oldest_modification值磁盘的redo日志都是可以被覆盖的。 将lsn过程中的一些数据统计。 7. undo日志（这部分不是很明白，所以大概说了）7.1 基本概念undo log有两个作用：提供回滚和多个行版本控制(MVCC)。 undo log和redo log记录物理日志不一样，它是逻辑日志。可以认为当delete一条记录时，undo log中会记录一条对应的insert记录，反之亦然，当update一条记录时，它记录一条对应相反的update记录。 举个例子: insert into a(id) values(1);(redo)这条记录是需要回滚的。回滚的语句是delete from a where id = 1;(undo) 试想想看。如果没有做insert into a(id) values(1);(redo)那么delete from a where id = 1;(undo)这句话就没有意义了。 现在看下正确的恢复:先insert into a(id) values(1);(redo)然后delete from a where id = 1;(undo)系统就回到了原先的状态，没有这条记录了 7.2 存储方式是存在段之中。 8. 事务8.1 引言事务中有一个隔离性特征，理论上在某个事务对某个数据进行访问时，其他事务应该排序，当该事务提交之后，其他事务才能继续访问这个数据。 但是这样子对性能影响太大，我们既想保持事务的隔离性，又想让服务器在出来多个事务时性能尽量高些，所以只能舍弃一部分隔离性而去性能。 8.2 事务并发执行的问题 脏写（这个太严重了，任何隔离级别都不允许发生）sessionA：修改了一条数据，回滚掉 sessionB：修改了同一条数据，提交掉 对于sessionB来说，明明数据更新了也提交了事务，不能说自己啥都没干 脏读：一个事务读到另一个未提交事务修改的数据session A：查询，得到某条数据 session B：修改某条数据，但是最后回滚掉啦 session A：在sessionB修改某条数据之后，在回滚之前，读取了该条记录 对于session A来说，读到了session回滚之前的脏数据 不可重复读：前后多次读取，同一个数据内容不一样session A：查询某条记录session B : 修改该条记录，并提交事务session A : 再次查询该条记录，发现前后查询不一致 幻读：前后多次读取，数据总量不一致session A：查询表内所有记录session B : 新增一条记录，并查询表内所有记录session A : 再次查询该条记录，发现前后查询不一致 8.3 四种隔离级别数据库都有的四种隔离级别，MySQL事务默认的隔离级别是可重复读，而且MySQL可以解决了幻读的问题。 未提交读：脏读，不可重复读，幻读都有可能发生 已提交读：不可重复读，幻读可能发生 可重复读：幻读可能发生 可串行化：都不可能发生但凡事没有百分百，emmmm，其实MySQL并没有百分之百解决幻读的问题。 举个例子： session A：查询某条不存在的记录。 session B：新增该条不存在的记录，并提交事务。 session A：再次查询该条不存在的记录，是查询不出来的，但是如果我尝试修改该条记录，并提交，其实他是可以修改成功的。 8.4 MVCC版本链：对于该记录的每次更新，都会将值放在一条undo日志中，算是该记录的一个旧版本，随着更新次数的增多，所有版本都会被roll_pointer属性连接成一个链表，即为版本链。 readview： 未提交读：因为可以读到未提交事务修改的记录，所以可以直接读取记录的最新版本就行 已提交读：每次读取之前都生成一个readview 可重复读：只有在第一次读取的时候才生成readview 可串行化：InnoDB涉及了加锁的方式来访问记录 本文转载自追逐仰望星空头条，原文连接点这里，版权归原作者所有，转载目的是为了避免以后找不到。","categories":[{"name":"MYSQL","slug":"MYSQL","permalink":"http://huermosi.xyz/categories/MYSQL/"}],"tags":[{"name":"MYSQL","slug":"MYSQL","permalink":"http://huermosi.xyz/tags/MYSQL/"},{"name":"索引","slug":"索引","permalink":"http://huermosi.xyz/tags/%E7%B4%A2%E5%BC%95/"},{"name":"EXPLAIN","slug":"EXPLAIN","permalink":"http://huermosi.xyz/tags/EXPLAIN/"},{"name":"REDO","slug":"REDO","permalink":"http://huermosi.xyz/tags/REDO/"},{"name":"事务","slug":"事务","permalink":"http://huermosi.xyz/tags/%E4%BA%8B%E5%8A%A1/"}]},{"title":"面试官：你是如何评估一个线程池需要设置多少个线程","slug":"java/java-thread-pool1","date":"2020-05-11T07:18:33.000Z","updated":"2021-04-08T08:49:31.628Z","comments":true,"path":"2020/952720210401151900/","link":"","permalink":"http://huermosi.xyz/2020/952720210401151900/","excerpt":"","text":"Java并发编程是大厂第一轮面试中的高频面试题，而线程池又是其中的典型代表，本文将梳理关于线程池的工作机制，并提出灵魂之问：你对线程池的工作机制这么了解，那你在工作中是如何判断一个线程池需要创建多少个线程的呢？ 1. 线程池基本工作原理与面试指南1.1 Java线程池的核心属性JAVA 线程池的核心属性如下： int corePoolSize核心线程数 int maximumPoolSize线程池最大线程数 long keepAliveTime线程保持活跃的时间 TimeUnit unitkeepAliveTime 的时间单位 BlockingQueue&lt; Runnable &gt; workQueue任务挤压队列 ThreadFactory threadFactory线程创建工厂类 RejectedExecutionHandler handler拒绝策略 1.2 向线程池提交任务时线程创建过程那当用户向线程池提交一个任务的时候，线程池会如何创建线程呢？ 首先线程池会判断当前已创建的线程是否小于 corePoolSize (核心线程数)，如果小于，则无论已创建的线程是否空闲，都会选择创建一个新的线程来执行该任务，直到已创建的线程等于核心线程数。 当线程池中已创建的线程数等于核心核心线程数时，用户继续向线程池提交任务时，此时会先判断任务队列是否已满：1）如果任务队列未满，则将任务放入队列中。2）如果任务队列已满，则判断当前线程数量是否超过了最大线程数量，如果未超过，则创建一个新的线程来执行该任务，如果线程池已创建的线程数量等最大线程数，则执行拒绝策略。 温馨提示：所以如果线程池使用的队列无界队列，最大线程数会变得没有意义。 1.3 线程池的拒绝策略、使用场景JUC 默认提供了如下拒绝策略： AbortPolicy拒绝，直接抛出 RejectedExecutionException，默认值。 CallerRunsPolicy由调用线程直接运行任务的 run 方法，即异步转同步。 DiscardOldestPolicy丢弃任务队列中最先进入的任务。 DiscardPolicy拒绝了，就不执行，“当没事人事”样。 拒绝策略触发的条件：线程池使用的是有界任务队列时，才有可能被触发，当队列已满，并且线程池创建的线程已经达到了最大允许的线程池时。 默认情况下，通常使用 AbortPolicy 即可。 CallerRunsPolicy 异步转同步在出现拒绝的情况下其实意义不大，没有想出其合适的场景，因为需要执行拒绝策略的时候，已经处理变慢了，再同步执行任务，只会增加服务器的负载，不利于恢复问题。 DiscardOldestPolicy 这种策略，通常用于类似记录轨迹，偶尔丢失点数据没关系，但希望最新的数据能得到保存。 DiscardPolicy 策略，通常用来异步打印日志，直接忽略不执行，期望保存旧的数据。 1.4 如何选择阻塞队列阿里内部的开源规范明确禁止使用无界队列，如果使用无界队列，任务会不受限制的往线程池中提交，有可能造成内存溢出。 如果使用无界队列，最大线程数这个参数将会失效，因为永远也不会创建多于核心线程数量的线程。 1.5 线程池工厂有何实际用处ThreadFactory threadFactory，线程池工厂，在使用线程池时，强烈推荐使用自己定义的线程工厂，这样能为线程池中的线程进行命名，方便跟大家使用 jsatck 命令查看线程栈时，能快速识别对应的线程。 1.6 keepAliveTime参数的作用keepAliveTime ：通俗点来说，这个参数表示线程的最大空闲时间，即如果线程没有在执行任务，能存活的时间。 默认情况下，该参数只针对超过核心线程数(corePoolSize) 的线程,可通过将allowCoreThreadTimeOut设置为true，则核心线程数也会因为空闲而被关闭。 2. 如何为一个线程池设置合适的线程数量目前根据我看过的一些开源框架，设置多少个线程数量通常是根据应用的类型：IO密集型、CPU密集型。 IO密集型通常设置为2n+1，其中n为CPU核数 CPU密集型通常设置为 n+1。实际情况往往复杂得多，并不会按照这个进行设置，上面的公司通常适合框架设置IO线程的个数，例如netty,dubbo这种底层通讯框架通常会参考上述标准进行设置。 关于在实际业务开发中，如何为一个线程池设置合适的线程呢？ 其实对于IO密集型类型的应用，网上还有一个公式：线程数 = CPU核心数/(1-阻塞系数) 引入了阻塞系数的概念，一般为0.8~0.9之间， 在我们的业务开发中，基本上都是IO密集型，因为往往都会去操作数据库，访问redis，es等存储型组件，都会涉及到磁盘IO，网络IO。 那什么场景下是CPU密集型呢？纯计算类，例如计算圆周率的位数，当然我们基本接触不到。 IO密集型，可以考虑多设置一些线程，主要目的是可以增加IO的并发度，CPU密集型不宜设置过多线程，因为是会造成线程切换，反而损耗性能。 接下来我们以一个实际的场景来说明如何设置线程数量。 一个4C8G的机器上部署了一个MQ消费者，在RocketMQ的实现中，消费端也是用一个线程池来消费线程的，那这个线程数要怎么设置呢？ 如果按照 2n + 1 的公式，线程数设置为 9个，但在我们实践过程中发现如果增大线程数量，会显著提高消息的处理能力，说明 2n + 1 对于业务场景来说，并不太合适。 如果套用 线程数 = CPU核心数/(1-阻塞系数) 阻塞系数取 0.8 ，线程数为 20 。阻塞系数取 0.9，大概线程数40，20个线程数我觉得可以。 如果我们发现数据库的操作耗时比较多，此时可以继续提高阻塞系数，从而增大线程数量。 那我们怎么判断需要增加更多线程呢？可以用jstack命令查看一下进程的线程栈，如果发现线程池中大部分线程都处于等待获取任务，则说明线程够用，如下图所示：如果大部分线程都处于运行状态，可以继续适当调高线程数量。 本文转载自中间件兴趣圈头条，原文连接点这里，版权归原作者所有，转载目的是为了避免以后找不到。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/categories/JAVA/"},{"name":"多线程","slug":"JAVA/多线程","permalink":"http://huermosi.xyz/categories/JAVA/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/tags/JAVA/"},{"name":"THREAD","slug":"THREAD","permalink":"http://huermosi.xyz/tags/THREAD/"},{"name":"线程","slug":"线程","permalink":"http://huermosi.xyz/tags/%E7%BA%BF%E7%A8%8B/"},{"name":"线程池","slug":"线程池","permalink":"http://huermosi.xyz/tags/%E7%BA%BF%E7%A8%8B%E6%B1%A0/"}]},{"title":"都前后端分离了，咱就别做页面跳转了！统统 JSON 交互","slug":"springsecurity/springsecurity-json-jwt-login-stateless","date":"2020-04-02T08:58:01.000Z","updated":"2021-04-08T09:08:55.592Z","comments":true,"path":"2020/9527218880001/","link":"","permalink":"http://huermosi.xyz/2020/9527218880001/","excerpt":"","text":"前两天有个小伙伴在微信上问松哥，这前后端分离开发后，认证这一块到底是使用传统的 session 还是使用像 JWT 这样的 token 来解决呢？ 这确实代表了两种不同的方向。 传统的通过 session 来记录用户认证信息的方式我们可以理解为这是一种有状态登录，而 JWT 则代表了一种无状态登录。可能有小伙伴对这个概念还不太熟悉，我这里就先来科普一下有状态登录和无状态登录。 1. 无状态登录1.1 什么是有状态有状态服务，即服务端需要记录每次会话的客户端信息，从而识别客户端身份，根据用户身份进行请求的处理，典型的设计如 Tomcat 中的 Session。例如登录：用户登录后，我们把用户的信息保存在服务端 session 中，并且给用户一个 cookie 值，记录对应的 session，然后下次请求，用户携带 cookie 值来（这一步有浏览器自动完成），我们就能识别到对应 session，从而找到用户的信息。这种方式目前来看最方便，但是也有一些缺陷，如下： 服务端保存大量数据，增加服务端压力 服务端保存用户状态，不支持集群化部署 1.2 什么是无状态微服务集群中的每个服务，对外提供的都使用 RESTful 风格的接口。而 RESTful 风格的一个最重要的规范就是：服务的无状态性，即： 服务端不保存任何客户端请求者信息 客户端的每次请求必须具备自描述信息，通过这些信息识别客户端身份 那么这种无状态性有哪些好处呢？ 客户端请求不依赖服务端的信息，多次请求不需要必须访问到同一台服务器 服务端的集群和状态对客户端透明 服务端可以任意的迁移和伸缩（可以方便的进行集群化部署） 减小服务端存储压力 1.3 如何实现无状态无状态登录的流程： 首先客户端发送账户名/密码到服务端进行认证 认证通过后，服务端将用户信息加密并且编码成一个 token，返回给客户端 以后客户端每次发送请求，都需要携带认证的 token 服务端对客户端发送来的 token 进行解密，判断是否有效，并且获取用户登录信息 1.4 各自优缺点使用 session 最大的优点在于方便。你不用做过多的处理，一切都是默认的即可。松哥本系列前面几篇文章我们也都是基于 session 来讲的。 但是使用 session 有另外一个致命的问题就是如果你的前端是 Android、iOS、小程序等，这些 App 天然的就没有 cookie，如果非要用 session，就需要这些工程师在各自的设备上做适配，一般是模拟 cookie，从这个角度来说，在移动 App 遍地开花的今天，我们单纯的依赖 session 来做安全管理，似乎也不是特别理想。 这个时候 JWT 这样的无状态登录就展示出自己的优势了，这些登录方式所依赖的 token 你可以通过普通参数传递，也可以通过请求头传递，怎么样都行，具有很强的灵活性。 不过话说回来，如果你的前后端分离只是网页+服务端，其实没必要上无状态登录，基于 session 来做就可以了，省事又方便。 好了，说了这么多，本文我还是先来和大家说说基于 session 的认证，关于 JWT 的登录以后我会和大家细说，如果小伙伴们等不及，也可以先看看松哥之前发的关于 JWT 的教程：Spring Security 结合 Jwt 实现无状态登录。 2. 登录交互在上篇文章中，松哥和大家捋了常见的登录参数配置问题，对于登录成功和登录失败，我们还遗留了一个回调函数没有讲，这篇文章就来和大家细聊一下。 2.1 前后端分离的数据交互在前后端分离这样的开发架构下，前后端的交互都是通过 JSON 来进行，无论登录成功还是失败，都不会有什么服务端跳转或者客户端跳转之类。 登录成功了，服务端就返回一段登录成功的提示 JSON 给前端，前端收到之后，该跳转该展示，由前端自己决定，就和后端没有关系了。 登录失败了，服务端就返回一段登录失败的提示 JSON 给前端，前端收到之后，该跳转该展示，由前端自己决定，也和后端没有关系了。 首先把这样的思路确定了，基于这样的思路，我们来看一下登录配置。 2.2 登录成功之前我们配置登录成功的处理是通过如下两个方法来配置的： defaultSuccessUrl successForwardUrl这两个都是配置跳转地址的，适用于前后端不分的开发。除了这两个方法之外，还有一个必杀技，那就是 successHandler。 successHandler 的功能十分强大，甚至已经囊括了 defaultSuccessUrl 和 successForwardUrl 的功能。我们来看一下： 12345678.successHandler((req, resp, authentication) -&gt; &#123; Object principal = authentication.getPrincipal(); resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); out.write(new ObjectMapper().writeValueAsString(principal)); out.flush(); out.close();&#125;) successHandler 方法的参数是一个AuthenticationSuccessHandler 对象，这个对象中我们要实现的方法是 onAuthenticationSuccess。 onAuthenticationSuccess 方法有三个参数，分别是： HttpServletRequest HttpServletResponse Authentication有了前两个参数，我们就可以在这里随心所欲的返回数据了。利用 HttpServletRequest 我们可以做服务端跳转，利用 HttpServletResponse 我们可以做客户端跳转，当然，也可以返回 JSON 数据。 第三个 Authentication 参数则保存了我们刚刚登录成功的用户信息。 配置完成后，我们再去登录，就可以看到登录成功的用户信息通过 JSON 返回到前端了，如下： 当然用户的密码已经被擦除掉了。擦除密码的问题，松哥之前和大家分享过，大家可以参考这篇文章：手把手带你捋一遍 Spring Security 登录流程 2.3 登录失败登录失败也有一个类似的回调，如下： 1234567.failureHandler((req, resp, e) -&gt; &#123; resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); out.write(e.getMessage()); out.flush(); out.close();&#125;) 失败的回调也是三个参数，前两个就不用说了，第三个是一个 Exception，对于登录失败，会有不同的原因，Exception 中则保存了登录失败的原因，我们可以将之通过 JSON 返回到前端。 当然大家也看到，在微人事中，我还挨个去识别了一下异常的类型，根据不同的异常类型，我们可以给用户一个更加明确的提示： 1234567891011121314151617resp.setContentType(\"application/json;charset=utf-8\");PrintWriter out = resp.getWriter();RespBean respBean = RespBean.error(e.getMessage());if (e instanceof LockedException) &#123; respBean.setMsg(\"账户被锁定，请联系管理员!\");&#125; else if (e instanceof CredentialsExpiredException) &#123; respBean.setMsg(\"密码过期，请联系管理员!\");&#125; else if (e instanceof AccountExpiredException) &#123; respBean.setMsg(\"账户过期，请联系管理员!\");&#125; else if (e instanceof DisabledException) &#123; respBean.setMsg(\"账户被禁用，请联系管理员!\");&#125; else if (e instanceof BadCredentialsException) &#123; respBean.setMsg(\"用户名或者密码输入错误，请重新输入!\");&#125;out.write(new ObjectMapper().writeValueAsString(respBean));out.flush();out.close(); 这里有一个需要注意的点。 我们知道，当用户登录时，用户名或者密码输入错误，我们一般只给一个模糊的提示，即「用户名或者密码输入错误，请重新输入」，而不会给一个明确的诸如“用户名输入错误”或“密码输入错误”这样精确的提示，但是对于很多不懂行的新手小伙伴，他可能就会给一个明确的错误提示，这会给系统带来风险。 但是使用了 Spring Security 这样的安全管理框架之后，即使你是一个新手，也不会犯这样的错误。 在 Spring Security 中，用户名查找失败对应的异常是： UsernameNotFoundException密码匹配失败对应的异常是： BadCredentialsException但是我们在登录失败的回调中，却总是看不到 UsernameNotFoundException 异常，无论用户名还是密码输入错误，抛出的异常都是 BadCredentialsException。 这是为什么呢？松哥在之前的文章手把手带你捋一遍 Spring Security 登录流程中介绍过，在登录中有一个关键的步骤，就是去加载用户数据，我们再来把这个方法拎出来看一下（部分）： 123456789101112131415161718public Authentication authenticate(Authentication authentication) throws AuthenticationException &#123; try &#123; user = retrieveUser(username, (UsernamePasswordAuthenticationToken) authentication); &#125; catch (UsernameNotFoundException notFound) &#123; logger.debug(\"User '\" + username + \"' not found\"); if (hideUserNotFoundExceptions) &#123; throw new BadCredentialsException(messages.getMessage( \"AbstractUserDetailsAuthenticationProvider.badCredentials\", \"Bad credentials\")); &#125; else &#123; throw notFound; &#125; &#125;&#125; 从这段代码中，我们看出，在查找用户时，如果抛出了 UsernameNotFoundException，这个异常会被捕获，捕获之后，如果hideUserNotFoundExceptions 属性的值为 true，就抛出一个 BadCredentialsException。相当于将 UsernameNotFoundException 异常隐藏了，而默认情况下，hideUserNotFoundExceptions 的值就为 true。 看到这里大家就明白了为什么无论用户还是密码写错，你收到的都是 BadCredentialsException 异常。 一般来说这个配置是不需要修改的，如果你一定要区别出来 UsernameNotFoundException 和 BadCredentialsException，我这里给大家提供三种思路： 自己定义 DaoAuthenticationProvider 代替系统默认的，在定义时将 hideUserNotFoundExceptions 属性设置为 false。 当用户名查找失败时，不抛出 UsernameNotFoundException 异常，而是抛出一个自定义异常，这样自定义异常就不会被隐藏，进而在登录失败的回调中根据自定义异常信息给前端用户一个提示。 当用户名查找失败时，直接抛出 BadCredentialsException，但是异常信息为 “用户名不存在”。三种思路仅供小伙伴们参考，除非情况特殊，一般不用修改这一块的默认行为。 官方这样做的好处是什么呢？很明显可以强迫开发者给一个模糊的异常提示，这样即使是不懂行的新手，也不会将系统置于危险之中。 好了，这样配置完成后，无论是登录成功还是失败，后端都将只返回 JSON 给前端了。 3. 未认证处理方案那未认证又怎么办呢？ 有小伙伴说，那还不简单，没有认证就访问数据，直接重定向到登录页面就行了，这没错，系统默认的行为也是这样。 但是在前后端分离中，这个逻辑明显是有问题的，如果用户没有登录就访问一个需要认证后才能访问的页面，这个时候，我们不应该让用户重定向到登录页面，而是给用户一个尚未登录的提示，前端收到提示之后，再自行决定页面跳转。 要解决这个问题，就涉及到 Spring Security 中的一个接口 AuthenticationEntryPoint ，该接口有一个实现类：LoginUrlAuthenticationEntryPoint ，该类中有一个方法 commence，如下： 1234567891011121314151617181920212223242526/** * Performs the redirect (or forward) to the login form URL. */public void commence(HttpServletRequest request, HttpServletResponse response, AuthenticationException authException) &#123; String redirectUrl = null; if (useForward) &#123; if (forceHttps &amp;&amp; \"http\".equals(request.getScheme())) &#123; redirectUrl = buildHttpsRedirectUrlForRequest(request); &#125; if (redirectUrl == null) &#123; String loginForm = determineUrlToUseForThisRequest(request, response, authException); if (logger.isDebugEnabled()) &#123; logger.debug(\"Server side forward to: \" + loginForm); &#125; RequestDispatcher dispatcher = request.getRequestDispatcher(loginForm); dispatcher.forward(request, response); return; &#125; &#125; else &#123; redirectUrl = buildRedirectUrlToLoginPage(request, response, authException); &#125; redirectStrategy.sendRedirect(request, response, redirectUrl);&#125; 首先我们从这个方法的注释中就可以看出，这个方法是用来决定到底是要重定向还是要 forward，通过 Debug 追踪，我们发现默认情况下 useForward 的值为 false，所以请求走进了重定向。 那么我们解决问题的思路很简单，直接重写这个方法，在方法中返回 JSON 即可，不再做重定向操作，具体配置如下： 123456789.csrf().disable().exceptionHandling().authenticationEntryPoint((req, resp, authException) -&gt; &#123; resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); out.write(\"尚未登录，请先登录\"); out.flush(); out.close(); &#125;); 在 Spring Security 的配置中加上自定义的 AuthenticationEntryPoint 处理方法，该方法中直接返回相应的 JSON 提示即可。这样，如果用户再去直接访问一个需要认证之后才可以访问的请求，就不会发生重定向操作了，服务端会直接给浏览器一个 JSON 提示，浏览器收到 JSON 之后，该干嘛干嘛。 4. 注销登录最后我们再来看看注销登录的处理方案。 注销登录我们前面说过，按照前面的配置，注销登录之后，系统自动跳转到登录页面，这也是不合适的，如果是前后端分离项目，注销登录成功后返回 JSON 即可，配置如下： 123456789101112.and().logout().logoutUrl(\"/logout\").logoutSuccessHandler((req, resp, authentication) -&gt; &#123; resp.setContentType(\"application/json;charset=utf-8\"); PrintWriter out = resp.getWriter(); out.write(\"注销成功\"); out.flush(); out.close();&#125;).permitAll().and() 这样，注销成功之后，前端收到的也是 JSON 了： 本文转载自江南一点雨头条，原文连接点这里，版权归原作者所有，转载目的是为了避免以后找不到。","categories":[{"name":"SPRING","slug":"SPRING","permalink":"http://huermosi.xyz/categories/SPRING/"},{"name":"SPRING SECURITY","slug":"SPRING/SPRING-SECURITY","permalink":"http://huermosi.xyz/categories/SPRING/SPRING-SECURITY/"}],"tags":[{"name":"SPRING","slug":"SPRING","permalink":"http://huermosi.xyz/tags/SPRING/"},{"name":"SPRING SECURITY","slug":"SPRING-SECURITY","permalink":"http://huermosi.xyz/tags/SPRING-SECURITY/"}]},{"title":"RabbitMQ如何防止数据丢失","slug":"rabbitmq/rabbitmq1","date":"2020-04-01T11:35:24.000Z","updated":"2021-04-08T08:49:31.707Z","comments":true,"path":"2020/952720210401193500/","link":"","permalink":"http://huermosi.xyz/2020/952720210401193500/","excerpt":"","text":"思维导图 2. 分析数据丢失的原因分析RabbitMQ消息丢失的情况，不妨先看看一条消息从生产者发送到消费者消费的过程：可以看出，一条消息整个过程要经历两次的网络传输：从生产者发送到RabbitMQ服务器，从RabbitMQ服务器发送到消费者。在消费者未消费前存储在队列(Queue)中。 所以可以知道，有三个场景下是会发生消息丢失的： 存储在队列中，如果队列没有对消息持久化，RabbitMQ服务器宕机重启会丢失数据。 生产者发送消息到RabbitMQ服务器过程中，RabbitMQ服务器如果宕机停止服务，消息会丢失。 消费者从RabbitMQ服务器获取队列中存储的数据消费，但是消费者程序出错或者宕机而没有正确消费，导致数据丢失。 针对以上三种场景，RabbitMQ提供了三种解决的方式，分别是消息持久化，confirm机制，ACK事务机制。 3. 消息持久化RabbitMQ是支持消息持久化的，消息持久化需要设置：Exchange为持久化和Queue持久化，这样当消息发送到RabbitMQ服务器时，消息就会持久化。首先看Exchange交换机的类图： 看这个类图其实是要说明上一篇文章介绍的四种交换机都是AbstractExchange抽象类的子类，所以根据java的特性，创建子类的实例会先调用父类的构造器，父类也就是AbstractExchange的构造器是怎么样的呢？ 从上面的注释可以看到durable参数表示是否持久化。默认是持久化(true)。创建持久化的Exchange可以这样写： 12345@Beanpublic DirectExchange rabbitmqDemoDirectExchange() &#123; //Direct交换机 return new DirectExchange(RabbitMQConfig.RABBITMQ_DEMO_DIRECT_EXCHANGE, true, false);&#125; 接着是Queue队列，我们先看看Queue的构造器是怎么样的： 也是通过durable参数设置是否持久化，默认是true。所以创建时可以不指定： 12345@Beanpublic Queue fanoutExchangeQueueA() &#123; //只需要指定名称，默认是持久化的 return new Queue(RabbitMQConfig.FANOUT_EXCHANGE_QUEUE_TOPIC_A);&#125; 这就完成了消息持久化的设置，接下来启动项目，发送几条消息，我们可以看到： 怎么证明是已经持久化了呢，实际上可以找到对应的文件： 找到对应磁盘中的目录：消息持久化可以防止消息在RabbitMQ Server中不会因为宕机重启而丢失。 3. 消息确认机制3.1 confirm机制在生产者发送到RabbitMQ Server时有可能因为网络问题导致投递失败，从而丢失数据。我们可以使用confirm模式防止数据丢失。工作流程是怎么样的呢，看以下图解： 从上图中可以看到是通过两个回调函数confirm()、returnedMessage()进行通知。 一条消息从生产者发送到RabbitMQ，首先会发送到Exchange，对应回调函数confirm()。第二步从Exchange路由分配到Queue中，对应回调函数则是returnedMessage()。 代码怎么实现呢，请看演示： 首先在application.yml配置文件中加上如下配置： 123456789spring: rabbitmq: publisher-confirms: true# publisher-returns: true template: mandatory: true# publisher-confirms：设置为true时。当消息投递到Exchange后，会回调confirm()方法进行通知生产者# publisher-returns：设置为true时。当消息匹配到Queue并且失败时，会通过回调returnedMessage()方法返回消息# spring.rabbitmq.template.mandatory: 设置为true时。指定消息在没有被队列接收时会通过回调returnedMessage()方法退回。 有个小细节，publisher-returns和mandatory如果都设置的话，优先级是以mandatory优先。可以看源码： 接着我们需要定义回调方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445@Componentpublic class RabbitmqConfirmCallback implements RabbitTemplate.ConfirmCallback, RabbitTemplate.ReturnCallback &#123; private Logger logger = LoggerFactory.getLogger(RabbitmqConfirmCallback.class); /** * 监听消息是否到达Exchange * * @param correlationData 包含消息的唯一标识的对象 * @param ack true 标识 ack，false 标识 nack * @param cause nack 投递失败的原因 */ @Override public void confirm(CorrelationData correlationData, boolean ack, String cause) &#123; if (ack) &#123; logger.info(\"消息投递成功~消息Id：&#123;&#125;\", correlationData.getId()); &#125; else &#123; logger.error(\"消息投递失败，Id：&#123;&#125;，错误提示：&#123;&#125;\", correlationData.getId(), cause); &#125; &#125; @Override public void returnedMessage(Message message, int replyCode, String replyText, String exchange, String routingKey) &#123; logger.info(\"消息没有路由到队列，获得返回的消息\"); Map map = byteToObject(message.getBody(), Map.class); logger.info(\"message body: &#123;&#125;\", map == null ? \"\" : map.toString()); logger.info(\"replyCode: &#123;&#125;\", replyCode); logger.info(\"replyText: &#123;&#125;\", replyText); logger.info(\"exchange: &#123;&#125;\", exchange); logger.info(\"routingKey: &#123;&#125;\", exchange); logger.info(\"------------&gt; end &lt;------------\"); &#125; @SuppressWarnings(\"unchecked\") private &lt;T&gt; T byteToObject(byte[] bytes, Class&lt;T&gt; clazz) &#123; T t; try (ByteArrayInputStream bis = new ByteArrayInputStream(bytes); ObjectInputStream ois = new ObjectInputStream(bis)) &#123; t = (T) ois.readObject(); &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; return t; &#125;&#125; 我这里就简单地打印回调方法返回的消息，在实际项目中，可以把返回的消息存储到日志表中，使用定时任务进行进一步的处理。 我这里是使用RabbitTemplate进行发送，所以在Service层的RabbitTemplate需要设置一下： 1234567891011121314151617181920212223242526272829303132333435363738394041@Servicepublic class RabbitMQServiceImpl implements RabbitMQService &#123; @Resource private RabbitmqConfirmCallback rabbitmqConfirmCallback; @Resource private RabbitTemplate rabbitTemplate; @PostConstruct public void init() &#123; //指定 ConfirmCallback rabbitTemplate.setConfirmCallback(rabbitmqConfirmCallback); //指定 ReturnCallback rabbitTemplate.setReturnCallback(rabbitmqConfirmCallback); &#125; @Override public String sendMsg(String msg) throws Exception &#123; Map&lt;String, Object&gt; message = getMessage(msg); try &#123; CorrelationData correlationData = (CorrelationData) message.remove(\"correlationData\"); rabbitTemplate.convertAndSend(RabbitMQConfig.RABBITMQ_DEMO_DIRECT_EXCHANGE, RabbitMQConfig.RABBITMQ_DEMO_DIRECT_ROUTING, message, correlationData); return \"ok\"; &#125; catch (Exception e) &#123; e.printStackTrace(); return \"error\"; &#125; &#125; private Map&lt;String, Object&gt; getMessage(String msg) &#123; String msgId = UUID.randomUUID().toString().replace(\"-\", \"\").substring(0, 32); CorrelationData correlationData = new CorrelationData(msgId); String sendTime = sdf.format(new Date()); Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); map.put(\"msgId\", msgId); map.put(\"sendTime\", sendTime); map.put(\"msg\", msg); map.put(\"correlationData\", correlationData); return map; &#125;&#125; 大功告成！接下来我们进行测试，发送一条消息，我们可以控制台：假设发送一条信息没有路由匹配到队列，可以看到如下信息：这就是confirm模式。它的作用是为了保障生产者投递消息到RabbitMQ不会出现消息丢失。 3.2 事务机制(ACK)最开始的那张图已经讲过，消费者从队列中获取到消息后，会直接确认签收，假设消费者宕机或者程序出现异常，数据没有正常消费，这种情况就会出现数据丢失。 所以关键在于把自动签收改成手动签收，正常消费则返回确认签收，如果出现异常，则返回拒绝签收重回队列。 代码怎么实现呢，请看演示： 首先在消费者的application.yml文件中设置事务提交为manual手动模式： 1234567spring: rabbitmq: listener: simple: acknowledge-mode: manual # 手动ack模式 concurrency: 1 # 最少消费者数量 max-concurrency: 10 # 最大消费者数量 然后编写消费者的监听器： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253@Componentpublic class RabbitDemoConsumer &#123; enum Action &#123; &#x2F;&#x2F;处理成功 SUCCESS, &#x2F;&#x2F;可以重试的错误，消息重回队列 RETRY, &#x2F;&#x2F;无需重试的错误，拒绝消息，并从队列中删除 REJECT &#125; @RabbitHandler @RabbitListener(queuesToDeclare &#x3D; @Queue(RabbitMQConfig.RABBITMQ_DEMO_TOPIC)) public void process(String msg, Message message, Channel channel) &#123; long tag &#x3D; message.getMessageProperties().getDeliveryTag(); Action action &#x3D; Action.SUCCESS; try &#123; System.out.println(&quot;消费者RabbitDemoConsumer从RabbitMQ服务端消费消息：&quot; + msg); if (&quot;bad&quot;.equals(msg)) &#123; throw new IllegalArgumentException(&quot;测试：抛出可重回队列的异常&quot;); &#125; if (&quot;error&quot;.equals(msg)) &#123; throw new Exception(&quot;测试：抛出无需重回队列的异常&quot;); &#125; &#125; catch (IllegalArgumentException e1) &#123; e1.printStackTrace(); &#x2F;&#x2F;根据异常的类型判断，设置action是可重试的，还是无需重试的 action &#x3D; Action.RETRY; &#125; catch (Exception e2) &#123; &#x2F;&#x2F;打印异常 e2.printStackTrace(); &#x2F;&#x2F;根据异常的类型判断，设置action是可重试的，还是无需重试的 action &#x3D; Action.REJECT; &#125; finally &#123; try &#123; if (action &#x3D;&#x3D; Action.SUCCESS) &#123; &#x2F;&#x2F;multiple 表示是否批量处理。true表示批量ack处理小于tag的所有消息。false则处理当前消息 channel.basicAck(tag, false); &#125; else if (action &#x3D;&#x3D; Action.RETRY) &#123; &#x2F;&#x2F;Nack，拒绝策略，消息重回队列 channel.basicNack(tag, false, true); &#125; else &#123; &#x2F;&#x2F;Nack，拒绝策略，并且从队列中删除 channel.basicNack(tag, false, false); &#125; channel.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 解释一下上面的代码，如果没有异常，则手动确认回复RabbitMQ服务端basicAck(消费成功)。 如果抛出某些可以重回队列的异常，我们就回复basicNack并且设置重回队列。 如果是抛出不可重回队列的异常，就回复basicNack并且设置从RabbitMQ的队列中删除。 接下来进行测试，发送一条普通的消息”hello”： 解释一下ack返回的三个方法的意思。 ①成功确认 1void basicAck(long deliveryTag, boolean multiple) throws IOException; 消费者成功处理后调用此方法对消息进行确认。 deliveryTag：该消息的index multiple：是否批量.。true：将一次性ack所有小于deliveryTag的消息。 ②失败确认 1void basicNack(long deliveryTag, boolean multiple, boolean requeue) throws IOException; deliveryTag：该消息的index。 multiple：是否批量。true：将一次性拒绝所有小于deliveryTag的消息。 requeue：被拒绝的是否重新入队列。 ③失败确认 1void basicReject(long deliveryTag, boolean requeue) throws IOException; deliveryTag:该消息的index。 requeue：被拒绝的是否重新入队列。basicNack()和basicReject()的区别在于：basicNack()可以批量拒绝，basicReject()一次只能拒接一条消息。 4. 遇到的坑4.1 启用nack机制后，导致的死循环上面的代码我故意写了一个bug。测试发送一条”bad”，然后会抛出重回队列的异常。这就有个问题：重回队列后消费者又消费，消费抛出异常又重回队列，就造成了死循环。 那怎么避免这种情况呢？ 既然nack会造成死循环的话，我提供的一个思路是不使用basicNack()，把抛出异常的消息落库到一张表中，记录抛出的异常，消息体，消息Id。通过定时任务去处理。 如果你有什么好的解决方案，也可以留言讨论~ 4.2 double ack有的时候比较粗心，不小心开启了自动Ack模式，又手动回复了Ack。那就会报这个错误： 123消费者RabbitDemoConsumer从RabbitMQ服务端消费消息：java技术爱好者2020-08-02 22:52:42.148 ERROR 4880 --- [ 127.0.0.1:5672] o.s.a.r.c.CachingConnectionFactory : Channel shutdown: channel error; protocol method: #method&lt;channel.close&gt;(reply-code&#x3D;406, reply-text&#x3D;PRECONDITION_FAILED - unknown delivery tag 1, class-id&#x3D;60, method-id&#x3D;80)2020-08-02 22:52:43.102 INFO 4880 --- [cTaskExecutor-1] o.s.a.r.l.SimpleMessageListenerContainer : Restarting Consumer@f4a3a8d: tags&#x3D;[&#123;amq.ctag-8MJeQ7el_PNbVJxGOOw7Rw&#x3D;rabbitmq.demo.topic&#125;], channel&#x3D;Cached Rabbit Channel: AMQChannel(amqp:&#x2F;&#x2F;guest@127.0.0.1:5672&#x2F;,5), conn: Proxy@782a1679 Shared Rabbit Connection: SimpleConnection@67c5b175 [delegate&#x3D;amqp:&#x2F;&#x2F;guest@127.0.0.1:5672&#x2F;, localPort&#x3D; 56938], acknowledgeMode&#x3D;AUTO local queue size&#x3D;0 出现这个错误，可以检查一下yml文件是否添加了以下配置： 1234567spring: rabbitmq: listener: simple: acknowledge-mode: manual concurrency: 1 max-concurrency: 10 如果上面这个配置已经添加了，还是报错，有可能你使用@Configuration配置了SimpleRabbitListenerContainerFactory，根据SpringBoot的特性，代码优于配置，代码的配置覆盖了yml的配置，并且忘记设置手动manual模式： 12345678@Beanpublic SimpleRabbitListenerContainerFactory rabbitListenerContainerFactory(ConnectionFactory connectionFactory) &#123; SimpleRabbitListenerContainerFactory factory = new SimpleRabbitListenerContainerFactory(); factory.setConnectionFactory(connectionFactory); //设置手动ack模式 factory.setAcknowledgeMode(AcknowledgeMode.MANUAL); return factory;&#125; 如果你还是有报错，那可能是写错地方了，写在生产者的项目了。以上的配置应该配置在消费者的项目。因为ack模式是针对消费者而言的。我就是写错了，写在生产者，折腾了几个小时，泪目~ 4.3 性能问题其实手动ACK相对于自动ACK肯定是会慢很多，我在网上查了一些资料，性能相差大概有10倍。所以一般在实际应用中不太建议开手动ACK模式。不过也不是绝对不可以开，具体情况具体分析，看并发量，还有数据的重要性等等。 所以在实际项目中还需要权衡一下并发量和数据的重要性，再决定具体的方案。 4.4 启用手动ack模式，如果没有及时回复，会造成队列异常如果开启了手动ACK模式，但是由于代码有bug的原因，没有回复RabbitMQ服务端，那么这条消息就会放到Unacked状态的消息堆里，只有等到消费者的连接断开才会转到Ready消息。如果消费者一直没有断开连接，那Unacked的消息就会越来越多，占用内存就越来越大，最后就会出现异常。 5. 总结通过上面的学习后，总结了RabbitMQ防止数据丢失有三种方式： 消息持久化 生产者消息确认机制(confirm模式) 消费者消息确认模式(ack模式) 本文转载自Java架构师联盟头条，原文连接点这里，版权归原作者所有，转载目的是为了避免以后找不到。","categories":[{"name":"MQ","slug":"MQ","permalink":"http://huermosi.xyz/categories/MQ/"},{"name":"RabbitMQ","slug":"MQ/RabbitMQ","permalink":"http://huermosi.xyz/categories/MQ/RabbitMQ/"}],"tags":[{"name":"rabbitmq","slug":"rabbitmq","permalink":"http://huermosi.xyz/tags/rabbitmq/"}]},{"title":"一时语噻：二面鹅厂，面试官问出Nginx你了解吗","slug":"nginx/nginx1","date":"2020-03-01T14:40:14.000Z","updated":"2021-04-26T12:37:27.917Z","comments":true,"path":"2020/952720210401887300000/","link":"","permalink":"http://huermosi.xyz/2020/952720210401887300000/","excerpt":"","text":"前天二面鹅厂，面试官问出了“nginx你了解吗？”这样宽泛直白的句式，我一时抓不到重点，一时语噻。下班想了一下，平时潜移默化用到不少nginx的能力，但在面试的时候没有吹成对应的概念。 面谈nginx核心能力nginx是老牌web服务器，以下口水话的nginx基础能力，大家都耳熟能详，看看就行，面试官也不打算考查这个。 高并发连接: 官方称单节点支持5万并发连接数，实际生产环境能够承受2-3万并发。内存消耗少: 在3万并发连接下，开启10个nginx进程仅消耗150M内存 (15M×10=150M)配置简单成本低廉: 开源免费 1. 正向、反向代理所谓“代理”，是指在内网边缘 设置一个硬件/软件转发请求；“正向”还是“反向”的说法，取决于转发的是”出站请求”还是”入站请求”. 正向代理：处理来自客户端的出站请求，将其转发到Internet，然后将生成的响应返回给客户端。反向代理：处理来自Internet的入站请求，将其转发给后端工作程序，然后将响应返回给Internet。 正向代理和反向代理 在代理的方向上不同，但都会代理处理HTTP请求/响应。 代理服务器存在的目的： 堡垒机/隔离内网： 内网客户端无法访问外网需要设置堡垒机、隐藏内网工作服务器 代理服务器附加功能： 对流量执行操作、使用缓存或压缩来提高性能、防御攻击并过滤信息 2. 负载均衡负载均衡一般伴随着反向代理， 起到了 分配流量、透明代理、 增强容错的效果 123456789101112131415http &#123; upstream myapp1 &#123; server srv1.example.com; server srv2.example.com; server srv3.example.com; &#125; server &#123; listen 80; location &#x2F; &#123; proxy_pass http:&#x2F;&#x2F;myapp1; &#125; &#125;&#125; 早期我们的核心产品部署在两台windows Sever IIS上，前面部署了一个nginx，做了负载均衡。 很明显，这里有个[负载均衡策略]的知识点。 round-robin 顾名思义：轮询least-connected ： 下一个请求将发往最小活动链接的服务器ip-hash： 根据客户端的ip地址和哈希函数 决定将请求发往哪个服务器http://nginx.org/en/docs/http/load_balancing.html 延伸技能点： [服务发现]:在容器/K8S环境，服务地址是由集群系统动态分配，一般都内置了服务发现能力，docker-comppose/K8s中定义的服务名就代表了整个服务。有个文章讲述了: 《巧用nginx 实现Docker-Comppose服务多实例》 [会话亲和力]：又叫“粘性会话”，确保在有状态的应用中，同一客户端的请求打到后端一个服务器上。也有个示例可参考：《巧用会话亲和力做图片上传和预览》 3. 动静分离动静分离与现在火热的前后端分离概念火热相关， 前端可以自行开发、测试，自行使用nginx形成静态资源服务器，后端服务仅作为附加资源。下面的例子表明 静态资源在/usr/share/nginx/html， 动态资源路径包含api或swagger。 1234567891011121314151617181920upstream eap_website &#123; server eapwebsite;&#125;server &#123; listen 80; location &#x2F; &#123; # 静态资源 root &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html; index index.html index.htm; try_files $uri &#x2F;index.html; &#125; location ^~ &#x2F;api&#x2F; &#123; # 动态资源 proxy_pass http:&#x2F;&#x2F;eap_website&#x2F;api&#x2F;; &#125; location ^~ &#x2F;swagger&#x2F; &#123; # 动态资源 proxy_pass http:&#x2F;&#x2F;eap_website&#x2F;swagger&#x2F;; &#125;&#125; 延伸技能点 以上流程也是《现代十二要素应用方法论》所推崇的第四点， 从这个体系来说，后端沦落为api开发，实属遗憾 这里有个《有关在容器生成阶段动态插入api基地址的妙招》，对动静分离的容器化很有裨益。 实用功能 通过端口支持 同一域名下多个webapp 绑定Https证书1、2点一起体现：一个域名绑定到443和8080端口两个https站点 12345678910111213141516171819202122232425262728upstream receiver_server &#123; server receiver:80;&#125;upstream app_server &#123; server app:80;&#125;server &#123; listen 443 ssl http2; server_name eqid.gridsum.com; ssl_certificate &#x2F;conf.crt&#x2F;live&#x2F;gridsum.com.crt; ssl_certificate_key &#x2F;conf.crt&#x2F;live&#x2F;gridsum.com.key; location &#x2F; &#123; proxy_pass http:&#x2F;&#x2F;receiver_server&#x2F;; &#125;&#125;server &#123; listen 8080 ssl http2; server_name eqid.gridsum.com:8080; ssl_certificate &#x2F;conf.crt&#x2F;live&#x2F;gridsum.com.crt; ssl_certificate_key &#x2F;conf.crt&#x2F;live&#x2F;gridsum.com.key; location &#x2F; &#123; proxy_pass http:&#x2F;&#x2F;app_server&#x2F;; &#125;&#125; 支持rewrite重写规则: 能够根据域名、url的不同，将http请求分发到后端不同的应用服务器节点上。 内置健康检查功能: 如果后端的某一应用节点挂了，请求不会再转发给这个节点，不影响线上功能。关键指令： max_fails, fail_timeout 1234567upstream backend &#123; server backend1.example.com weight&#x3D;5; server 127.0.0.1:8080 max_fails&#x3D;3 fail_timeout&#x3D;30s; server unix:&#x2F;tmp&#x2F;backend3; server backup1.example.com backup;&#125; 节省带宽: 支持gzip压缩 解决跨域问题① 反向代理② 增加CORS响应头5、6点一起体现： 在前后端分离项目，对跨域请求增加CORS响应头、对静态资源开启gzip压缩 12345678910111213location &#x2F; &#123; gzip on; gzip_types application&#x2F;javascript text&#x2F;css image&#x2F;jpeg; root &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html; index index.html index.htm; try_files $uri &#x2F;index.html; add_header &#39;Access-Control-Allow-Origin&#39; &#39;*&#39;; add_header &#39;Access-Control-Allow-Methods&#39; &#39;GET, POST, OPTIONS, PUT, DELETE&#39;; add_header &#39;Access-Control-Allow-Headers&#39; &#39;Content-Type&#39;; add_header &#39;Access-Control-Allow-Credentials&#39; &#39;true&#39;;&#125; 本文转载自IT互联网新资讯头条，原文连接点这里，版权归原作者所有，转载目的是为了避免以后找不到。","categories":[{"name":"更多","slug":"更多","permalink":"http://huermosi.xyz/categories/%E6%9B%B4%E5%A4%9A/"},{"name":"NGINX","slug":"更多/NGINX","permalink":"http://huermosi.xyz/categories/%E6%9B%B4%E5%A4%9A/NGINX/"}],"tags":[{"name":"NGINX","slug":"NGINX","permalink":"http://huermosi.xyz/tags/NGINX/"},{"name":"代理","slug":"代理","permalink":"http://huermosi.xyz/tags/%E4%BB%A3%E7%90%86/"},{"name":"反向代理","slug":"反向代理","permalink":"http://huermosi.xyz/tags/%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86/"}]},{"title":"一条简单的更新语句，MySQL是如何加锁的？","slug":"mysql/mysql-statement-lock","date":"2020-02-11T14:13:46.000Z","updated":"2021-04-08T08:49:31.668Z","comments":true,"path":"2020/95272021213546/","link":"","permalink":"http://huermosi.xyz/2020/95272021213546/","excerpt":"","text":"看如下一条sql语句： 12# table T (id int, name varchar(20))delete from T where id = 10; MySQL在执行的过程中，是如何加锁呢？再看下面这条语句： 1select * from T where id = 10; 那这条语句呢？其实这其中包含太多知识点了。要回答这两个问题，首先需要了解一些知识。 1. 相关知识介绍多版本并发控制在MySQL默认存储引擎InnoDB中，实现的是基于多版本的并发控制协议——MVCC（Multi-Version Concurrency Control）（注：与MVVC相对的，是基于锁的并发控制，Lock-Based Concurrency Control）。其中MVCC最大的好处是：读不加锁，读写不冲突。在读多写少的OLTP应用中，读写不冲突是非常重要的，极大的提高了系统的并发性能，在现阶段，几乎所有的RDBMS，都支持MVCC。其实，MVCC就一句话总结：同一份数据临时保存多个版本的一种方式，进而实现并发控制。 当前读和快照读在MVCC并发控制中，读操作可以分为两类：快照读与当前读。 快照读（简单的select操作）：读取的是记录中的可见版本（可能是历史版本），不用加锁。这你就知道第二个问题的答案了吧。 当前读（特殊的select操作、insert、delete和update）：读取的是记录中最新版本，并且当前读返回的记录都会加上锁，这样保证了了其他事务不会再并发修改这条记录。 聚集索引也叫做聚簇索引。在InnoDB中，数据的组织方式就是聚簇索引：完整的记录，储存在主键索引中，通过主键索引，就可以获取记录中所有的列。 最左前缀原则也就是最左优先，这条原则针对的是组合索引和前缀索引，理解： 1、在MySQL中，进行条件过滤时，是按照向右匹配直到遇到范围查询（&gt;,&lt;,between,like）就停止匹配，比如说a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a, b, c, d)顺序的索引，d是用不到索引的，如果建立(a, b, d, c)索引就都会用上，其中a，b，d的顺序可以任意调整。 2、= 和 in 可以乱序，比如 a = 1 and b = 2 and c = 3 建立(a, b, c)索引可以任意顺序，MySQL的查询优化器会优化索引可以识别的形式。 两阶段锁传统的RDMS加锁的一个原则，就是2PL(Two-Phase Locking，二阶段锁)。也就是说锁操作分为两个阶段：加锁阶段和解锁阶段，并且保证加锁阶段和解锁阶段不想交。也就是说在一个事务中，不管有多少条增删改，都是在加锁阶段加锁，在 commit 后，进入解锁阶段，才会全部解锁。 隔离级别MySQL/InnoDB中，定义了四种隔离级别： Read Uncommitted：可以读取未提交记录。此隔离级别不会使用。 Read Committed（RC）：针对当前读，RC隔离级别保证了对读取到的记录加锁（记录锁），存在幻读现象。 Repeatable Read（RR）：针对当前读，RR隔离级别保证对读取到的记录加锁（记录锁），同时保证对读取的范围加锁，新的满足查询条件的记录不能够插入（间隙锁），不存在幻读现象。 Serializable：从MVCC并发控制退化为基于锁的并发控制。不区别快照读和当前读，所有的读操作都是当前读，读加读锁（S锁），写加写锁（X锁）。在该隔离级别下，读写冲突，因此并发性能急剧下降，在MySQL/InnoDB中不建议使用。 Gap锁和Next-Key锁在InnoDB中完整行锁包含三部分： 记录锁（Record Lock）：记录锁锁定索引中的一条记录。 间隙锁（Gap Lock）：间隙锁要么锁住索引记录中间的值，要么锁住第一个索引记录前面的值或最后一个索引记录后面的值。 Next-Key Lock：Next-Key锁时索引记录上的记录锁和在记录之前的间隙锁的组合。 2. 进行分析了解完以上的小知识点，我们开始分析第一个问题。当看到这个问题的时候，你可能会毫不犹豫的说，加写锁啊。这答案也错也对，因为已知条件太少。那么有那些需要已知的前提条件呢？ 前提一：id列是不是主键？前提二：当前系统的隔离级别是什么？前提三：id列如果不是主键，那么id列上有没有索引呢？前提四：id列上如果有二级索引，那么是唯一索引吗？前提五：SQL执行计划是什么？索引扫描？还是全表扫描根据上面的前提条件，可以有九种组合，当然还没有列举完全。 id列是主键，RC隔离级别id列是二级唯一索引，RC隔离级别id列是二级不唯一索引，RC隔离级别id列上没有索引，RC隔离级别id列是主键，RR隔离级别id列是二级唯一索引，RR隔离级别id列是二级不唯一索引，RR隔离级别id列上没有索引，RR隔离级别 组合一：id主键 + RC这个组合是分析最简单的，到执行该语句时，只需要将主键id = 10的记录加上X锁。如下图所示： 结论：id是主键是，此SQL语句只需要在id = 10这条记录上加上X锁即可。 组合二：id唯一索引 + RC这个组合，id不是主键，而是一个Unique的二级索引键值。在RC隔离级别下，是怎么加锁的呢？看下图： 由于id是Unique索引，因此delete语句会选择走id列的索引进行where条件过滤，在找到id = 10的记录后，首先会将Unique索引上的id = 10的记录加上X锁，同时，会根据读取到的name列，回到主键索引（聚簇索引），然后将聚簇索引上的name = ‘e’ 对应的主键索引项加X锁。 结论：若id列是Unique列，其上有Unique索引，那么SQL需要加两个X锁，一个对应于id Unique索引上的id = 10的记录，另一把锁对应于聚簇索引上的(name = ‘e’, id = 10)的记录。 组合三：id不唯一索引+RC该组合中，id列不在唯一，而是个普通索引，那么当执行sql语句时，MySQL又是如何加锁呢？看下图： 由上图可以看出，首先，id列索引上，满足id = 10查询的记录，均加上X锁。同时，这些记录对应的主键索引上的记录也加上X锁。与组合er的唯一区别，组合二最多只有一个满足条件的记录，而在组合三中会将所有满足条件的记录全部加上锁。 结论：若id列上有非唯一索引，那么对应的所有满足SQL查询条件的记录，都会加上锁。同时，这些记录在主键索引上也会加上锁。 组合四：id无索引+RC相对于前面的组合，该组合相对特殊，因为id列上无索引，所以在 where id = 10 这个查询条件下，没法通过索引来过滤，因此只能全表扫描做过滤。对于该组合，MySQL又会进行怎样的加锁呢？看下图： 由于id列上无索引，因此只能走聚簇索引，进行全表扫描。由图可以看出满足条件的记录只有两条，但是，聚簇索引上的记录都会加上X锁。但在实际操作中，MySQL进行了改进，在进行过滤条件时，发现不满足条件后，会调用 unlock_row 方法，把不满足条件的记录放锁（违背了2PL原则）。这样做，保证了最后满足条件的记录加上锁，但是每条记录的加锁操作是不能省略的。 结论：若id列上没有索引，MySQL会走聚簇索引进行全表扫描过滤。由于是在MySQl Server层面进行的。因此每条记录无论是否满足条件，都会加上X锁，但是，为了效率考虑，MySQL在这方面进行了改进，在扫描过程中，若记录不满足过滤条件，会进行解锁操作。同时优化违背了2PL原则。 组合五：id主键+RR该组合为id是主键，Repeatable Read隔离级别，针对于上述的SQL语句，加锁过程和组合一（id主键+RC）一致。 组合六：id唯一索引+RR该组合与组合二的加锁过程一致。 组合七：id不唯一索引+RR在组合一到组合四中，隔离级别是Read Committed下，会出现幻读情况，但是在该组合Repeatable Read级别下，不会出现幻读情况，这是怎么回事呢？而MySQL又是如何给上述语句加锁呢？看下图： 该组合和组合三看起来很相似，但差别很大，在改组合中加入了一个间隙锁（Gap锁）。这个Gap锁就是相对于RC级别下，RR级别下不会出现幻读情况的关键。实质上，Gap锁不是针对于记录本身的，而是记录之间的Gap。所谓幻读，就是同一事务下，连续进行多次当前读，且读取一个范围内的记录(包括直接查询所有记录结果或者做聚合统计), 发现结果不一致(标准档案一般指记录增多, 记录的减少应该也算是幻读)。 那么该如何解决这个问题呢？如何保证多次当前读返回一致的记录，那么就需要在多个当前读之间，其他事务不会插入新的满足条件的记录并提交。为了实现该结果，Gap锁就应运而生。 如图所示，有些位置可以插入新的满足条件的记录，考虑到B+树的有序性，满足条件的记录一定是具有连续性的。因此会在 [4, b], [10, c], [10, d], [20, e] 之间加上Gap锁。 Insert操作时，如insert(10, aa)，首先定位到 [4, b], [10, c]间，然后插入在插入之前，会检查该Gap是否加锁了，如果被锁上了，则Insert不能加入记录。因此通过第一次当前读，会把满足条件的记录加上X锁，还会加上三把Gap锁，将可能插入满足条件记录的3个Gap锁上，保证后续的Insert不能插入新的满足 id = 10 的记录，也就解决了幻读问题。 而在组合五，组合六中，同样是RR级别，但是不用加上Gap锁，在组合五中id是主键，组合六中id是Unique键，都能保证唯一性。一个等值查询，最多只能返回一条满足条件的记录，而且新的相同取值的记录是无法插入的。 结论：在RR隔离级别下，id列上有非唯一索引，对于上述的SQL语句；首先，通过id索引定位到第一条满足条件的记录，给记录加上X锁，并且给Gap加上Gap锁，然后在主键聚簇索引上满足相同条件的记录加上X锁，然后返回；之后读取下一条记录重复进行。直至第一条出现不满足条件的记录，此时，不需要给记录加上X锁，但是需要给Gap加上Gap锁吗，最后返回结果。 组合八：id无索引+RR该组合中，id列上无索引，只能进行全表扫描，那么该如何加锁，看下图： 如图，可以看出这是一个很恐怖的事情，全表每条记录要加X锁，每个Gap加上Gap锁，如果表上存在大量数据时，又是什么情景呢？这种情况下，这个表，除了不加锁的快照读，其他任何加锁的并发SQL，均不能执行，不能更新，删除，插入，这样，全表锁死。 当然，和组合四一样，MySQL进行了优化，就是semi-consistent read。semi-consistent read开启的情况下，对于不满足条件的记录，MySQL会提前放锁，同时Gap锁也会释放。而semi-consistent read是如何触发：要么在Read Committed隔离级别下；要么在Repeatable Read隔离级别下，设置了innodb_locks_unsafe_for_binlog 参数。 结论：在Repeatable Read隔离级别下，如果进行全表扫描的当前读，那么会锁上表上的所有记录，并且所有的Gap加上Gap锁，杜绝所有的 delete/update/insert 操作。当然在MySQL中，可以触发 semi-consistent read来缓解锁开销与并发影响，但是semi-consistent read本身也会带来其他的问题，不建议使用。 组合九：Serializable在最后组合中，对于上诉的删除SQL语句，加锁过程和组合八一致。但是，对于查询语句（比如select * from T1 where id = 10）来说，在RC，RR隔离级别下，都是快照读，不加锁。在Serializable隔离级别下，无论是查询语句也会加锁，也就是说快照读不存在了，MVCC降级为Lock-Based CC。 3. 结论在MySQL/InnoDB中，所谓的读不加锁，并不适用于所有的情况，而是和隔离级别有关。在Serializable隔离级别下，所有的操作都会加锁。 一条简单的删除语句加锁情况也就分析完成了，但是学习不止于此，还在继续，对于复杂SQL语句又是如何加锁的呢？MySQL中的索引的分析又是怎样的呢？性能分析、性能优化这些又是怎么呢？还需要进一步的学习探索。 本文转载自Java技术架构头条，原文连接点这里，版权归原作者所有，转载目的是为了避免以后找不到。","categories":[{"name":"MYSQL","slug":"MYSQL","permalink":"http://huermosi.xyz/categories/MYSQL/"}],"tags":[{"name":"MYSQL","slug":"MYSQL","permalink":"http://huermosi.xyz/tags/MYSQL/"}]},{"title":"线上MySQL读写分离，出现写完读不到问题如何解决？","slug":"mysql/mysql-read-write-split-problem","date":"2020-02-01T14:27:22.000Z","updated":"2021-04-08T08:49:31.660Z","comments":true,"path":"2020/9527202104014223000/","link":"","permalink":"http://huermosi.xyz/2020/9527202104014223000/","excerpt":"","text":"今天我们来详细了解一下主从同步延迟时读写分离发生写后读不到的问题，依次讲解问题出现的原因，解决策略以及 Sharding-jdbc、MyCat 和 MaxScale 等开源数据库中间件具体的实现方案。 1. 写后读不到问题MySQL 经典的一主两从三节点架构是大多数创业公司初期使用的主流数据存储方案之一，主节点处理写操作，两个从节点处理读操作，分摊了主库的压力。 但是，有时候可能会遇到执行完写操作后，立刻去读发现读不到或者读到旧状态的尴尬场景。这是由于主从同步可能存在延迟，在主节点执行完写操作，再去从节点执行读操作，读取了之前旧的状态。 上图展示了此类问题出现的操作顺序示意图： 客户端首先通过代理向主节点 Master 进行了写入操作 紧接着第二步去从节点 Slave A 执行读操作，此时 Master 和 Slave A 之间的同步还未完成，所以第二步的读操作读取到了旧状态 当第五步再次进行读操作时，此时同步已经完成，所以可以从 Slave B 中读取到正确的状态。 下面，我们就来看一下为什么会出现此类问题。 2. MySQL 主从同步理解问题背后发生的原因，才能更好的解决问题。MySQL 主从复制的过程大致如下图所示，本篇文章只讲解同步过程中的流程，建立同步连接和失联重传不是重点，暂不讲解，感兴趣的同学可以自行了解。 MySQL 主从复制，涉及主从两个节点，一共四个四个线程参与其中： 主节点的 Client Thread，处理客户端请求的线程，执行如图所示的1~5步骤，2，3，4步骤是为了保证数据的一致性和尽量减少丢失，第三步骤时会通知 Dump Thread； 主节点的 Dump Thread，接收到 Client Thread 通知后，负责读取本地的 binlog 的数据，将 binlog 数据，binlog 文件名 以及当前发送 binlog 的位置信息发送给从节点； 从节点的 IO Thread 负责接收 Dump Thread 发送的 binlog 数据和相关位置信息，将其追加到本地的 relay log 等文件中； 从节点的 SQL Thread 检测到 relay log 追加了新数据，则解析其内容(其实就是解析 binlog 文件的内容)为可以执行的 SQL 语句，然后在本地数据执行，并记录下当前执行的 relay log 位置。 上述是默认的异步同步模式，我们发现，从主节点提交成功到从节点同步完成，中间间隔了6，7，8，9，10多个步骤，涉及到一次网络传输，多次文件读取和写入的磁盘 IO 操作，以及最后的 SQL 执行的 CPU 操作。 所以，当主从节点间网络传输出现问题，或者从节点性能较低时，主从节点间的同步就会出现延迟，导致文章一开始提及的写后读不到的问题。在高并发场景，从节点一般要过几十毫秒，甚至几百毫秒才能读到最新的状态。 3. 常见的解决策略一般来讲，大致有如下方案解决写后读不出问题： 强制走主库 判断主备无延迟 等主库位点或 GTID 方案 强制走主库强制走主库方案最容易理解和实现，它也是最常用的方案。顾名思义，它就是强制让部分必须要读到最新状态的读操作去主节点执行，这样就不会出现写后读不出问题。这种方案问题在于将一部分读压力给了主节点，部分破化了读写分离的目的，降低了整个系统的扩展性。 一般主流的数据库中间件都提供了强制走主库的机制，比如，在 sharding-jdbc 中，可以使用 Hint 来强制路由主库。 123HintManager hintManager = HintManager.getInstance();hintManager.setMasterRouteOnly();// 继续JDBC操作 它的原理就是在 SQL 语句前添加 Hint，然后数据库中间件会识别出 Hint，将其路由到主节点。 下面，我们就来看一下如果要去从库查询，并且要避免过期读的方案，并分析各个方案的优缺点。 判断主备无延迟第二种方案是使用 show slave status 语句结果中的部分值来判断主从同步的延迟时间： 1234567891011&gt; show slave status*************************** 1. row ***************************Master_Log_File: mysql-bin.001822Read_Master_Log_Pos: 290072815Seconds_Behind_Master: 2923Relay_Master_Log_File: mysql-bin.001821Exec_Master_Log_Pos: 256529431Auto_Position: 0Retrieved_Gtid_Set: Executed_Gtid_Set: ..... seconds_behind_master，表示落后主节点秒数，如果此值为0，则表示主从无延迟 Master_Log_File 和 Read_Master_Log_Pos，表示的是读到的主库的最新位点，Relay_Master_Log_File 和 Exec_Master_Log_Pos，表示的是备库执行的最新位点。如果这两组值相等，则表示主从无延迟 Auto_Position=1 ，表示使用了 GTID 协议，并且备库收到的所有日志的 GTID 集合 Retrieved_Gtid_Set 和 执行完成的 GTID 集合 Executed_Gtid_Set 相等，则表示主从无延迟。 在进行读操作前，先根据上述方式来判断主从是否有延迟，如果有延迟，则一直等待到无延迟后执行。但是这类方案在判断是否有延迟时存在着假阳和假阴的问题： 判断无延迟，其他延迟了。因为上述判断是基于从节点的状态，当主节点的 Dump Thread 尚未将最新状态发送给从节点的 IO SQL 时，从节点可能会错误的判断自己和主节点无延迟。 判断有延迟，但是读操作读取的最新状态已经同步。因为 MySQL主从复制是一直在进行的，写后直接读的同时可能还有其他无关写操作，虽然主从有延迟，但是对于第一次写操作的同步已经完成，所以读操作已经可以读到最新的状态。 对于第一个问题，需要使用主从复制的 semi-sync 模式，上文中讲解介绍的是默认的异步模式，semi-sync 模式的流程如下图所示： 当主节点事务提交的时候，Dump Thread 把 binlog 发给从节点； 从节点的 IO Thread 收到 binlog 以后，发回给主节点一个 ack，表示收到了； 主节点的 Dump Thread 收到这个 ack 以后，再通知 Client Thread ，此时才能给客户端返回执行成功的响应。 这样，写操作执行后，就确保从节点已经读取到主节点发送的 binglog 数据，即 Master_Log_File、 Read_Master_Log_Pos 或 Retrieved_Gtid_Set 是最新的，这样才能与执行的相关数据进行对比，判断是否有延迟。 可惜的是，上述 semi-sync 模式只需要等待一个从节点的ACK，所以一主多从的模式该方案将会无效。 虽然该方案有种种问题，但是对于一致性要求不那么高的场景也能适用，比如 MyCat 就是用 seconds_behind_master 是否落后主节点过多，如果超过一定阈值，就将其从有效从节点列表中删除，不再将读请求路由到它身上。 在 MyCAT 的用于监听从节点状态，发送心跳的 MySQLDetector 类中，它会读取从节点的 seconds_behind_master，如果其值大于配置的 slaveThreshold，则将打印日志，并将延迟时间设置到心跳信息中。 123456789101112String Seconds_Behind_Master = resultResult.get( \"Seconds_Behind_Master\"); if (null == Seconds_Behind_Master )&#123; MySQLHeartbeat.LOGGER.warn(\"Master is down but its relay log is clean.\"); heartbeat.setSlaveBehindMaster(0);&#125;else if(!\"\".equals(Seconds_Behind_Master)) &#123; int Behind_Master = Integer.parseInt(Seconds_Behind_Master); if ( Behind_Master &gt; source.getHostConfig().getSlaveThreshold() ) &#123; MySQLHeartbeat.LOGGER.warn(\"found MySQL master/slave Replication delay !!! \" + heartbeat.getSource().getConfig() + \", binlog sync time delay: \" + Behind_Master + \"s\" ); &#125; heartbeat.setSlaveBehindMaster( Behind_Master );&#125; 下面，我们就介绍能够解决第二个问题的方案，即判断有延迟，但是读操作读取的特定最新状态已经同步。 等GTID 方案首先介绍一下 GTID，也就是全局事务 ID，是一个事务在提交的时候生成的，是这个事务的唯一标识。它由MySQL 实例的uuid和一个整数组成，该整数由该实例维护，初始值是 1，每次该实例提交事务后都会加一。 MySQL 提供了一条基于 GTID 的命令，用于在从节点上执行，等待从库同步到了对应的 GTID（binlog文件中会包含 GTID），或者超时返回。 1select wait_for_executed_gtid_set(gtid_set, timeout); MySQL 在执行完事务后，会将该事务的 GTID 会给客户端，然后客户端可以使用该命令去要执行读操作的从库中执行，等待该 GTID，等待成功后，再执行读操作；如果等待超时，则去主库执行读操作，或者再换一个从库执行上述流程。 MariaDB 的 MaxScale 就是使用该方案，MaxScale 是 MariaDB 开发的一个数据库智能代理服务(也支持 MySQL)，允许根据数据库 SQL 语句将请求转向目标一个到多个服务器，可设定各种复杂程度的转向规则。 MaxScale 在其 readwritesplit.hh 头文件和 rwsplit_causal_reads.cc 文件中的 add_prefix_wait_gtid 函数中使用了上述方案。 12345678910#define MYSQL_WAIT_GTID_FUNC &quot;WAIT_FOR_EXECUTED_GTID_SET&quot;static const char gtid_wait_stmt[] &#x3D; &quot;SET @maxscale_secret_variable&#x3D;(SELECT CASE WHEN %s(&#39;%s&#39;, %s) &#x3D; 0 &quot; &quot;THEN 1 ELSE (SELECT 1 FROM INFORMATION_SCHEMA.ENGINES) END);&quot;;GWBUF* RWSplitSession::add_prefix_wait_gtid(uint64_t version, GWBUF* origin) &#123; .... snprintf(prefix_sql, prefix_len, gtid_wait_stmt, wait_func, gtid_position.c_str(), gtid_wait_timeout); ....&#125; 举个例子，原来要执行读操作的 SQL 和添加了前缀的 SQL 如下所示： 12SELECT * FROM `city`;SET @maxscale_secret_variable=(SELECT CASE WHEN WAIT_FOR_EXECUTED_GTID_SET('232-1-1', 10) = 0 THEN 1 ELSE (SELECT 1 FROM INFORMATION_SCHEMA.ENGINES) END); SELECT * FROM `city`; 当WAIT_FOR_EXECUTED_GTID_SET 执行失败后，原 SQL 就不会再执行，而是将该 SQL 去主节点执行。 本文转载自代码小当家头条，原文连接点这里，版权归原作者所有，转载目的是为了避免以后找不到。","categories":[{"name":"MYSQL","slug":"MYSQL","permalink":"http://huermosi.xyz/categories/MYSQL/"}],"tags":[{"name":"MYSQL","slug":"MYSQL","permalink":"http://huermosi.xyz/tags/MYSQL/"},{"name":"读写分离","slug":"读写分离","permalink":"http://huermosi.xyz/tags/%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/"},{"name":"复制","slug":"复制","permalink":"http://huermosi.xyz/tags/%E5%A4%8D%E5%88%B6/"},{"name":"主从同步","slug":"主从同步","permalink":"http://huermosi.xyz/tags/%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5/"}]},{"title":"单点登录SSO方法之Spring Security JWT实现","slug":"springsecurity/springsecurity-jwt-sso","date":"2020-01-01T13:56:05.000Z","updated":"2021-04-08T08:49:31.753Z","comments":true,"path":"2020/95272021042143/","link":"","permalink":"http://huermosi.xyz/2020/95272021042143/","excerpt":"","text":"1. 什么是单点登陆单点登录（Single Sign On），简称为 SSO，是目前比较流行的企业业务整合的解决方案之一。SSO的定义是在多个应用系统中，用户只需要登录一次就可以访问所有相互信任的应用系统 2. 简单的运行机制单点登录的机制其实是比较简单的，用一个现实中的例子做比较。某公园内部有许多独立的景点，游客可以在各个景点门口单独买票。对于需要游玩所有的景点的游客，这种买票方式很不方便，需要在每个景点门口排队买票，钱包拿 进拿出的，容易丢失，很不安全。于是绝大多数游客选择在大门口买一张通票（也叫套票），就可以玩遍所有的景点而不需要重新再买票。他们只需要在每个景点门 口出示一下刚才买的套票就能够被允许进入每个独立的景点。单点登录的机制也一样，如下图所示: 用户认证：这一环节主要是用户向认证服务器发起认证请求，认证服务器给用户返回一个成功的令牌token，主要在认证服务器中完成，即图中的认证系统，注意认证系统只能有一个。身份校验：这一环节是用户携带token去访问其他服务器时，在其他服务器中要对token的真伪进行检验，主要在资源服务器中完成，即图中的应用系统2 3 3. JWT介绍概念说明从分布式认证流程中，我们不难发现，这中间起最关键作用的就是token，token的安全与否，直接关系到系统的健壮性，这里我们选择使用JWT来实现token的生成和校验。JWT，全称JSON Web Token，官网地址，是一款出色的分布式身份校验方案。可以生成token，也可以解析检验token。 JWT生成的token由三部分组成： 头部：主要设置一些规范信息，签名部分的编码格式就在头部中声明。 载荷：token中存放有效信息的部分，比如用户名，用户角色，过期时间等，但是不要放密码，会泄露！ 签名：将头部与载荷分别采用base64编码后，用“.”相连，再加入盐，最后使用头部声明的编码类型进行编码，就得到了签名。 JWT生成token的安全性分析从JWT生成的token组成上来看，要想避免token被伪造，主要就得看签名部分了，而签名部分又有三部分组成，其中头部和载荷的base64编码，几乎是透明的，毫无安全性可言，那么最终守护token安全的重担就落在了加入的盐上面了！试想：如果生成token所用的盐与解析token时加入的盐是一样的。岂不是类似于中国人民银行把人民币防伪技术公开了？大家可以用这个盐来解析token，就能用来伪造token。这时，我们就需要对盐采用非对称加密的方式进行加密，以达到生成token与校验token方所用的盐不一致的安全效果！ 非对称加密RSA介绍 基本原理：同时生成两把密钥：私钥和公钥，私钥隐秘保存，公钥可以下发给信任客户端 私钥加密，持有私钥或公钥才可以解密 公钥加密，持有私钥才可解密 优点：安全，难以破解 缺点：算法比较耗时，为了安全，可以接受 历史：三位数学家Rivest、Shamir 和 Adleman 设计了一种算法，可以实现非对称加密。这种算法用他们三个人的名字缩写：RSA。 4. SpringSecurity整合JWT4.1 认证思路分析SpringSecurity主要是通过过滤器来实现功能的！我们要找到SpringSecurity实现认证和校验身份的过滤器！ 回顾集中式认证流程用户认证：使用UsernamePasswordAuthenticationFilter过滤器中attemptAuthentication方法实现认证功能，该过滤器父类中successfulAuthentication方法实现认证成功后的操作。身份校验：使用BasicAuthenticationFilter过滤器中doFilterInternal方法验证是否登录，以决定能否进入后续过滤器。 分析分布式认证流程用户认证：由于分布式项目，多数是前后端分离的架构设计，我们要满足可以接受异步post的认证请求参数，需要修改UsernamePasswordAuthenticationFilter过滤器中attemptAuthentication方法，让其能够接收请求体。另外，默认successfulAuthentication方法在认证通过后，是把用户信息直接放入session就完事了，现在我们需要修改这个方法，在认证通过后生成token并返回给用户。身份校验：原来BasicAuthenticationFilter过滤器中doFilterInternal方法校验用户是否登录，就是看session中是否有用户信息，我们要修改为，验证用户携带的token是否合法，并解析出用户信息，交给SpringSecurity，以便于后续的授权功能可以正常使用。 4.2 具体实现为了演示单点登录的效果，我们设计如下项目结构 4.2.1 父工程创建因为本案例需要创建多个系统，所以我们使用maven聚合工程来实现，首先创建一个父工程，导入springboot的父依赖即可 123456&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.3.RELEASE&lt;/version&gt; &lt;relativePath/&gt;&lt;/parent&gt; 4.2.2 公共工程创建然后创建一个common工程，其他工程依赖此系统 导入JWT相关的依赖 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;io.jsonwebtoken&lt;/groupId&gt; &lt;artifactId&gt;jjwt-api&lt;/artifactId&gt; &lt;version&gt;0.10.7&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.jsonwebtoken&lt;/groupId&gt; &lt;artifactId&gt;jjwt-impl&lt;/artifactId&gt; &lt;version&gt;0.10.7&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.jsonwebtoken&lt;/groupId&gt; &lt;artifactId&gt;jjwt-jackson&lt;/artifactId&gt; &lt;version&gt;0.10.7&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;!--jackson包--&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.9.9&lt;/version&gt; &lt;/dependency&gt; &lt;!--日志包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;joda-time&lt;/groupId&gt; &lt;artifactId&gt;joda-time&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建相关的工具类 Payload 123456789101112/** * @program: springboot-54-security-jwt-demo * @description: * @author: 波波烤鸭 * @create: 2019-12-03 10:28 */@Datapublic class Payload &lt;T&gt;&#123; private String id; private T userInfo; private Date expiration;&#125; JsonUtils 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package com.dpb.utils;import com.fasterxml.jackson.core.JsonProcessingException;import com.fasterxml.jackson.core.type.TypeReference;import com.fasterxml.jackson.databind.ObjectMapper;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;import java.util.List;import java.util.Map;/** * @author: 波波烤鸭 **/public class JsonUtils &#123; public static final ObjectMapper mapper = new ObjectMapper(); private static final Logger logger = LoggerFactory.getLogger(JsonUtils.class); public static String toString(Object obj) &#123; if (obj == null) &#123; return null; &#125; if (obj.getClass() == String.class) &#123; return (String) obj; &#125; try &#123; return mapper.writeValueAsString(obj); &#125; catch (JsonProcessingException e) &#123; logger.error(\"json序列化出错：\" + obj, e); return null; &#125; &#125; public static &lt;T&gt; T toBean(String json, Class&lt;T&gt; tClass) &#123; try &#123; return mapper.readValue(json, tClass); &#125; catch (IOException e) &#123; logger.error(\"json解析出错：\" + json, e); return null; &#125; &#125; public static &lt;E&gt; List&lt;E&gt; toList(String json, Class&lt;E&gt; eClass) &#123; try &#123; return mapper.readValue(json, mapper.getTypeFactory().constructCollectionType(List.class, eClass)); &#125; catch (IOException e) &#123; logger.error(\"json解析出错：\" + json, e); return null; &#125; &#125; public static &lt;K, V&gt; Map&lt;K, V&gt; toMap(String json, Class&lt;K&gt; kClass, Class&lt;V&gt; vClass) &#123; try &#123; return mapper.readValue(json, mapper.getTypeFactory().constructMapType(Map.class, kClass, vClass)); &#125; catch (IOException e) &#123; logger.error(\"json解析出错：\" + json, e); return null; &#125; &#125; public static &lt;T&gt; T nativeRead(String json, TypeReference&lt;T&gt; type) &#123; try &#123; return mapper.readValue(json, type); &#125; catch (IOException e) &#123; logger.error(\"json解析出错：\" + json, e); return null; &#125; &#125;&#125; JwtUtils 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104package com.dpb.utils;import com.dpb.domain.Payload;import io.jsonwebtoken.Claims;import io.jsonwebtoken.Jws;import io.jsonwebtoken.Jwts;import io.jsonwebtoken.SignatureAlgorithm;import org.joda.time.DateTime;import java.security.PrivateKey;import java.security.PublicKey;import java.util.Base64;import java.util.UUID;/** * @author: 波波烤鸭 * 生成token以及校验token相关方法 */public class JwtUtils &#123; private static final String JWT_PAYLOAD_USER_KEY = \"user\"; /** * 私钥加密token * * @param userInfo 载荷中的数据 * @param privateKey 私钥 * @param expire 过期时间，单位分钟 * @return JWT */ public static String generateTokenExpireInMinutes(Object userInfo, PrivateKey privateKey, int expire) &#123; return Jwts.builder() .claim(JWT_PAYLOAD_USER_KEY, JsonUtils.toString(userInfo)) .setId(createJTI()) .setExpiration(DateTime.now().plusMinutes(expire).toDate()) .signWith(privateKey, SignatureAlgorithm.RS256) .compact(); &#125; /** * 私钥加密token * * @param userInfo 载荷中的数据 * @param privateKey 私钥 * @param expire 过期时间，单位秒 * @return JWT */ public static String generateTokenExpireInSeconds(Object userInfo, PrivateKey privateKey, int expire) &#123; return Jwts.builder() .claim(JWT_PAYLOAD_USER_KEY, JsonUtils.toString(userInfo)) .setId(createJTI()) .setExpiration(DateTime.now().plusSeconds(expire).toDate()) .signWith(privateKey, SignatureAlgorithm.RS256) .compact(); &#125; /** * 公钥解析token * * @param token 用户请求中的token * @param publicKey 公钥 * @return Jws&lt;Claims&gt; */ private static Jws&lt;Claims&gt; parserToken(String token, PublicKey publicKey) &#123; return Jwts.parser().setSigningKey(publicKey).parseClaimsJws(token); &#125; private static String createJTI() &#123; return new String(Base64.getEncoder().encode(UUID.randomUUID().toString().getBytes())); &#125; /** * 获取token中的用户信息 * * @param token 用户请求中的令牌 * @param publicKey 公钥 * @return 用户信息 */ public static &lt;T&gt; Payload&lt;T&gt; getInfoFromToken(String token, PublicKey publicKey, Class&lt;T&gt; userType) &#123; Jws&lt;Claims&gt; claimsJws = parserToken(token, publicKey); Claims body = claimsJws.getBody(); Payload&lt;T&gt; claims = new Payload&lt;&gt;(); claims.setId(body.getId()); claims.setUserInfo(JsonUtils.toBean(body.get(JWT_PAYLOAD_USER_KEY).toString(), userType)); claims.setExpiration(body.getExpiration()); return claims; &#125; /** * 获取token中的载荷信息 * * @param token 用户请求中的令牌 * @param publicKey 公钥 * @return 用户信息 */ public static &lt;T&gt; Payload&lt;T&gt; getInfoFromToken(String token, PublicKey publicKey) &#123; Jws&lt;Claims&gt; claimsJws = parserToken(token, publicKey); Claims body = claimsJws.getBody(); Payload&lt;T&gt; claims = new Payload&lt;&gt;(); claims.setId(body.getId()); claims.setExpiration(body.getExpiration()); return claims; &#125;&#125; RsaUtils 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123package com.dpb.utils;import java.io.File;import java.io.IOException;import java.nio.file.Files;import java.security.*;import java.security.spec.InvalidKeySpecException;import java.security.spec.PKCS8EncodedKeySpec;import java.security.spec.X509EncodedKeySpec;import java.util.Base64;/** * @author 波波烤鸭 */public class RsaUtils &#123; private static final int DEFAULT_KEY_SIZE = 2048; /** * 从文件中读取公钥 * * @param filename 公钥保存路径，相对于classpath * @return 公钥对象 * @throws Exception */ public static PublicKey getPublicKey(String filename) throws Exception &#123; byte[] bytes = readFile(filename); return getPublicKey(bytes); &#125; /** * 从文件中读取密钥 * * @param filename 私钥保存路径，相对于classpath * @return 私钥对象 * @throws Exception */ public static PrivateKey getPrivateKey(String filename) throws Exception &#123; byte[] bytes = readFile(filename); return getPrivateKey(bytes); &#125; /** * 获取公钥 * * @param bytes 公钥的字节形式 * @return * @throws Exception */ private static PublicKey getPublicKey(byte[] bytes) throws Exception &#123; bytes = Base64.getDecoder().decode(bytes); X509EncodedKeySpec spec = new X509EncodedKeySpec(bytes); KeyFactory factory = KeyFactory.getInstance(\"RSA\"); return factory.generatePublic(spec); &#125; /** * 获取密钥 * * @param bytes 私钥的字节形式 * @return * @throws Exception */ private static PrivateKey getPrivateKey(byte[] bytes) throws NoSuchAlgorithmException, InvalidKeySpecException &#123; bytes = Base64.getDecoder().decode(bytes); PKCS8EncodedKeySpec spec = new PKCS8EncodedKeySpec(bytes); KeyFactory factory = KeyFactory.getInstance(\"RSA\"); return factory.generatePrivate(spec); &#125; /** * 根据密文，生存rsa公钥和私钥,并写入指定文件 * * @param publicKeyFilename 公钥文件路径 * @param privateKeyFilename 私钥文件路径 * @param secret 生成密钥的密文 */ public static void generateKey(String publicKeyFilename, String privateKeyFilename, String secret, int keySize) throws Exception &#123; KeyPairGenerator keyPairGenerator = KeyPairGenerator.getInstance(\"RSA\"); SecureRandom secureRandom = new SecureRandom(secret.getBytes()); keyPairGenerator.initialize(Math.max(keySize, DEFAULT_KEY_SIZE), secureRandom); KeyPair keyPair = keyPairGenerator.genKeyPair(); // 获取公钥并写出 byte[] publicKeyBytes = keyPair.getPublic().getEncoded(); publicKeyBytes = Base64.getEncoder().encode(publicKeyBytes); writeFile(publicKeyFilename, publicKeyBytes); // 获取私钥并写出 byte[] privateKeyBytes = keyPair.getPrivate().getEncoded(); privateKeyBytes = Base64.getEncoder().encode(privateKeyBytes); writeFile(privateKeyFilename, privateKeyBytes); &#125; private static byte[] readFile(String fileName) throws Exception &#123; return Files.readAllBytes(new File(fileName).toPath()); &#125; private static void writeFile(String destPath, byte[] bytes) throws IOException &#123; File dest = new File(destPath); if (!dest.exists()) &#123; dest.createNewFile(); &#125; Files.write(dest.toPath(), bytes); &#125;&#125;123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103在通用子模块中编写测试类生成rsa公钥和私钥/** * @program: springboot-54-security-jwt-demo * @description: * @author: 波波烤鸭 * @create: 2019-12-03 11:08 */public class JwtTest &#123; private String privateKey = \"c:/tools/auth_key/id_key_rsa\"; private String publicKey = \"c:/tools/auth_key/id_key_rsa.pub\"; @Test public void test1() throws Exception&#123; RsaUtils.generateKey(publicKey,privateKey,\"dpb\",1024); &#125;&#125; 4.2.3 认证系统创建接下来我们创建我们的认证服务。导入相关的依赖 1234567891011121314151617181920212223242526272829303132333435&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;artifactId&gt;security-jwt-common&lt;/artifactId&gt; &lt;groupId&gt;com.dpb&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建配置文件 1234567891011121314151617spring: datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://localhost:3306/srm username: root password: 123456 type: com.alibaba.druid.pool.DruidDataSourcemybatis: type-aliases-package: com.dpb.domain mapper-locations: classpath:mapper/*.xmllogging: level: com.dpb: debugrsa: key: pubKeyFile: c:\\tools\\auth_key\\id_key_rsa.pub priKeyFile: c:\\tools\\auth_key\\id_key_rsa 提供公钥私钥的配置类1234567891011121314151617181920212223242526272829303132333435363738package com.dpb.config;import com.dpb.utils.RsaUtils;import lombok.Data;import org.springframework.boot.context.properties.ConfigurationProperties;import org.springframework.context.annotation.Configuration;import javax.annotation.PostConstruct;import java.security.PrivateKey;import java.security.PublicKey;/** * @program: springboot-54-security-jwt-demo * @description: * @author: 波波烤鸭 * @create: 2019-12-03 11:25 */@Data@ConfigurationProperties(prefix = \"rsa.key\")public class RsaKeyProperties &#123; private String pubKeyFile; private String priKeyFile; private PublicKey publicKey; private PrivateKey privateKey; /** * 系统启动的时候触发 * @throws Exception */ @PostConstruct public void createRsaKey() throws Exception &#123; publicKey = RsaUtils.getPublicKey(pubKeyFile); privateKey = RsaUtils.getPrivateKey(priKeyFile); &#125;&#125; 创建启动类123456789101112131415/** * @program: springboot-54-security-jwt-demo * @description: 启动类 * @author: 波波烤鸭 * @create: 2019-12-03 11:23 */@SpringBootApplication@MapperScan(\"com.dpb.mapper\")@EnableConfigurationProperties(RsaKeyProperties.class)public class App &#123; public static void main(String[] args) &#123; SpringApplication.run(App.class,args); &#125;&#125; 完成数据认证的逻辑12345678910111213141516171819202122232425package com.dpb.domain;import com.fasterxml.jackson.annotation.JsonIgnore;import lombok.Data;import org.springframework.security.core.GrantedAuthority;/** * @program: springboot-54-security-jwt-demo * @description: * @author: 波波烤鸭 * @create: 2019-12-03 15:21 */@Datapublic class RolePojo implements GrantedAuthority &#123; private Integer id; private String roleName; private String roleDesc; @JsonIgnore @Override public String getAuthority() &#123; return roleName; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package com.dpb.domain;import com.fasterxml.jackson.annotation.JsonIgnore;import lombok.Data;import org.springframework.security.core.GrantedAuthority;import org.springframework.security.core.authority.SimpleGrantedAuthority;import org.springframework.security.core.userdetails.UserDetails;import java.util.ArrayList;import java.util.Collection;import java.util.List;/** * @program: springboot-54-security-jwt-demo * @description: * @author: 波波烤鸭 * @create: 2019-12-03 11:33 */@Datapublic class UserPojo implements UserDetails &#123; private Integer id; private String username; private String password; private Integer status; private List&lt;RolePojo&gt; roles; @JsonIgnore @Override public Collection&lt;? extends GrantedAuthority&gt; getAuthorities() &#123; List&lt;SimpleGrantedAuthority&gt; auth = new ArrayList&lt;&gt;(); auth.add(new SimpleGrantedAuthority(\"ADMIN\")); return auth; &#125; @Override public String getPassword() &#123; return this.password; &#125; @Override public String getUsername() &#123; return this.username; &#125; @JsonIgnore @Override public boolean isAccountNonExpired() &#123; return true; &#125; @JsonIgnore @Override public boolean isAccountNonLocked() &#123; return true; &#125; @JsonIgnore @Override public boolean isCredentialsNonExpired() &#123; return true; &#125; @JsonIgnore @Override public boolean isEnabled() &#123; return true; &#125;&#125; Mapper接口 123public interface UserMapper &#123; public UserPojo queryByUserName(@Param(\"userName\") String userName);&#125; Mapper映射文件 123456789&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"com.dpb.mapper.UserMapper\"&gt; &lt;select id=\"queryByUserName\" resultType=\"UserPojo\"&gt; select * from t_user where username = #&#123;userName&#125; &lt;/select&gt;&lt;/mapper&gt; Service 123public interface UserService extends UserDetailsService &#123;&#125; 1234567891011121314@Service@Transactionalpublic class UserServiceImpl implements UserService &#123; @Autowired private UserMapper mapper; @Override public UserDetails loadUserByUsername(String s) throws UsernameNotFoundException &#123; UserPojo user = mapper.queryByUserName(s); return user; &#125;&#125; 自定义认证过滤器1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package com.dpb.filter;import com.dpb.config.RsaKeyProperties;import com.dpb.domain.RolePojo;import com.dpb.domain.UserPojo;import com.dpb.utils.JwtUtils;import com.fasterxml.jackson.databind.ObjectMapper;import net.bytebuddy.agent.builder.AgentBuilder;import org.springframework.security.authentication.AuthenticationManager;import org.springframework.security.authentication.UsernamePasswordAuthenticationToken;import org.springframework.security.core.Authentication;import org.springframework.security.core.AuthenticationException;import org.springframework.security.core.authority.SimpleGrantedAuthority;import org.springframework.security.core.userdetails.User;import org.springframework.security.core.userdetails.UserDetails;import org.springframework.security.web.authentication.UsernamePasswordAuthenticationFilter;import javax.servlet.FilterChain;import javax.servlet.ServletException;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import java.io.IOException;import java.io.PrintWriter;import java.util.ArrayList;import java.util.HashMap;import java.util.List;import java.util.Map;/** * @program: springboot-54-security-jwt-demo * @description: * @author: 波波烤鸭 * @create: 2019-12-03 11:57 */public class TokenLoginFilter extends UsernamePasswordAuthenticationFilter &#123; private AuthenticationManager authenticationManager; private RsaKeyProperties prop; public TokenLoginFilter(AuthenticationManager authenticationManager, RsaKeyProperties prop) &#123; this.authenticationManager = authenticationManager; this.prop = prop; &#125; public Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response) throws AuthenticationException &#123; try &#123; UserPojo sysUser = new ObjectMapper().readValue(request.getInputStream(), UserPojo.class); UsernamePasswordAuthenticationToken authRequest = new UsernamePasswordAuthenticationToken(sysUser.getUsername(), sysUser.getPassword()); return authenticationManager.authenticate(authRequest); &#125;catch (Exception e)&#123; try &#123; response.setContentType(\"application/json;charset=utf-8\"); response.setStatus(HttpServletResponse.SC_UNAUTHORIZED); PrintWriter out = response.getWriter(); Map resultMap = new HashMap(); resultMap.put(\"code\", HttpServletResponse.SC_UNAUTHORIZED); resultMap.put(\"msg\", \"用户名或密码错误！\"); out.write(new ObjectMapper().writeValueAsString(resultMap)); out.flush(); out.close(); &#125;catch (Exception outEx)&#123; outEx.printStackTrace(); &#125; throw new RuntimeException(e); &#125; &#125; public void successfulAuthentication(HttpServletRequest request, HttpServletResponse response, FilterChain chain, Authentication authResult) throws IOException, ServletException &#123; UserPojo user = new UserPojo(); user.setUsername(authResult.getName()); user.setRoles((List&lt;RolePojo&gt;)authResult.getAuthorities()); String token = JwtUtils.generateTokenExpireInMinutes(user, prop.getPrivateKey(), 24 * 60); response.addHeader(\"Authorization\", \"Bearer \"+token); try &#123; response.setContentType(\"application/json;charset=utf-8\"); response.setStatus(HttpServletResponse.SC_OK); PrintWriter out = response.getWriter(); Map resultMap = new HashMap(); resultMap.put(\"code\", HttpServletResponse.SC_OK); resultMap.put(\"msg\", \"认证通过！\"); out.write(new ObjectMapper().writeValueAsString(resultMap)); out.flush(); out.close(); &#125;catch (Exception outEx)&#123; outEx.printStackTrace(); &#125; &#125;&#125; 自定义校验token的过滤器12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package com.dpb.filter;import com.dpb.config.RsaKeyProperties;import com.dpb.domain.Payload;import com.dpb.domain.UserPojo;import com.dpb.utils.JwtUtils;import com.fasterxml.jackson.databind.ObjectMapper;import org.springframework.security.authentication.AuthenticationManager;import org.springframework.security.authentication.UsernamePasswordAuthenticationToken;import org.springframework.security.core.context.SecurityContextHolder;import org.springframework.security.web.authentication.www.BasicAuthenticationFilter;import javax.servlet.FilterChain;import javax.servlet.ServletException;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import java.io.IOException;import java.io.PrintWriter;import java.util.HashMap;import java.util.Map;/** * @program: springboot-54-security-jwt-demo * @description: * @author: 波波烤鸭 * @create: 2019-12-03 12:39 */public class TokenVerifyFilter extends BasicAuthenticationFilter &#123; private RsaKeyProperties prop; public TokenVerifyFilter(AuthenticationManager authenticationManager, RsaKeyProperties prop) &#123; super(authenticationManager); this.prop = prop; &#125; public void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain chain) throws IOException, ServletException &#123; String header = request.getHeader(\"Authorization\"); if (header == null || !header.startsWith(\"Bearer \")) &#123; //如果携带错误的token，则给用户提示请登录！ chain.doFilter(request, response); response.setContentType(\"application/json;charset=utf-8\"); response.setStatus(HttpServletResponse.SC_FORBIDDEN); PrintWriter out = response.getWriter(); Map resultMap = new HashMap(); resultMap.put(\"code\", HttpServletResponse.SC_FORBIDDEN); resultMap.put(\"msg\", \"请登录！\"); out.write(new ObjectMapper().writeValueAsString(resultMap)); out.flush(); out.close(); &#125; else &#123; //如果携带了正确格式的token要先得到token String token = header.replace(\"Bearer \", \"\"); //验证tken是否正确 Payload&lt;UserPojo&gt; payload = JwtUtils.getInfoFromToken(token, prop.getPublicKey(), UserPojo.class); UserPojo user = payload.getUserInfo(); if(user!=null)&#123; UsernamePasswordAuthenticationToken authResult = new UsernamePasswordAuthenticationToken(user.getUsername(), null, user.getAuthorities()); SecurityContextHolder.getContext().setAuthentication(authResult); chain.doFilter(request, response); &#125; &#125; &#125;&#125; 编写SpringSecurity的配置类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package com.dpb.config;import com.dpb.filter.TokenLoginFilter;import com.dpb.filter.TokenVerifyFilter;import com.dpb.service.UserService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.security.config.annotation.authentication.builders.AuthenticationManagerBuilder;import org.springframework.security.config.annotation.method.configuration.EnableGlobalMethodSecurity;import org.springframework.security.config.annotation.web.builders.HttpSecurity;import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;import org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;import org.springframework.security.config.http.SessionCreationPolicy;import org.springframework.security.crypto.bcrypt.BCryptPasswordEncoder;/** * @program: springboot-54-security-jwt-demo * @description: * @author: 波波烤鸭 * @create: 2019-12-03 12:41 */@Configuration@EnableWebSecurity@EnableGlobalMethodSecurity(securedEnabled=true)public class WebSecurityConfig extends WebSecurityConfigurerAdapter &#123; @Autowired private UserService userService; @Autowired private RsaKeyProperties prop; @Bean public BCryptPasswordEncoder passwordEncoder()&#123; return new BCryptPasswordEncoder(); &#125; //指定认证对象的来源 public void configure(AuthenticationManagerBuilder auth) throws Exception &#123; auth.userDetailsService(userService).passwordEncoder(passwordEncoder()); &#125; //SpringSecurity配置信息 public void configure(HttpSecurity http) throws Exception &#123; http.csrf() .disable() .authorizeRequests() .antMatchers(\"/user/query\").hasAnyRole(\"ADMIN\") .anyRequest() .authenticated() .and() .addFilter(new TokenLoginFilter(super.authenticationManager(), prop)) .addFilter(new TokenVerifyFilter(super.authenticationManager(), prop)) .sessionManagement().sessionCreationPolicy(SessionCreationPolicy.STATELESS); &#125;&#125; 启动服务测试 通过Postman来访问测试 根据token信息我们访问其他资源 4.2.4 资源系统创建说明资源服务可以有很多个，这里只拿产品服务为例，记住，资源服务中只能通过公钥验证认证。不能签发token！创建产品服务并导入jar包根据实际业务导包即可，咱们就暂时和认证服务一样了。接下来我们再创建一个资源服务 导入相关的依赖1234567891011121314151617181920212223242526272829303132333435&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;artifactId&gt;security-jwt-common&lt;/artifactId&gt; &lt;groupId&gt;com.dpb&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 编写产品服务配置文件切记这里只能有公钥地址！ 123456789101112131415161718server: port: 9002spring: datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://localhost:3306/srm username: root password: 123456 type: com.alibaba.druid.pool.DruidDataSourcemybatis: type-aliases-package: com.dpb.domain mapper-locations: classpath:mapper/*.xmllogging: level: com.dpb: debugrsa: key: pubKeyFile: c:\\tools\\auth_key\\id_key_rsa.pub 编写读取公钥的配置类12345678910111213141516171819202122232425262728293031323334package com.dpb.config;import com.dpb.utils.RsaUtils;import lombok.Data;import org.springframework.boot.context.properties.ConfigurationProperties;import javax.annotation.PostConstruct;import java.security.PrivateKey;import java.security.PublicKey;/** * @program: springboot-54-security-jwt-demo * @description: * @author: 波波烤鸭 * @create: 2019-12-03 11:25 */@Data@ConfigurationProperties(prefix = \"rsa.key\")public class RsaKeyProperties &#123; private String pubKeyFile; private PublicKey publicKey; /** * 系统启动的时候触发 * @throws Exception */ @PostConstruct public void createRsaKey() throws Exception &#123; publicKey = RsaUtils.getPublicKey(pubKeyFile); &#125;&#125; 编写启动类1234567891011121314151617181920212223package com.dpb;import com.dpb.config.RsaKeyProperties;import org.mybatis.spring.annotation.MapperScan;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.boot.context.properties.EnableConfigurationProperties;/** * @program: springboot-54-security-jwt-demo * @description: * @author: 波波烤鸭 * @create: 2019-12-03 17:23 */@SpringBootApplication@MapperScan(\"com.dpb.mapper\")@EnableConfigurationProperties(RsaKeyProperties.class)public class App &#123; public static void main(String[] args) &#123; SpringApplication.run(App.class,args); &#125;&#125; 复制认证服务中，用户对象，角色对象和校验认证的接口复制认证服务中的相关内容即可 复制认证服务中SpringSecurity配置类做修改12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.dpb.config;import com.dpb.filter.TokenVerifyFilter;import com.dpb.service.UserService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.security.config.annotation.authentication.builders.AuthenticationManagerBuilder;import org.springframework.security.config.annotation.method.configuration.EnableGlobalMethodSecurity;import org.springframework.security.config.annotation.web.builders.HttpSecurity;import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;import org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;import org.springframework.security.config.http.SessionCreationPolicy;import org.springframework.security.crypto.bcrypt.BCryptPasswordEncoder;/** * @program: springboot-54-security-jwt-demo * @description: * @author: 波波烤鸭 * @create: 2019-12-03 12:41 */@Configuration@EnableWebSecurity@EnableGlobalMethodSecurity(securedEnabled=true)public class WebSecurityConfig extends WebSecurityConfigurerAdapter &#123; @Autowired private UserService userService; @Autowired private RsaKeyProperties prop; @Bean public BCryptPasswordEncoder passwordEncoder()&#123; return new BCryptPasswordEncoder(); &#125; //指定认证对象的来源 public void configure(AuthenticationManagerBuilder auth) throws Exception &#123; auth.userDetailsService(userService).passwordEncoder(passwordEncoder()); &#125; //SpringSecurity配置信息 public void configure(HttpSecurity http) throws Exception &#123; http.csrf() .disable() .authorizeRequests() //.antMatchers(\"/user/query\").hasAnyRole(\"USER\") .anyRequest() .authenticated() .and() .addFilter(new TokenVerifyFilter(super.authenticationManager(), prop)) // 禁用掉session .sessionManagement().sessionCreationPolicy(SessionCreationPolicy.STATELESS); &#125;&#125; 去掉“增加自定义认证过滤器”即可！ 编写产品处理器1234567891011121314151617181920212223242526package com.dpb.controller;import org.springframework.security.access.annotation.Secured;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;/** * @program: springboot-54-security-jwt-demo * @description: * @author: 波波烤鸭 * @create: 2019-12-03 11:55 */@RestController@RequestMapping(\"/user\")public class UserController &#123; @RequestMapping(\"/query\") public String query()&#123; return \"success\"; &#125; @RequestMapping(\"/update\") public String update()&#123; return \"update\"; &#125;&#125; 测试 本文转载自Java码农之路头条，原文连接点这里，版权归原作者所有，转载目的是为了避免以后找不到。","categories":[{"name":"SPRING","slug":"SPRING","permalink":"http://huermosi.xyz/categories/SPRING/"},{"name":"SPRING SECURITY","slug":"SPRING/SPRING-SECURITY","permalink":"http://huermosi.xyz/categories/SPRING/SPRING-SECURITY/"}],"tags":[{"name":"SPRING","slug":"SPRING","permalink":"http://huermosi.xyz/tags/SPRING/"},{"name":"SSO","slug":"SSO","permalink":"http://huermosi.xyz/tags/SSO/"},{"name":"JWT","slug":"JWT","permalink":"http://huermosi.xyz/tags/JWT/"},{"name":"SPRING BOOT","slug":"SPRING-BOOT","permalink":"http://huermosi.xyz/tags/SPRING-BOOT/"}]},{"title":"MySQL 5.7 JSON 数据类型使用总结","slug":"mysql/mysql57-json","date":"2019-12-10T02:26:38.000Z","updated":"2021-04-08T08:49:31.698Z","comments":true,"path":"2019/95270310102638/","link":"","permalink":"http://huermosi.xyz/2019/95270310102638/","excerpt":"","text":"文章目录一、JSON数据类型简介二、简单使用示例数据准备数据查询数据修改其他函数总结三、JSON函数概览 一、JSON数据类型简介从版本5.7.8开始，mysql开始支持json数据类型，json数据类型存储时会做格式检验，不满足json格式会报错，json数据类型默认值不允许为空。 二、简单使用示例数据准备1234567891011create table json_tab( id int unsigned primary key auto_increment comment '主键', json_info json comment 'json数据', json_id int generated always as (json_info -&gt; '$.id') comment 'json数据的虚拟字段', index json_info_id_idx (json_id)) comment 'json示例表';insert into json_tab(json_info) values ('&#123;\"id\": 1, \"name\": \"张三\", \"age\": 18, \"sister\": [&#123;\"name\": \"张大姐\", \"age\": 30&#125;, &#123;\"name\": \"张二姐\", \"age\": 20&#125;]&#125;');insert into json_tab(json_info) values (JSON_OBJECT('id', 2, 'name', '李四', 'age', 18, 'sister', JSON_ARRAY(JSON_OBJECT('name', '李大姐', 'age', 28), JSON_OBJECT('name', '李二姐', 'age', 25))));insert into json_tab(json_info) values ('&#123;\"id\": 3, \"name\": \"小明\", \"age\": 18, \"sister\": [&#123;\"name\": \"小明大姐\", \"age\": 25, \"friend\": [&#123;\"name\": \"大姐朋友一\", \"age\": 25&#125;, &#123;\"name\": \"大姐朋友二\", \"age\": 25&#125;]&#125;, &#123;\"name\": \"小明二姐\", \"age\": 20, \"friend\": [&#123;\"name\": \"二姐朋友一\", \"age\": 22&#125;, &#123;\"name\": \"二姐朋友二\", \"age\": 21&#125;]&#125;]&#125;'); json_id是虚拟列，插入数据时不需要往该字段插入值，json数据类型不能直接建立索引，需要通过建立虚拟列再将索引建在虚拟列上这样的方式来建立索引； json字段插入数据时有两种方式，一种是直接插入满足json格式的字符串，不符合json格式的字符串插入时会报错；另一种是通过JSON_OBJECT、JSON_ARRAY这两个json函数先构建好json数据再插入。 数据查询123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 先看看数据，注意虚拟列json_id，未插入值确显示有值select * from json_tab;select * from json_tab order by json_id desc;select * from json_tab where json_info -&gt; '$.name' = '李四';# JSON_TYPE 函数判断JSON数据类型select JSON_TYPE(json_info) as info_type, JSON_TYPE(json_info -&gt; '$.age') as age_type, JSON_TYPE(json_info -&gt; '$.name') as name_type, JSON_TYPE(json_info -&gt; '$.sister') as sister_typefrom json_tab;# 查询姓名以及他们的年龄select json_info -&gt; '$.name' as name, json_info -&gt; '$.age' as age from json_tab;select json_info -&gt; '$**.name' as name, json_info -&gt; '$**.age' as age from json_tab;# -&gt; 等价于 JSON_EXTRACT(column, path)select JSON_EXTRACT(json_info, '$.name') as name, JSON_EXTRACT(json_info, '$.age') as age from json_tab;# 去掉双引号select json_info -&gt;&gt; '$.name' as name, json_info -&gt; '$.age' as age from json_tab;# -&gt;&gt; 等价于 JSON_UNQUOTE(JSON_EXTRACT(column, path))select JSON_UNQUOTE(JSON_EXTRACT(json_info, '$.name')) as name, JSON_EXTRACT(json_info, '$.age') as age from json_tab;# 查询姓名和他们的姐姐select json_info -&gt;&gt; '$.name' as name, json_info -&gt;&gt; '$.sister[*].name' as sisters from json_tab;# 查询姓名和他们的姐姐以及朋友select json_info -&gt;&gt; '$.name' as name, json_info -&gt;&gt; '$.sister[*].name' as sisters, json_info -&gt; '$**.friend[*].name' as friends from json_tab;# 查询key# JSON_KEYS(json_doc[, path])select JSON_KEYS(json_info) from json_tab;select JSON_KEYS(json_info, '$.sister[1]') from json_tab;# 查询名字是否存在# JSON_CONTAINS(target, candidate[, path])select JSON_CONTAINS(json_info, '\"张三\"', '$.name') from json_tab;# 查询是否包含路径# JSON_CONTAINS_PATH(json_doc, one_or_all, path[, path] ...)# one 至少存在一个路径select JSON_CONTAINS_PATH(json_info,'one', '$.name', '$.xxx') from json_tab;# all 所有路径都存在select JSON_CONTAINS_PATH(json_info,'all', '$.name', '$**.friend') from json_tab;# 查询字符串的路径# JSON_SEARCH(json_doc, one_or_all, search_str[, escape_char[, path] ...])# one 只匹配第一个select JSON_SEARCH(json_info,'one', '张三') from json_tab;select JSON_SEARCH(json_info,'one', '%朋友%', null, '$**.name') from json_tab;# all 找到所有的路径select JSON_SEARCH(json_info,'all', '%朋友_', '朋', '$**.name') from json_tab; 数据修改12345678910111213141516171819202122232425262728293031323334353637# 插入json数组insert into json_tab(json_info) values ('[1, &#123;\"a\": \"b\"&#125;, [2, \"qwe\"]]');# json数组指定位置上追加数据，path可以不是数组元素，结果都是数组# JSON_ARRAY_APPEND(json_doc, path, val[, path, val] ...)select * from json_tab;select JSON_ARRAY_APPEND(json_info, '$', 1) from json_tab where id=4;select JSON_ARRAY_APPEND(json_info, '$[0]', 1) from json_tab where id=4;select JSON_ARRAY_APPEND(json_info, '$[2]', 1) from json_tab where id=4;select JSON_ARRAY_APPEND(json_info, '$[1].a', 1) from json_tab where id=4;# json数组指定位置前插入数据，path必须是数组元素# JSON_ARRAY_INSERT(json_doc, path, val[, path, val] ...)select JSON_ARRAY_INSERT(json_info, '$[1]', 1) from json_tab where id=4;# json指定位置插入值，已存在的path忽略，类似map.putIfAbsent# JSON_INSERT(json_doc, path, val[, path, val] ...)select JSON_INSERT(json_info, '$[1].b', 'bbb') from json_tab where id=4;select JSON_INSERT(json_info, '$[2]', 'aaa') from json_tab where id=4;select JSON_INSERT(json_info, '$[3]', 'aaa') from json_tab where id=4;# json指定位置设置值，已存在的替换，不存在的新增，类似map.put# JSON_SET(json_doc, path, val[, path, val] ...)select JSON_SET(json_info, '$[1].b', 'bbb') from json_tab where id=4;select JSON_SET(json_info, '$[2]', 'aaa') from json_tab where id=4;select JSON_SET(json_info, '$[3]', 'aaa') from json_tab where id=4;# json指定位置替换值，已存在的替换，不存在的忽略，类似map.replace# JSON_REPLACE(json_doc, path, val[, path, val] ...)select JSON_REPLACE(json_info, '$[1].b', 'bbb') from json_tab where id=4;select JSON_REPLACE(json_info, '$[2]', 'aaa') from json_tab where id=4;select JSON_REPLACE(json_info, '$[3]', 'aaa') from json_tab where id=4;# 删除指定位置元素# JSON_REMOVE(json_doc, path[, path] ...)select JSON_REMOVE(json_info, '$[1].a') from json_tab where id=4;select JSON_REMOVE(json_info, '$[2]') from json_tab where id=4; 其他函数12345678910111213141516# 查询json数据的深度# JSON_DEPTH(json_doc)select JSON_DEPTH(json_info),json_info from json_tab;# 查询json数据的长度# JSON_LENGTH(json_doc[, path])select JSON_LENGTH(json_info),json_info from json_tab;# 验证字符串是否有效的json格式# JSON_VALID(val)SELECT JSON_VALID('&#123;\"a\": 1&#125;');SELECT JSON_VALID('hello'), JSON_VALID('\"hello\"');# JSON_QUOTE(string)SELECT JSON_QUOTE('null'), JSON_QUOTE('\"null\"');SELECT JSON_QUOTE('[1, 2, 3]'); 总结路径不允许使用通配符的函数 JSON_KEYS(json_doc[, path]) JSON_CONTAINS(target, candidate[, path]) JSON_ARRAY_APPEND(json_doc, path, val[, path, val] …) JSON_INSERT(json_doc, path, val[, path, val] …) JSON_REMOVE(json_doc, path[, path] …) JSON_REPLACE(json_doc, path, val[, path, val] …) JSON_SET(json_doc, path, val[, path, val] …) JSON_LENGTH(json_doc[, path]) 5.7.22新增函数 JSON_MERGE_PATCH JSON_MERGE_PRESERVE JSON_STORAGE_SIZE JSON_PRETTY 三、JSON函数概览 Name Description JSON_APPEND() (deprecated 5.7.9) Append data to JSON document JSON_ARRAY() Create JSON array JSON_ARRAY_APPEND() Append data to JSON document JSON_ARRAY_INSERT() Insert into JSON array -&gt; Return value from JSON column after evaluating path; equivalent to JSON_EXTRACT(). JSON_CONTAINS() Whether JSON document contains specific object at path JSON_CONTAINS_PATH() Whether JSON document contains any data at path JSON_DEPTH() Maximum depth of JSON document JSON_EXTRACT() Return data from JSON document -&gt;&gt; Return value from JSON column after evaluating path and unquoting the result; equivalent to JSON_UNQUOTE(JSON_EXTRACT()). JSON_INSERT() Insert data into JSON document JSON_KEYS() Array of keys from JSON document JSON_LENGTH() Number of elements in JSON document JSON_MERGE() (deprecated 5.7.22) Merge JSON documents, preserving duplicate keys. Deprecated synonym for JSON_MERGE_PRESERVE() JSON_MERGE_PATCH() Merge JSON documents, replacing values of duplicate keys JSON_MERGE_PRESERVE() Merge JSON documents, preserving duplicate keys JSON_OBJECT() Create JSON object JSON_PRETTY() Print a JSON document in human-readable format JSON_QUOTE() Quote JSON document JSON_REMOVE() Remove data from JSON document JSON_REPLACE() Replace values in JSON document JSON_SEARCH() Path to value within JSON document JSON_SET() Insert data into JSON document JSON_STORAGE_SIZE() Space used for storage of binary representation of a JSON document JSON_TYPE() Type of JSON value JSON_UNQUOTE() Unquote JSON value JSON_VALID() Whether JSON value is valid 本文转载自KevanLiu博客，为避免原文链接失效找不到等原因转载到本博客，版权归原博主所有。版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。本文链接：https://blog.csdn.net/u011207553/article/details/88912219","categories":[{"name":"MYSQL","slug":"MYSQL","permalink":"http://huermosi.xyz/categories/MYSQL/"}],"tags":[{"name":"MYSQL","slug":"MYSQL","permalink":"http://huermosi.xyz/tags/MYSQL/"},{"name":"JSON","slug":"JSON","permalink":"http://huermosi.xyz/tags/JSON/"}]},{"title":"【JVM从小白学成大佬】8.弄明白CMS和G1，就靠这一篇了","slug":"java/java-cms-g1","date":"2019-10-25T10:33:11.000Z","updated":"2021-04-08T08:49:31.558Z","comments":true,"path":"2019/95270314183311/","link":"","permalink":"http://huermosi.xyz/2019/95270314183311/","excerpt":"","text":"在开始介绍CMS和G1前，我们可以剧透几点： 根据不同分代的特点，收集器可能不同。有些收集器可以同时用于新生代和老年代，而有些时候，则需要分别为新生代或老年代选用合适的收集器。一般来说，新生代收集器的收集频率较高，应选用性能高效的收集器；而老年代收集器收集次数相对较少，对空间较为敏感，应当避免选择基于复制算法的收集器。 在垃圾收集执行的时刻，应用程序需要暂停运行。 可以串行收集，也可以并行收集。 如果能做到并发收集（应用程序不必暂停），那绝对是很妙的事情。 如果收集行为可控，那也是很妙的事情。 CMS和G1作为垃圾收集器里的大杀器，是需要好好弄明白的，而且面试中也经常被问到。 希望大家带着下面的问题进行阅读，有目标的阅读，收获更多: 为什么没有一种牛逼的收集器像银弹一样适配所有场景？ CMS的优点、缺点、适用场景？ 为什么CMS只能用作老年代收集器，而不能应用在新生代的收集？ G1的优点、缺点、适用场景？ 1 CMS收集器CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。这是因为CMS收集器工作时，GC工作线程与用户线程可以并发执行，以此来达到降低收集停顿时间的目的。 CMS收集器仅作用于老年代的收集，是基于标记-清除算法的，它的运作过程分为4个步骤： 初始标记（CMS initial mark） 并发标记（CMS concurrent mark） 重新标记（CMS remark） 并发清除（CMS concurrent sweep） 其中，初始标记、重新标记这两个步骤仍然需要Stop-the-world。初始标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快，并发标记阶段就是进行GC Roots Tracing的过程，而重新标记阶段则是为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始阶段稍长一些，但远比并发标记的时间短。 CMS以流水线方式拆分了收集周期，将耗时长的操作单元保持与应用线程并发执行。只将那些必需STW才能执行的操作单元单独拎出来，控制这些单元在恰当的时机运行，并能保证仅需短暂的时间就可以完成。这样，在整个收集周期内，只有两次短暂的暂停（初始标记和重新标记），达到了近似并发的目的。 CMS收集器优点： 并发收集 低停顿。 CMS收集器缺点： CMS收集器对CPU资源非常敏感。 CMS收集器无法处理浮动垃圾（Floating Garbage）。 CMS收集器是基于标记-清除算法，该算法的缺点都有。 CMS收集器之所以能够做到并发，根本原因在于采用基于“标记-清除”的算法并对算法过程进行了细粒度的分解。前面篇章介绍过标记-清除算法将产生大量的内存碎片这对新生代来说是难以接受的，因此新生代的收集器并未提供CMS版本。 另外要补充一点，JVM在暂停的时候，需要选准一个时机。由于JVM系统运行期间的复杂性，不可能做到随时暂停，因此引入了安全点的概念。 安全点(Safepoint)安全点，即程序执行时并非在所有地方都能停顿下来开始GC，只有在到达安全点时才能暂停。Safepoint的选定既不能太少以至于让GC等待时间太长，也不能过于频繁以致于过分增大运行时的负荷。 安全点的初始目的并不是让其他线程停下，而是找到一个稳定的执行状态。在这个执行状态下，Java虚拟机的堆栈不会发生变化。这么一来，垃圾回收器便能够“安全”地执行可达性分析。只要不离开这个安全点，Java虚拟机便能够在垃圾回收的同时，继续运行这段本地代码。 程序运行时并非在所有地方都能停顿下来开始GC，只有在到达安全点时才能暂停。安全点的选定基本上是以程序“是否具有让程序长时间执行的特征”为标准进行选定的。“长时间执行”的最明显特征就是指令序列复用，例如方法调用、循环跳转、异常跳转等，所以具有这些功能的指令才会产生Safepoint。 对于安全点，另一个需要考虑的问题就是如何在GC发生时让所有线程（这里不包括执行JNI调用的线程）都“跑”到最近的安全点上再停顿下来。 两种解决方案： 抢先式中断（Preemptive Suspension）抢先式中断不需要线程的执行代码主动去配合，在GC发生时，首先把所有线程全部中断，如果发现有线程中断的地方不在安全点上，就恢复线程，让它“跑”到安全点上。现在几乎没有虚拟机采用这种方式来暂停线程从而响应GC事件。 主动式中断（Voluntary Suspension）主动式中断的思想是当GC需要中断线程的时候，不直接对线程操作，仅仅简单地设置一个标志，各个线程执行时主动去轮询这个标志，发现中断标志为真时就自己中断挂起。轮询标志的地方和安全点是重合的，另外再加上创建对象需要分配内存的地方。 安全区域指在一段代码片段中，引用关系不会发生变化。在这个区域中任意地方开始GC都是安全的。也可以把Safe Region看作是被扩展了的Safepoint。 2 G1收集器G1重新定义了堆空间，打破了原有的分代模型，将堆划分为一个个区域。这么做的目的是在进行收集时不必在全堆范围内进行，这是它最显著的特点。区域划分的好处就是带来了停顿时间可预测的收集模型：用户可以指定收集操作在多长时间内完成。即G1提供了接近实时的收集特性。 G1与CMS的特征对比如下： 特征 G1 CMS 并发和分代 是 是 最大化释放堆内存 是 否 低延时 是 是 吞吐量 高 低 压实 是 否 可预测性 强 弱 新生代和老年代的物理隔离 否 是 G1具备如下特点： 并行与并发：G1能充分利用多CPU、多核环境下的硬件优势，使用多个CPU来缩短Stop-the-world停顿的时间，部分其他收集器原来需要停顿Java线程执行的GC操作，G1收集器仍然可以通过并发的方式让Java程序继续运行。 分代收集 空间整合：与CMS的标记-清除算法不同，G1从整体来看是基于标记-整理算法实现的收集器，从局部（两个Region之间）上来看是基于“复制”算法实现的。但无论如何，这两种算法都意味着G1运作期间不会产生内存空间碎片，收集后能提供规整的可用内存。这种特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次GC。 可预测的停顿：这是G1相对于CMS的一个优势，降低停顿时间是G1和CMS共同的关注点。 在G1之前的其他收集器进行收集的范围都是整个新生代或者老年代，而G1不再是这样。在堆的结构设计时，G1打破了以往将收集范围固定在新生代或老年代的模式，G1将堆分成许多相同大小的区域单元，每个单元称为Region。Region是一块地址连续的内存空间，G1模块的组成如下图所示： G1收集器将整个Java堆划分为多个大小相等的独立区域（Region），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分Region（不需要连续）的集合。Region的大小是一致的，数值是在1M到32M字节之间的一个2的幂值数，JVM会尽量划分2048个左右、同等大小的Region，这一点可以参看如下源码。其实这个数字既可以手动调整，G1也会根据堆大小自动进行调整。 123456789101112131415161718192021222324252627282930#ifndef SHARE_VM_GC_G1_HEAPREGIONBOUNDS_HPP#define SHARE_VM_GC_G1_HEAPREGIONBOUNDS_HPP#include \"memory/allocation.hpp\"class HeapRegionBounds : public AllStatic &#123;private: // Minimum region size; we won't go lower than that. // We might want to decrease this in the future, to deal with small // heaps a bit more efficiently. static const size_t MIN_REGION_SIZE = 1024 * 1024; // Maximum region size; we don't go higher than that. There's a good // reason for having an upper bound. We don't want regions to get too // large, otherwise cleanup's effectiveness would decrease as there // will be fewer opportunities to find totally empty regions after // marking. static const size_t MAX_REGION_SIZE = 32 * 1024 * 1024; // The automatic region size calculation will try to have around this // many regions in the heap (based on the min heap size). static const size_t TARGET_REGION_NUMBER = 2048;public: static inline size_t min_size(); static inline size_t max_size(); static inline size_t target_number();&#125;;#endif // SHARE_VM_GC_G1_HEAPREGIONBOUNDS_HPP G1收集器之所以能建立可预测的停顿时间模型，是因为它可以有计划地避免在整个Java堆中进行全区域的垃圾收集。G1会通过一个合理的计算模型，计算出每个Region的收集成本并量化，这样一来，收集器在给定了“停顿”时间限制的情况下，总是能选择一组恰当的Regions作为收集目标，让其收集开销满足这个限制条件，以此达到实时收集的目的。 对于打算从CMS或者ParallelOld收集器迁移过来的应用，按照官方的建议，如果发现符合如下特征，可以考虑更换成G1收集器以追求更佳性能： 实时数据占用了超过半数的堆空间； 对象分配率或“晋升”的速度变化明显； 期望消除耗时较长的GC或停顿（超过0.5——1秒）。 原文如下：Applications running today with either the CMS or the ParallelOld garbage collector would benefit switching to G1 if the application has one or more of the following traits. More than 50% of the Java heap is occupied with live data. The rate of object allocation rate or promotion varies significantly. Undesired long garbage collection or compaction pauses (longer than 0.5 to 1 second) G1收集的运作过程大致如下： 初始标记（Initial Marking）：仅仅只是标记一下GC Roots能直接关联到的对象，并且修改TAMS（Next Top at Mark Start）的值，让下一阶段用户程序并发运行时，能在正确可用的Region中创建新对象，这阶段需要停顿线程，但耗时很短。 并发标记（Concurrent Marking）：是从GC Roots开始堆中对象进行可达性分析，找出存活的对象，这阶段耗时较长，但可与用户程序并发执行。 最终标记（Final Marking）：是为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程Remembered Set Logs里面，最终标记阶段需要把Remembered Set Logs的数据合并到Remembered Set中，这阶段需要停顿线程，但是可并行执行。 筛选回收（Live Data Counting and Evacuation）：首先对各个Region的回收价值和成本进行排序，根据用户所期望的GC停顿时间来制定回收计划。这个阶段也可以做到与用户程序一起并发执行，但是因为只回收一部分Region，时间是用户可控制的，而且停顿用户线程将大幅提高收集效率。 全局变量和栈中引用的对象是可以列入根集合的，这样在寻找垃圾时，就可以从根集合出发扫描堆空间。在G1中，引入了一种新的能加入根集合的类型，就是记忆集（Remembered Set）。Remembered Sets（也叫RSets）用来跟踪对象引用。G1的很多开源都是源自Remembered Set，例如，它通常约占Heap大小的20%或更高。并且，我们进行对象复制的时候，因为需要扫描和更改Card Table的信息，这个速度影响了复制的速度，进而影响暂停时间。 卡表（Card Table）有个场景，老年代的对象可能引用新生代的对象，那标记存活对象的时候，需要扫描老年代中的所有对象。因为该对象拥有对新生代对象的引用，那么这个引用也会被称为GC Roots。那不是得又做全堆扫描？成本太高了吧。 HotSpot给出的解决方案是一项叫做卡表（Card Table）的技术。该技术将整个堆划分为一个个大小为512字节的卡，并且维护一个卡表，用来存储每张卡的一个标识位。这个标识位代表对应的卡是否可能存有指向新生代对象的引用。如果可能存在，那么我们就认为这张卡是脏的。 在进行Minor GC的时候，我们便可以不用扫描整个老年代，而是在卡表中寻找脏卡，并将脏卡中的对象加入到Minor GC的GC Roots里。当完成所有脏卡的扫描之后，Java虚拟机便会将所有脏卡的标识位清零。 想要保证每个可能有指向新生代对象引用的卡都被标记为脏卡，那么Java虚拟机需要截获每个引用型实例变量的写操作，并作出对应的写标识位操作。 卡表能用于减少老年代的全堆空间扫描，这能很大的提升GC效率。 我们可以看下官方文档对G1的展望（这段英文描述比较简单，我就不翻译了）： Future:G1 is planned as the long term replacement for the Concurrent Mark-Sweep Collector (CMS). Comparing G1 with CMS, there are differences that make G1 a better solution. One difference is that G1 is a compacting collector.G1 compacts sufficiently to completely avoid the use of fine-grained free lists for allocation, and instead relies on regions. This considerably simplifies parts of the collector, and mostly eliminates potential fragmentation issues. Also, G1 offers more predictable garbage collection pauses than the CMS collector, and allows users to specify desired pause targets. 3 总结查了下度娘有关G1的文章，绝大部分文章对G1的介绍都是停留在JDK7或更早期的实现很多结论已经存在较大偏差了，甚至一些过去的GC选项已经不再推荐使用。举个例子，JDK9中JVM和GC日志进行了重构，如PrintGCDetails已经被标记为废弃，而PrintGCDateStamps已经被移除，指定它会导致JVM无法启动。 本文对CMS和G1的介绍绝大部分内容也是基于JDK7，新版本中的内容有一点介绍，倒没做过多介绍（本人对新版本JVM还没有深入研究），后面有机会可以再出专门的文章来重点介绍。 4 参考《深入理解Java虚拟机》《HotSpot实战》《极客时间专栏》 本文转载自猿人谷博客，原文连接点这里，版权归原作者所有。微信公众号：猿人谷如果您认为阅读这篇博客让您有些收获，不妨点击一下右下角的【推荐】如果您希望与我交流互动，欢迎关注微信公众号本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文连接。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/categories/JAVA/"},{"name":"JVM","slug":"JAVA/JVM","permalink":"http://huermosi.xyz/categories/JAVA/JVM/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/tags/JAVA/"},{"name":"JVM","slug":"JVM","permalink":"http://huermosi.xyz/tags/JVM/"}]},{"title":"解决Docker容器内挂载目录无权限问题：cannot open directory. Permission denied","slug":"docker/docker-ls-permission-denied","date":"2019-10-06T13:06:59.000Z","updated":"2021-04-08T08:49:31.556Z","comments":true,"path":"2019/20200216210659/","link":"","permalink":"http://huermosi.xyz/2019/20200216210659/","excerpt":"","text":"将主机中的目录挂在到容器中后，进入到Docker容器内对应的目录，运行命令ls后提示权限不够： 1ls: cannot open directory .: Permission denied 解决方法：在docker run命令加上--privileged=true参数，给容器开启特权。","categories":[{"name":"DOCKER","slug":"DOCKER","permalink":"http://huermosi.xyz/categories/DOCKER/"}],"tags":[{"name":"DOCKER","slug":"DOCKER","permalink":"http://huermosi.xyz/tags/DOCKER/"}]},{"title":"运行自己的Docker Registry","slug":"docker/docker-local-registry","date":"2019-10-06T04:32:48.000Z","updated":"2021-04-08T08:49:31.556Z","comments":true,"path":"2019/20200214123248/","link":"","permalink":"http://huermosi.xyz/2019/20200214123248/","excerpt":"","text":"有时候我们可能希望构建和存储包含不想被公开的信息或数据的镜像。这时候有两种选择。 利用Docker Hub上的私有仓库。 运行自己的Registry。 Docker公司开源了他们用于运行Docker Registry的代码，我们可以基于此代码在内部运行自己的Registry。 从容器运行Registry1$ docker run -p 5001:5000 registry 该命令将会启动一个运行Registry应用的容器，Registry应用运行在容器的5000端口，并将容器的5000端口绑定到本地宿主机的5001端口。 使用新的Registry下面我们将本地已经存在的镜像huyaoban/static_web上传到我们的新Registry上去。 找到镜像的ID 123$ docker images huyaoban/static_webREPOSITORY TAG IMAGE ID CREATED SIZEhuyaoban/static_web latest 9f98ad5f2a2f 3 days ago 152 MB 给镜像打标签镜像ID为9f98ad5f2a2f，并使用新的Registry给镜像打上标签。为了指定新的Registry目的地址，需要在镜像名前加上主机名（也可以用IP）和端口前缀。 1$ docker tag 9f98ad5f2a2f 127.0.0.1:5001/huyaoban/static_web push镜像到新的Registry 1$ docker push 127.0.0.1:5001/huyaoban/static_web 使用新的镜像 1$ docker run -ti 127.0.0.1:5001/huyaoban/static_web","categories":[{"name":"DOCKER","slug":"DOCKER","permalink":"http://huermosi.xyz/categories/DOCKER/"}],"tags":[{"name":"DOCKER","slug":"DOCKER","permalink":"http://huermosi.xyz/tags/DOCKER/"},{"name":"REGISTRY","slug":"REGISTRY","permalink":"http://huermosi.xyz/tags/REGISTRY/"}]},{"title":"使用Docker构建静态网站","slug":"docker/build-static-website-use-docker","date":"2019-10-05T13:25:52.000Z","updated":"2021-04-08T08:49:31.550Z","comments":true,"path":"2019/20200216212552/","link":"","permalink":"http://huermosi.xyz/2019/20200216212552/","excerpt":"","text":"将Docker作为本地Web开发环境是使用Docker的一个最简单的场景。这个环境可以完全重现生产环境，保证开发环境和部署环境一致。下面从将Nginx安装到容器来架构一个简单的网站开始。这个网站暂且命名为Sample。 创建构建上下文目录，并创建初始化的Dockerfile文件123$ mkdir sample$ cd sample$ touch Dockerfile 添加nginx配置12345$ cd sample$ mdkir nginx$ cd nginx$ touch global.conf$ touch nginx.conf global.conf文件内容如下 12345678910server &#123; listen 0.0.0.0:80; server_name _; root &#x2F;var&#x2F;www&#x2F;html&#x2F;website; index index.html index.htm; access_log &#x2F;var&#x2F;log&#x2F;nginx&#x2F;default_access.log; error_log &#x2F;var&#x2F;log&#x2F;nginx&#x2F;default_error.log;&#125; nginx.conf文件内容如下 123456789101112131415161718192021user root;worker_processes 4;pid &#x2F;run&#x2F;nginx.pid;daemon off;events &#123;&#125;http &#123; sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 2048; include &#x2F;etc&#x2F;nginx&#x2F;mime.types; default_type application&#x2F;octet-stream; access_log &#x2F;var&#x2F;log&#x2F;nginx&#x2F;access.log; error_log &#x2F;var&#x2F;log&#x2F;nginx&#x2F;error.log; gzip on; gzip_disable &quot;msie6&quot;; include &#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;*.conf;&#125; 添加我们的静态网站，很简单，就一个index.html文件1234$ cd sample$ mdkir website$ cd website$ touch index.html index.html文件内容如下 12345678&lt;html&gt;&lt;head&gt;&lt;title&gt;This is a test website&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h3&gt;This is a test website&lt;/h3&gt;&lt;/body&gt;&lt;/html&gt; 最后看一下我们的Dockerfile文件的内容123456789FROM ubuntu:14.04MAINTAINER Jerry \"xxxxx@163.com\"ENV REFRESHED_AT 2019-02-15RUN apt-get updateRUN apt-get -y -q install nginxRUN mkdir -p /var/www/htmlADD nginx/global.conf /etc/nginx/conf.d/ADD nginx/nginx.conf /etc/nginx/nginx.confEXPOSE 80 构建镜像所有文件准备好后，我们开始构建镜像，并取名为huyaoban/nginx 12$ cd sample$ docker build -t huyaoban/nginx . 使用构建成功的镜像创建容器12$ cd sample$ docker run -d -p 80 --privileged=true --name website -v $PWD/website:/var/www/html/website huyaoban/nginx nginx 这里使用-v参数将宿主机构建上下文的website目录挂载到容器的/var/www/html/website目录。在nginx的配置里，已经指定了这个目录为nginx服务器的工作目录。 查看容器80端口绑定的宿主机端口 12$ docker port 8830a464f0c2 800.0.0.0:32773 访问宿主机的32773端口试试，打开的就是Sample网站。由于我们已经将构建上下文的website目录挂在到了容器中，直接修改构建上下文website中的index.html文件，Sample网站会自动更新。","categories":[{"name":"DOCKER","slug":"DOCKER","permalink":"http://huermosi.xyz/categories/DOCKER/"}],"tags":[{"name":"DOCKER","slug":"DOCKER","permalink":"http://huermosi.xyz/tags/DOCKER/"}]},{"title":"Dockerfile指令","slug":"docker/dockerfile-command","date":"2019-10-04T14:18:49.000Z","updated":"2021-04-08T08:49:31.557Z","comments":true,"path":"2019/20200212221849/","link":"","permalink":"http://huermosi.xyz/2019/20200212221849/","excerpt":"","text":"Dockerfile由一系列的指令和参数组成。每条指令，如FROM，都必须为大写字母。Dockerfile中的指令会按顺序从上到下执行，所以应该根据需要合理安排指令的顺序。Dockerfile中可以放入很多指令，这些指令包括CMD、ENTRYPOINT、ADD、COPY、VOLUME、WORKDIR、USER、ONBUILD、ENV、FROM、MAINTAINER、RUN和EXPOSE等。可以在http://docs.docker.com/reference/builder/查看Dockerfile中可以使用的全部指令清单。 1. CMDCMD指令用于指定一个容器启动时要运行的命令。docker run中指定的命令会覆盖CMD中的命令。如果Dockerfile中有多条CMD指令，只有最后一条有效。 12CMD [\"/bin/bash\"]CMD [\"/bin/bash\", \"-l\"] 2. ENTRYPOINT会使CMD指令失效。docker run命令行中的任何参数都会被当作参数再次传递给ENTRYPOINT指令中指定的命令。如果docker run命令行中没有指定参数，则会把CMD中的命令传给ENTRYPOINT。可在docker run中用--entrypoint覆盖ENTRYPOINT指令。 12ENTRYPOINT [\"/usr/sbin/nginx\"]ENTRYPOINT [\"/usr/sbin/nginx\", \"-g\", \"daemon off;\"] 3. WORKDIRWORKDIR指令用来为容器设置工作目录，也可以为Dockerfile中的一系列指令设置工作目录。ENTRYPOINT和CMD指定的程序会在这个目录下执行。 1234WORKDIR /opt/webapp/dbRUN bundle installWORKDIR /opt/webappENTRYPOINT [\"rackup\"] 可在docker run中用-w标志在运行时覆盖工作目录。 12$ docker run -ti -w /var/log ubuntu pwd/var/log 4. ENVENV指令用来在镜像构建过程中设置环境变量。这些环境变量也会被持久保存到从我们的镜像创建的任何容器中。 12ENV TARGET_DIR /opt/appWORKDIR $TARGET_DIR 在docker run中可用-e标志来传递环境变量，这些变量只会在运行时有效。 12$ docker run -ti -e \"WEB_PORT=8080\" ubuntu envWEB_PORT=8080 5. USERUSER指令用来指定该镜像会以什么样的用户去运行。如果不通过USER指令指定用户，默认用户为root。 1USER nginx 基于该镜像启动的容器会以nginx用户的身份来运行。 可用如下方式指定用户。 123456USER usernameUSER username:groupUSER uidUSER uid:gidUSER username:gidUSER uid:group 在docker run中可以用-u标志来覆盖该指令的值。 6. VOLUMEVOLUME指令用来向基于镜像创建的容器添加卷。当我们因为某种原因不想把某些数据构建到镜像中，但是又希望在使用这个镜像启动的容器中使用这些数据，可以通过卷把数据挂在到容器中去。 一个卷是可以存在于一个或者多个容器内的特定的目录。其有如下特性： 卷可以在容器间共享和重用。 对卷的修改是立即生效的。 对卷的修改不会对镜像产生影响。 卷会一直存在知道没有任何容器再使用它。 卷功能让我们可以将数据（源代码）、数据库或者其他内容添加到镜像中而不是将这些内容提交到镜像中，并允许我们在多个容器间共享这些内容。 1VOLUME [\"/opt/project\", \"/data\"] 这条指令将会为基于此镜像创建的任何容器创建一个名为/opt/project和/data的挂载点。 在docker run中可以用-v标志来为容器挂在卷。 1$ docker run -d -p 80 --name website -v $PWD/website:/var/www/html/website huyaoban/nginx nginx 把宿主机的$PWD/website目录挂载到容器的/var/www/html/website目录。 更多关于卷的信息可以访问http://docs.docker.com/userguide/dockervolumes/ 7. ADDADD指令用来将构建环境下的文件和目录复制到镜像中。如果目的位置不存在，自动创建全路径目录，文件和目录权限为0755。 12ADD software.lic /opt/application/software.licADD http://wordpress.org/latest.zip /root/wordpress.zip 如果目的地址以/结尾，那么Docker就认为源位置指向的是目录；如果目的地址不是以/结尾，那么Docker就认为源位置指向的是文件。文件源也可以是URL格式。 将构建环境中的latest.tar.gz解压到镜像的/var/www/wordpress/目录下。目录下已存在的文件不会被覆盖。 1ADD latest.tar.gz /var/www/wordpress/ 归档文件解压特性只针对本地构建环境中的文件，URL方式的归档文件不会解压。 8. COPYCOPY指令用于复制构建上下文中的文件到镜像中，不做提取和解压。目的位置不存在时自动创建，目的位置必须是容器内的一个绝对路径。 1COPY conf.d/ /etc/apache2/ 将构建环境中conf.d目录中的文件复制到镜像的/etc/apache2/目录中。 9. ONBUILDONBUILD指令能为镜像添加触发器。当一个镜像被用做其他镜像的基础镜像时，该镜像中的触发器将会被执行。触发器会在构建过程中插入新指令，可认为这些指令是紧跟FROM之后指定的。 ONBUILD触发器只会在子镜像中执行，不会在孙子镜像中执行。 12ONBUILD ADD . /app/srcONBUILD RUN cd /app/src &amp;&amp; make 10. FROMFROM指令用于从某个基础镜像构建另外一个镜像。 123FROM ubuntu:14.04FROM centos:latestFROM huyaoban/centos-web 11. MAINTAINERMAINTAINER指令用于设置镜像的维护者信息。 1MAINTAINER Jerry \"xxxxx@163.com\" 12. RUNRUN指令用来在当前镜像中运行指定的命令。 123RUN apt-get updateRUN apt-get install -y nginxRUN [\"apt-get\", \"install\", \"-y\", \"nginx\"] 13. EXPOSEEXPOSE指令用来向外部公开容器的指定端口。可以指定多个EXPOSE指令来公开多个端口。 123EXPOSE 80EXPOSE 6379EXPOSE 2181","categories":[{"name":"DOCKER","slug":"DOCKER","permalink":"http://huermosi.xyz/categories/DOCKER/"}],"tags":[{"name":"DOCKER","slug":"DOCKER","permalink":"http://huermosi.xyz/tags/DOCKER/"},{"name":"DOCKERFILE","slug":"DOCKERFILE","permalink":"http://huermosi.xyz/tags/DOCKERFILE/"},{"name":"指令","slug":"指令","permalink":"http://huermosi.xyz/tags/%E6%8C%87%E4%BB%A4/"}]},{"title":"用Dockerfile文件和docker build命令构建镜像","slug":"docker/dockerfile-build","date":"2019-10-02T13:58:28.000Z","updated":"2021-04-08T08:49:31.556Z","comments":true,"path":"2019/20200212215828/","link":"","permalink":"http://huermosi.xyz/2019/20200212215828/","excerpt":"","text":"Docker推荐使用Dockerfile文件和docker build命令来构建镜像。对比docker commit构建镜像的方式，这种方式更灵活、更强大。Dockerfile使用基本的基于DSL语法的指令来构建一个Docker镜像，之后使用docker build命令基于该Dockerfile中的指令构建一个新的镜像。详细的Dockerfile指令看。 创建一个目录作为构建镜像的工作目录（也叫构建上下文），并在里面创建初始的Dockerfile。 123$ makedir static_web$ cd static_web$ touch Dockerfile Dockerfile文件内容 123456# version: 0.0.1FROM ubuntu:14.04MAINTAINER Jerry \"xxxx@163.com\"RUN apt-get updateRUN apt-get install -y nginxEXPOSE 80 使用docker build命令构建第一个镜像 12345$ cd static_web$ docker build -t=\"huyaoban/static_web\" .$ docker build -t=\"huyaoban/static_web:v1\" .$ docker build --no-cache -t=\"huyaoban/static_web:v1\" .$ docker build -t=\"huyaoban/static_web:v1\" git@github.com:jamtur01/docker-static_web -t选项为新镜像设置仓库和名称，也可以指定标签，如果不指定，默认为latest。--no-cache选项告诉Docker构建过程不要使用构建缓存。命令最后面的.告诉Docker到本地目录中去找Dockerfile文件。也可以指定一个Git仓库地址来指定Dockerfile的位置，Dockerfile文件必须在这个Git仓库的根目录下。","categories":[{"name":"DOCKER","slug":"DOCKER","permalink":"http://huermosi.xyz/categories/DOCKER/"}],"tags":[{"name":"DOCKER","slug":"DOCKER","permalink":"http://huermosi.xyz/tags/DOCKER/"},{"name":"镜像","slug":"镜像","permalink":"http://huermosi.xyz/tags/%E9%95%9C%E5%83%8F/"},{"name":"BUILD","slug":"BUILD","permalink":"http://huermosi.xyz/tags/BUILD/"},{"name":"DOCKERFILE","slug":"DOCKERFILE","permalink":"http://huermosi.xyz/tags/DOCKERFILE/"}]},{"title":"Docker容器的基本命令","slug":"docker/docker-basic-command","date":"2019-10-02T07:35:49.000Z","updated":"2021-04-08T08:49:31.552Z","comments":true,"path":"2019/20200212153549/","link":"","permalink":"http://huermosi.xyz/2019/20200212153549/","excerpt":"","text":"本文主要介绍一些Docker容器的基本命令，以便日后可方便快速查阅，方便他人的同时提升自己。 1 查看Docker信息1$ docker info 2 创建容器12345678910111213$ docker run -i -t ubuntu /bin/bash$ docker run --restart=always --name jerry_container -d ubuntu /bin/sh -c \"while true; do echo hello world; sleep 1; done;\"$ docker run --name jerry_container -i -t ubuntu /bin/bash将容器的80端口绑定到宿主机的一个随机端口$ docker run -d -p 80 --name static_web 9f98ad5f2a2f nginx -g \"daemon off;\"将容器的80端口绑定到宿主机的8080端口$ docker run -d -p 8080:80 --name static_web 9f98ad5f2a2f nginx -g \"daemon off;\"将容器的80端口绑定到宿主机的127.0.0.1这个IP的8080端口$ docker run -d -p 127.0.0.1:8080:80 --name static_web 9f98ad5f2a2f nginx -g \"daemon off;\"将容器的80端口绑定到宿主机的127.0.0.1这个IP的随机端口$ docker run -d -p 127.0.0.1::80 --name static_web 9f98ad5f2a2f nginx -g \"daemon off;\"将Dockerfile中EXPOSE指令中设置的所有端口绑定到宿主机的随机端口$ docker run -d -P --name static_web 9f98ad5f2a2f nginx -g \"daemon off;\" 以上命令都能创建容器，但是各个命令指定的参数不同。 -i参数开启容器中的标准输入STDIN。 -t参数告诉Docker为容器分配一个伪tty终端，和-i参数组合使用可以创建一个Shell交互式容器。 --name参数指定容器的名称。 -p参数将容器的端口绑定到宿主机上的端口，这样我们访问宿主机的端口，就相当于访问容器的端口。 -P参数将Dockerfile中EXPOSE指令中设置的所有端口绑定到宿主机的随机端口 -d参数创建一个守护式容器，即长期运行的容器。守护式容器没有交互式会话。 --restart参数由于某种错误导致容器停止，该参数可自动重启容器。值有always和on-failure。on-failure在容器退出码非0时才会自动重启，还可接受重启次数参数on-failure:5。 ubuntu和9f98ad5f2a2f指定容器使用的镜像。 /bin/bash和nginx指定容器启动时将要执行的命令。 3 启动停止容器1234$ docker start 9f98ad5f2a2f$ docker restart 9f98ad5f2a2f$ docker stop 9f98ad5f2a2f$ docker kill 9f98ad5f2a2f docker stop命令会向Docker容器进程发送SIGTERM信号。如果你想快速停止某个容器，可以使用docker kill命令来向容器进程发送SIGKILL信号。 4 查看容器列表查看运行中的容器 1$ docker ps 查看所有容器 1$ docker ps -a 查看最后一个容器 1$ docker ps -l 查看最近5个容器 1$ docker ps -n 5 5 查看容器日志附着到容器上 1$ docker attach 9f98ad5f2a2f 12345$ docker logs 9f98ad5f2a2f$ docker logs -f 9f98ad5f2a2f$ docker logs -ft 9f98ad5f2a2f$ docker logs --tail 10 9f98ad5f2a2f$ docker logs --tail 0 -f 9f98ad5f2a2f 6 容器内的进程查看容器内运行的进程 1$ docker top 9f98ad5f2a2f 在容器内部运行新进程 12$ docker exec -d 9f98ad5f2a2f touch /etc/new_config_file$ docker exec -i -t 9f98ad5f2a2f /bin/bash 在容器内运行的进程有两种类型：后台任务和交互式任务。 7 查看容器详细信息1$ docker inspect 9f98ad5f2a2f 8 查看容器的端口映射12$ docker port 95d266e2e9ed$ docker port 95d266e2e9ed 80 9 删除容器12$ docker rm 9f98ad5f2a2f$ docker rm `docker ps -a -q`","categories":[{"name":"DOCKER","slug":"DOCKER","permalink":"http://huermosi.xyz/categories/DOCKER/"}],"tags":[{"name":"DOCKER","slug":"DOCKER","permalink":"http://huermosi.xyz/tags/DOCKER/"},{"name":"容器","slug":"容器","permalink":"http://huermosi.xyz/tags/%E5%AE%B9%E5%99%A8/"},{"name":"命令","slug":"命令","permalink":"http://huermosi.xyz/tags/%E5%91%BD%E4%BB%A4/"}]},{"title":"Docker镜像","slug":"docker/docker-image-intro","date":"2019-10-01T12:15:41.000Z","updated":"2021-04-08T08:49:31.553Z","comments":true,"path":"2019/20200212201541/","link":"","permalink":"http://huermosi.xyz/2019/20200212201541/","excerpt":"","text":"Docker镜像是由文件系统叠加而成。最底端是一个引导文件系统bootfs。Docker镜像的第二层是root文件系统rootfs，它位于引导文件系统之上。 在传统的Linux引导过程中，root文件系统会最先以只读的方式加载，当引导结束并完成完整性检查之后，它才会被切换为读写模式。但是在Docker里，root文件系统永远只能是只读状态，并且Docker利用联合加载（union mount）技术又会在root文件系统层上加载更多的只读文件系统。联合加载指的是一次同时加载多个文件系统，但是在外面看起来只能看到一个文件系统。联合加载会将各层文件系统叠加到一起，这样最终的文件系统会包含所有底层的文件和目录。 Docker将这样的文件系统称为镜像。一个镜像可以放到另一个镜像的顶部。位于下面的镜像称为父镜像，可以依次类推，直到镜像栈的最底部，最底部的镜像称为基础镜像。最后，当从一个镜像启动容器时，Docker会在该镜像的最顶层加载一个读写文件系统。我们在Docker中运行的程序就是在这个读写层中执行的。 1 基本命令 列出镜像 123$ docker images$ docker images ubuntu$ docker images huyaoban/centos-web 拉取镜像 123$ docker pull centos$ docker pull ubuntu$ docker pull ubuntu:12.04 拉取镜像时可指定标签，不指定默认为latest。 查看镜像详细信息 12$ docker inspect huyaoban/centos-web$ docker inspect huyaoban/centos-web:httpd 查看镜像构建历史 1$ docker history 9f98ad5f2a2f 删除镜像 1$ docker rmi 37d32ce8889a 登录Docker Hub 1$ docker login 登录的认证信息将会保存到$HOME/.dockercfg文件中。 推送镜像到Docker Hub 1$ docker push huyaoban/centos-web:httpd 2 制作镜像制作Docker镜像有以下两种方法。 使用docker commit命令。 使用docker build命令和Dockerfile文件。 下面先介绍如何使用docker commit构建Docker镜像。通过Dockerfile构建镜像查看另一篇文章。 2.1 登录Docker Hub1$ docker login 2.2 从基础镜像创建一个容器1$ docker run -i -t ubuntu /bin/bash 2.2 在新创建的容器里安装我们需要的软件以及做其他配置2.4 配置完成后，提交所做的修改12$ docker commit 4aab3ce3cb76 huyaoban/centos-web$ docker commit -m=\"A new custom image\" --author=\"Jerry\" 4aab3ce3cb76 huyaoban/centos-web:httpd","categories":[{"name":"DOCKER","slug":"DOCKER","permalink":"http://huermosi.xyz/categories/DOCKER/"}],"tags":[{"name":"DOCKER","slug":"DOCKER","permalink":"http://huermosi.xyz/tags/DOCKER/"},{"name":"镜像","slug":"镜像","permalink":"http://huermosi.xyz/tags/%E9%95%9C%E5%83%8F/"},{"name":"BOOTFS","slug":"BOOTFS","permalink":"http://huermosi.xyz/tags/BOOTFS/"},{"name":"ROOTFS","slug":"ROOTFS","permalink":"http://huermosi.xyz/tags/ROOTFS/"},{"name":"COMMIT","slug":"COMMIT","permalink":"http://huermosi.xyz/tags/COMMIT/"},{"name":"BUILD","slug":"BUILD","permalink":"http://huermosi.xyz/tags/BUILD/"}]},{"title":"Docker简介","slug":"docker/docker-introduction","date":"2019-10-01T03:06:28.000Z","updated":"2021-04-08T08:49:31.554Z","comments":true,"path":"2019/95270212110628/","link":"","permalink":"http://huermosi.xyz/2019/95270212110628/","excerpt":"","text":"Docker是一个能够把开发的应用程序自动部署到容器的开源引擎。 1. Docker的核心组件 Docker客户端和服务器（守护进程） Docker镜像 Registry Docker容器 1.1. Docker客户端和服务器（守护进程）Docker是一个客户-服务器（C/S）架构的程序。Docker客户端只需向Docker服务器或守护进程发出请求，服务器或守护进程将完成所有工作并返回结果。 1.2. Docker镜像镜像是构建Docker世界的基石。用户基于镜像来运行自己的容器。镜像是基于联合（Union）文件系统的一种层式的结构，由一系列指令一步一步构建出来，类似于制作出来的模板。例如： 添加一个文件； 执行一个命令； 打开一个端口。 1.3. RegistryDocker用Registry来保存用户构建的镜像。Registry分为公共和私有两种。Docker公司运营的公共Registry叫做Docker Hub。我们也可以架设自己的私有Registry。 1.4. 容器容器是基于镜像启动起来的，容器中可以运行一个或多个进程。镜像是Docker生命周期中的构建或打包阶段，而容器则是启动或执行阶段。 2. Docker的技术组件Docker可以运行于任何安装了Linux内核的X64主机上。它包括以下几个部分。一个原生的Linux容器格式，Docker中称为libcontainer，或者很流行的容器平台lxc。libcontainer格式现在是Docker容器的默认格式。Linux内核的命名空间（namespace），用于隔离文件系统、进程和网络。 文件系统隔离：每个容器都有自己的root文件系统。 进程隔离：每个容器都运行在自己的进程环境中。 网络隔离：容器间的虚拟网络接口和IP地址都是分开的。 资源隔离和分组：使用cgroups（即control group，Linux的内核特性之一）将CPU和内存之类的资源独立分配给每个Docker容器。","categories":[{"name":"DOCKER","slug":"DOCKER","permalink":"http://huermosi.xyz/categories/DOCKER/"}],"tags":[{"name":"DOCKER","slug":"DOCKER","permalink":"http://huermosi.xyz/tags/DOCKER/"}]},{"title":"【JVM从小白学成大佬】7.小伙伴栽在了JVM的内存分配策略。。。","slug":"java/jvm-memory-allocation","date":"2019-09-28T12:48:35.000Z","updated":"2021-04-08T08:49:31.636Z","comments":true,"path":"2019/952703132234835/","link":"","permalink":"http://huermosi.xyz/2019/952703132234835/","excerpt":"","text":"周末有小伙伴留言说上周面试时被问到内存分配策略的问题，但回答的不够理想，小伙伴说之前公号里看过这一块的文章的，当时看时很清楚，也知道各个策略是干嘛的，但面试时脑子里清楚，心里很明白，但嘴里就是说不清楚，说出来的就是像云像雾又像风，最后面试官说他应该是不清楚这一块的内容 这里给小伙伴要再次说明下，任何知识点，先抓主干，再摸细节。对于面试来说，能把各个主干捋清楚，只要面试官要求不是太高，都是能过关的。毕竟jvm参数那么多，难不成面试官揪着各个参数的作用不放？如果真遇到这种太过揪细节的，只能说江湖路远，有缘再见！ 对象的内存分配，往大方向上讲，就是在堆上分配（但也可能经过JIT编译后被拆散为标量类型并间接地栈上分配），对象主要分配在新生代的Eden区上，如果启动了本地线程分配缓冲，将按线程优先在TLAB上分配。少数情况下可能会直接分配在老年代中。 1. 对象优先在Eden分配大多数情况下，对象在新生代Eden区中分配。当Eden区没有足够空间进行分配时，虚拟机将发起一次Minor GC（前面篇章中有介绍过Minor GC）。但也有一种情况，在内存担保机制下，无法安置的对象会直接进到老年代。 2. 大对象直接进入老年代大对象时指需要大量连续内存空间的Java对象，最典型的大对象就是那种很长的字符串以及数组。 虚拟机提供了一个-XX：PretenureSizeThreshold参数，令大于这个设置值的对象直接在老年代分配。目的就是避免在Eden区及两个Survivor区之间发生大量的内存复制。 3. 长期存活的对象将进入老年代虚拟机给每个对象定义了一个对象年龄（Age）计数器。如果对象在Eden出生并经过第一次Minor GC后仍然存活，并且能被Survivor容纳的话，将被移动到Survivor空间中，并且对象年龄设为1 。对象在Survivor区中没经过一次Minor GC，年龄就加1岁，当年龄达到15岁（默认值），就会被晋升到老年代中。 对象晋升老年代的年龄阈值，可以通过参数-XX：MaxTenuringThreshold设置。 接下来我们来回答JVM的分代年龄为什么是15？而不是16,20之类的呢？ 真的不是为什么不能是其它数（除了15），着实是臣妾做不到啊！ 事情是这样的，HotSpot虚拟机的对象头其中一部分用于存储对象自身的运行时数据，如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等，这部分数据的长度在32位和64位的虚拟机（未开启压缩指针）中分别为32bit和64bit，官方称它为“Mark word”。 例如，在32位的HotSpot虚拟机中，如果对象处于未被锁定的状态下，那么Mark Word的32bit空间中25bit用于存储对象哈希码，4bit用于存储对象分代年龄，2bit用于存储锁标志位，1bit固定为0 。 明白是什么原因了吗？对象的分代年龄占4位，也就是0000，最大值为1111也就是最大为15，而不可能为16，20之类的了。 4. 动态对象年龄判定为了能更好的适应不同程序的内存状况，虚拟机并不是永远地要求兑现过的年龄必须达到了MaxTenuringThreshold才能晋升老年代。 满足如下条件之一，对象能晋升老年代： 对象的年龄达到了MaxTenuringThreshold（默认15）能晋升老年代。 如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代，无须等到MaxTenuringThreshold中要求的年龄。 很多文章都只是注意到了上面描述的情况（包括阿里中间件公众号发的一篇文章里也只是这么简单的介绍），但如果只是这么认识的话，会发现在实际的内存回收中有悖于此条规定。 举个小栗子，如对象年龄5的占34%，年龄6的占36%，年龄7的占30%，按那两个标准，对象是不能进入老年代的，但Survivor都已经100%了啊？ 大家可以关注这个参数TargetSurvivorRatio，目标存活率，默认为50%。大致意思就是说年龄从小到大累加，如加入某个年龄段（如栗子中的年龄6）后，总占用超过Survivor空间TargetSurvivorRatio的时候，从该年龄段开始及大于的年龄对象就要进入老年代（即栗子中的年龄6,7对象）。动态对象年龄判断，主要是被TargetSurvivorRatio这个参数来控制。而且算的是年龄从小到大的累加和，而不是某个年龄段对象的大小。 5. 空间分配担保在发生Minor GC之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果这个条件成立，那么Minor GC可以确保是安全的。如果不成立，则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。如果允许，那么会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次Minor GC，尽管这次Minor GC是有风险的；如果小于，或者HandlePromotionFailure设置不允许冒险，那这时也要改为进行一次Full GC 。 上面说的风险是什么呢？我们知道，新生代使用复制收集算法，但为了内存利用率，只使用其中一个Survivor空间来作为轮换备份，因此当出现大量对象在Minor GC后仍然存活的情况（最极端的情况就是内存回收后新生代中所有对象都存活），就需要老年代进行分配担保，把Survivor无法容纳的对象直接进入老年代。 本文转载自猿人谷博客，原文连接点这里，版权归原作者所有。微信公众号：猿人谷如果您认为阅读这篇博客让您有些收获，不妨点击一下右下角的【推荐】如果您希望与我交流互动，欢迎关注微信公众号本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文连接。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/categories/JAVA/"},{"name":"JVM","slug":"JAVA/JVM","permalink":"http://huermosi.xyz/categories/JAVA/JVM/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/tags/JAVA/"},{"name":"JVM","slug":"JVM","permalink":"http://huermosi.xyz/tags/JVM/"}]},{"title":"【JVM从小白学成大佬】6.创建对象及对象的访问定位","slug":"java/java-object-locator","date":"2019-09-05T04:36:35.000Z","updated":"2021-04-08T08:49:31.611Z","comments":true,"path":"2019/95270313223635/","link":"","permalink":"http://huermosi.xyz/2019/95270313223635/","excerpt":"","text":"《JVM从小白学成大佬》系列推出到现在，收到了很多小伙伴的好评，也收到了一些小伙伴的建议，在此表示感谢。 有几个小伙伴提出了希望出一篇介绍对象的创建及访问，猿人谷向来是没有原则的，小伙们要求啥，咱就尽力满足，毕竟文章就是对自己学习的一个总结及和各位小伙伴交流学习的机会。话不多说，直接开撸！ 1 创建对象在Java程序运行过程中无时无刻都有对象被创建出来，java中对象可以采用new或反射或clone或反序列化的方法创建。接下来我们我们介绍在虚拟机中，对象（限于普通Java对象，不包括数组和Class对象等）的创建过程。 字节码new表示创建对象，虚拟机遇到该指令时，从栈顶取得目标对象在常量池中的索引，接着定位到目标对象的类型。接下来，虚拟机将根据该类的状态，采取相应的内存分配技术，在内存中分配实例空间，并完成实例数据和对象头的初始化。这样，一个对象就在JVM中创建好了。 实例的创建过程，首先根据从类常量池中获取对象类型信息并验证类是否已被解析过，若确保该类已被加载和正确解析，使用快速分配（fast allocation）技术为该类分配对象空间；若该类尚未解析过，则只能通过慢速分配（slow allocation）方式分配实例对象。实例的创建流程如下图所示。 对象创建的基本流程： 验证类已被解析。 获取instanceKlass，确保Klass已完全初始化。 若满足快速分配条件，则进入快速分配流程。 若不满足快速分配条件，或者快速分配失败，则进入慢速分配流程。 1.1 快速分配如果在实例分配之前已经完成了类型的解析，那么分配操作仅仅是在内存空间中划分可用内存，因此能以较高效率实现内存分配，这就是快速分配。 根据分配空间是来自于线程私有区域还是共享的堆空间，快速分配可以分为两种空间选择策略。HotSpot通过线程局部分配缓存技术(Thread-Local Allocation Buffers,即TLABs)可以在线程私有区域实现空间的分配。 可以通过VM选项UseTLAB来开启或关闭TLAB功能。 根据是否使用TLAB，快速分配方式有两种选择策略： 选择TLAB：首先尝试在TLAB中分配，因为TLAB是线程私有区域，故不需要加锁便能够确保线程安全。在分配一个新的对象空间时，将首先尝试在TLAB空间中分配对象空间，若分配空间的请求失败，则再尝试使用加锁机制在Eden区分配对象。 选择Eden空间：若失败，则尝试在共享的Eden区进行分配，Eden区是所有线程共享区域，需要保证线程安全，故采用原子操作进行分配。若分配失败，则再次尝试该操作，直到分配成功为止。 实例空间分配成功以后，将对实例进行初始化。待完成对象的空间分配和初始化后，就可以设置栈顶对象引用。当然，对象的空间分配和初始化操作都是基于从类常量池中获取对象类型并确保该类已被加载和正确解析的前提下进行的，如果类未被解析，则需要进行慢速分配。 1.2 慢速分配之所以成为慢速分配，正是因为在分配实例前需要对类进行解析，确保类及依赖类已得到正确的解析和初始化。慢速分配是调用InterpreterRuntime模块_new()进行的，实现代码如下。 12345678// 确保要初始化的类不是抽象类型klass-&gt;check_valid_for_instantiation(true, CHECK);// 确保类已初始化klass-&gt;initialize(CHECK);// 分配实例oop obj = klass-&gt;allocate_instance(CHECK);// 在线程栈中设置对象引用thread-&gt;set_vm_result(obj); 2 对象的访问定位建立对象是为了使用对象，Java程序需要通过栈上的reference数据来操作堆上的具体对象。由于reference类型在Java虚拟机规范中只规定了一个指向对象的引用，并没有定义这个引用应该通过何种方式去定位、访问堆中的对象的具体位置，所以对象访问方式也是取决于虚拟机实现而定的。 目前主流的访问方式有使用句柄和直接指针两种： 如果使用句柄访问的话，那么Java堆中将会划分出一块内存来作为句柄池，reference中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息，如下图所示。 如果使用直接指针访问，那么Java堆对象的布局中就必须考虑如何放置访问类型数据的相关信息，而reference中存储的直接就是对象地址。即使用直接指针访问在对象被移动时reference本身需要被修改，reference存储的就是对象地址。如下图所示。 这两种对象访问方式各有优势： 使用句柄来访问的最大好处就是reference中存储的是稳定的句柄地址，在对象被移动（垃圾收集时移动对象时非常普遍的行为）时只会改变句柄中的实例数据指针，而reference本身不需要修改。 使用直接指针访问方式的最大好处就是速度更快，它节省了一次指针定位的时间开销，由于对象的访问在Java中非常频繁，因此这类开销积少成多后也是一项非常可观的执行成本。 HotSpot就是使用第二种方式进行对象访问的，但从整个软件开发的范围来看，各种语言和框架使用句柄来访问的情况也十分常见。 本文转载自猿人谷博客，原文连接点这里，版权归原作者所有。微信公众号：猿人谷如果您认为阅读这篇博客让您有些收获，不妨点击一下右下角的【推荐】如果您希望与我交流互动，欢迎关注微信公众号本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文连接。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/categories/JAVA/"},{"name":"JVM","slug":"JAVA/JVM","permalink":"http://huermosi.xyz/categories/JAVA/JVM/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/tags/JAVA/"},{"name":"JVM","slug":"JVM","permalink":"http://huermosi.xyz/tags/JVM/"}]},{"title":"【JVM从小白学成大佬】5.垃圾收集器及内存分配策略","slug":"java/java-gc-collector-and-memory-allocation-strategy","date":"2019-08-29T14:06:48.000Z","updated":"2021-04-08T08:49:31.596Z","comments":true,"path":"2019/95270311220648/","link":"","permalink":"http://huermosi.xyz/2019/95270311220648/","excerpt":"","text":"前面介绍了垃圾回收算法，接下来我们介绍垃圾收集器和内存分配的策略。有没有一种牛逼的收集器像银弹一样适配所有场景？很明显，不可能有，不然我也没必要单独搞一篇文章来介绍垃圾收集器了。熟悉不同收集器的优缺点，在实际的场景中灵活运用，才是王道。 在开始介绍垃圾收集器前，我们可以剧透几点： 根据不同分代的特点，收集器可能不同。有些收集器可以同时用于新生代和老年代，而有些时候，则需要分别为新生代或老年代选用合适的收集器。一般来说，新生代收集器的收集频率较高，应选用性能高效的收集器；而老年代收集器收集次数相对较少，对空间较为敏感，应当避免选择基于复制算法的收集器。 在垃圾收集执行的时刻，应用程序需要暂停运行。 可以串行收集，也可以并行收集。 如果能做到并发收集（应用程序不必暂停），那绝对是很妙的事情。 如果收集行为可控，那也是很妙的事情。 希望大家带着下面的问题进行阅读，有目标的阅读，可能收获更多。 为什么没有一种牛逼的收集器像银弹一样适配所有场景？ CMS和G1的对比，你知道他两的区别吗？ 为什么CMS只能用作老年代收集器，而不能应用在新生代的收集？ 为什么JVM的分代年龄是15？而不是16,20之类的呢？ “动态对象年龄判定”里有个“天坑”哦，是啥坑呢？ 1 垃圾收集器GC线程与应用线程保持相对独立，当系统需要执行垃圾回收任务时，先停止工作线程，然后命令GC线程工作。以串行模式工作的收集器，称为串行收集器（即Serial Collector）。与之相对的是以并行模式工作的收集器，称为并行收集器（即Paraller Collector）。 1.1 串行收集器：Serial串行收集器采用单线程方式进行收集，且在GC线程工作时，系统不允许应用线程打扰。此时，应用程序进入暂停状态，即Stop-the-world。 Stop-the-world暂停时间的长短，是度量一款收集器性能高低的重要指标。 是针对新生代的垃圾回收器，基于标记-复制算法。 1.2 并行收集器：ParNew并行收集器充分利用了多处理器的优势，采用多个GC线程并行收集。可想而知，多条GC线程执行显然比只使用一条GC线程执行的效率更高。一般来说，与串行收集器相比，在多处理器环境下工作的并行收集器能够极大地缩短Stop-the-world时间。 针对新生代的垃圾回收器，标记-复制算法，可以看成是Serial的多线程版本 1.3 吞吐量优先收集器：Parallel Scavenge针对新生代的垃圾回收器，标记-复制算法，和ParNew类似，但更注重吞吐率。在ParNew的基础上演化而来的Parallel Scanvenge收集器被誉为“吞吐量优先”收集器。吞吐量就是CPU用于运行用户代码的时间与CPU总消耗时间的比值，即吞吐量=运行用户代码时间 /（运行用户代码时间 + 垃圾收集时间）。如虚拟机总运行了100分钟，其中垃圾收集花掉1分钟，那吞吐量就是99%。 Parallel Scanvenge收集器在ParNew的基础上提供了一组参数，用于配置期望的收集时间或吞吐量，然后以此为目标进行收集。 通过VM选项可以控制吞吐量的大致范围： -XX：MaxGCPauseMills：期望收集时间上限。用来控制收集对应用程序停顿的影响。 -XX：GCTimeRatio：期望的GC时间占总时间的比例，用来控制吞吐量。 -XX：UseAdaptiveSizePolicy：自动分代大小调节策略。 但要注意停顿时间与吞吐量这两个目标是相悖的，降低停顿时间的同时也会引起吞吐的降低。因此需要将目标控制在一个合理的范围中。 1.4 Serial Old收集器Serial Old是Serial收集器的老年代版本，单线程收集器，使用标记-整理算法。这个收集器的主要意义也是在于给Client模式下的虚拟机使用。 1.5 Parallel Old收集器Parallel Old是Parallel Scanvenge收集器的老年代版本，多线程收集器，使用标记-整理算法。 1.6 CMS收集器CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。 CMS收集器仅作用于老年代的收集，是基于标记-清除算法的，它的运作过程分为4个步骤： 初始标记（CMS initial mark） 并发标记（CMS concurrent mark） 重新标记（CMS remark） 并发清除（CMS concurrent sweep） 其中，初始标记、重新标记这两个步骤仍然需要Stop-the-world。初始标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快，并发标记阶段就是进行GC Roots Tracing的过程，而重新标记阶段则是为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始阶段稍长一些，但远比并发标记的时间短。 CMS以流水线方式拆分了收集周期，将耗时长的操作单元保持与应用线程并发执行。只将那些必需STW才能执行的操作单元单独拎出来，控制这些单元在恰当的时机运行，并能保证仅需短暂的时间就可以完成。这样，在整个收集周期内，只有两次短暂的暂停（初始标记和重新标记），达到了近似并发的目的。 CMS收集器优点： 并发收集 低停顿。 CMS收集器缺点： CMS收集器对CPU资源非常敏感。 CMS收集器无法处理浮动垃圾（Floating Garbage）。 CMS收集器是基于标记-清除算法，该算法的缺点都有。 CMS收集器之所以能够做到并发，根本原因在于采用基于“标记-清除”的算法并对算法过程进行了细粒度的分解。前面篇章介绍过标记-清除算法将产生大量的内存碎片这对新生代来说是难以接受的，因此新生代的收集器并未提供CMS版本。 1.7 G1收集器G1重新定义了堆空间，打破了原有的分代模型，将堆划分为一个个区域。这么做的目的是在进行收集时不必在全堆范围内进行，这是它最显著的特点。区域划分的好处就是带来了停顿时间可预测的收集模型：用户可以指定收集操作在多长时间内完成。即G1提供了接近实时的收集特性。 G1与CMS的特征对比如下： 特征 G1 CMS 并发和分代 是 是 最大化释放堆内存 是 否 低延时 是 是 吞吐量 高 低 压实 是 否 可预测性 强 弱 新生代和老年代的物理隔离 否 是 G1具备如下特点： 并行与并发：G1能充分利用多CPU、多核环境下的硬件优势，使用多个CPU来缩短Stop-the-world停顿的时间，部分其他收集器原来需要停顿Java线程执行的GC操作，G1收集器仍然可以通过并发的方式让Java程序继续运行。 分代收集 空间整合：与CMS的标记-清除算法不同，G1从整体来看是基于标记-整理算法实现的收集器，从局部（两个Region之间）上来看是基于“复制”算法实现的。但无论如何，这两种算法都意味着G1运作期间不会产生内存空间碎片，收集后能提供规整的可用内存。这种特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次GC。 可预测的停顿：这是G1相对于CMS的一个优势，降低停顿时间是G1和CMS共同的关注点。 在G1之前的其他收集器进行收集的范围都是整个新生代或者老年代，而G1不再是这样。在堆的结构设计时，G1打破了以往将收集范围固定在新生代或老年代的模式，G1将堆分成许多相同大小的区域单元，每个单元称为Region。Region是一块地址连续的内存空间，G1模块的组成如下图所示： G1收集器将整个Java堆划分为多个大小相等的独立区域（Region），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分Region（不需要连续）的集合。G1收集器之所以能建立可预测的停顿时间模型，是因为它可以有计划地避免在整个Java堆中进行全区域的垃圾收集。G1会通过一个合理的计算模型，计算出每个Region的收集成本并量化，这样一来，收集器在给定了“停顿”时间限制的情况下，总是能选择一组恰当的Regions作为收集目标，让其收集开销满足这个限制条件，以此达到实时收集的目的。 对于打算从CMS或者ParallelOld收集器迁移过来的应用，按照官方的建议，如果发现符合如下特征，可以考虑更换成G1收集器以追求更佳性能： 实时数据占用了超过半数的堆空间； 对象分配率或“晋升”的速度变化明显； 期望消除耗时较长的GC或停顿（超过0.5——1秒）。 原文如下：Applications running today with either the CMS or the ParallelOld garbage collector would benefit switching to G1 if the application has one or more of the following traits. More than 50% of the Java heap is occupied with live data. The rate of object allocation rate or promotion varies significantly. Undesired long garbage collection or compaction pauses (longer than 0.5 to 1 second) G1收集的运作过程大致如下： 初始标记（Initial Marking）：仅仅只是标记一下GC Roots能直接关联到的对象，并且修改TAMS（Next Top at Mark Start）的值，让下一阶段用户程序并发运行时，能在正确可用的Region中创建新对象，这阶段需要停顿线程，但耗时很短。 并发标记（Concurrent Marking）：是从GC Roots开始堆中对象进行可达性分析，找出存活的对象，这阶段耗时较长，但可与用户程序并发执行。 最终标记（Final Marking）：是为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程Remembered Set Logs里面，最终标记阶段需要把Remembered Set Logs的数据合并到Remembered Set中，这阶段需要停顿线程，但是可并行执行。 筛选回收（Live Data Counting and Evacuation）：首先对各个Region的回收价值和成本进行排序，根据用户所期望的GC停顿时间来制定回收计划。这个阶段也可以做到与用户程序一起并发执行，但是因为只回收一部分Region，时间是用户可控制的，而且停顿用户线程将大幅提高收集效率。 我们可以看下官方文档对G1的展望（这段英文描述比较简单，我就不翻译了）： Future:G1 is planned as the long term replacement for the Concurrent Mark-Sweep Collector (CMS). Comparing G1 with CMS, there are differences that make G1 a better solution. One difference is that G1 is a compacting collector. G1 compacts sufficiently to completely avoid the use of fine-grained free lists for allocation, and instead relies on regions. This considerably simplifies parts of the collector, and mostly eliminates potential fragmentation issues. Also, G1 offers more predictable garbage collection pauses than the CMS collector, and allows users to specify desired pause targets. 2 内存分配策略对象的内存分配，往大方向上讲，就是在堆上分配（但也可能经过JIT编译后被拆散为标量类型并间接地栈上分配），对象主要分配在新生代的Eden区上，如果启动了本地线程分配缓冲，将按线程优先在TLAB上分配。少数情况下可能会直接分配在老年代中。 2.1 对象优先在Eden分配大多数情况下，对象在新生代Eden区中分配。当Eden区没有足够空间进行分配时，虚拟机将发起一次Minor GC（前面篇章中有介绍过Minor GC）。但也有一种情况，在内存担保机制下，无法安置的对象会直接进到老年代。 2.2 大对象直接进入老年代大对象时指需要大量连续内存空间的Java对象，最典型的大对象就是那种很长的字符串以及数组。 虚拟机提供了一个-XX：PretenureSizeThreshold参数，令大于这个设置值的对象直接在老年代分配。目的就是避免在Eden区及两个Survivor区之间发生大量的内存复制。 2.3 长期存活的对象将进入老年代虚拟机给每个对象定义了一个对象年龄（Age）计数器。如果对象在Eden出生并经过第一次Minor GC后仍然存活，并且能被Survivor容纳的话，将被移动到Survivor空间中，并且对象年龄设为1 。对象在Survivor区中没经过一次Minor GC，年龄就加1岁，当年龄达到15岁（默认值），就会被晋升到老年代中。 对象晋升老年代的年龄阈值，可以通过参数-XX：MaxTenuringThreshold设置。 接下来我们来回答为什么JVM的分代年龄为什么是15？而不是16,20之类的呢？ 真的不是为什么不能是其它数（除了15），着实是臣妾做不到啊！ 事情是这样的，HotSpot虚拟机的对象头其中一部分用于存储对象自身的运行时数据，如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等，这部分数据的长度在32位和64位的虚拟机（未开启压缩指针）中分别为32bit和64bit，官方称它为“Mark word”。 例如，在32位的HotSpot虚拟机中，如果对象处于未被锁定的状态下，那么Mark Word的32bit空间中25bit用于存储对象哈希码，4bit用于存储对象分代年龄，2bit用于存储锁标志位，1bit固定为0 。 明白是什么原因了吗？对象的分代年龄占4位，也就是0000，最大值为1111也就是最大为15，而不可能为16，20之类的了。 2.4 动态对象年龄判定为了能更好的适应不同程序的内存状况，虚拟机并不是永远地要求兑现过的年龄必须达到了MaxTenuringThreshold才能晋升老年代。 满足如下条件之一，对象能晋升老年代： 1.对象的年龄达到了MaxTenuringThreshold（默认15）能晋升老年代。 2.如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代，无须等到MaxTenuringThreshold中要求的年龄。 很多文章都只是注意到了上面描述的情况（包括阿里中间件公众号发的一篇文章里也只是这么简单的介绍，当时给它们后台留过言说明情况），但如果只是这么认识的话，会发现在实际的内存回收中有悖于此条规定。 举个小栗子，如如对象年龄5的占30%，年龄6的占36%，年龄7的占34%，按那两个标准，对象是不能进入老年代的，但Survivor都已经100%了啊？ 大家可以关注这个参数TargetSurvivorRatio，目标存活率，默认为50%。大致意思就是说年龄从小到大累加，如加入某个年龄段（如栗子中的年龄6）后，总占用超过Survivor空间*TargetSurvivorRatio的时候，从该年龄段开始及大于的年龄对象就要进入老年代（即栗子中的年龄6对象，就是年龄6和年龄7晋升到老年代）。动态对象年龄判断，主要是被TargetSurvivorRatio这个参数来控制。而且算的是年龄从小到大的累加和，而不是某个年龄段对象的大小。 2.5 空间分配担保在发生Minor GC之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果这个条件成立，那么Minor GC可以确保是安全的。如果不成立，则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。如果允许，那么会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次Minor GC，尽管这次Minor GC是有风险的；如果小于，或者HandlePromotionFailure设置不允许冒险，那这时也要改为进行一次Full GC 。 上面说的风险是什么呢？我们知道，新生代使用复制收集算法，但为了内存利用率，只使用其中一个Survivor空间来作为轮换备份，因此当出现大量对象在Minor GC后仍然存活的情况（最极端的情况就是内存回收后新生代中所有对象都存活），就需要老年代进行分配担保，把Survivor无法容纳的对象直接进入老年代。 3.总结脑图 本文转载自猿人谷博客，原文连接点这里，版权归原作者所有。微信公众号：猿人谷如果您认为阅读这篇博客让您有些收获，不妨点击一下右下角的【推荐】如果您希望与我交流互动，欢迎关注微信公众号本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文连接。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/categories/JAVA/"},{"name":"JVM","slug":"JAVA/JVM","permalink":"http://huermosi.xyz/categories/JAVA/JVM/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/tags/JAVA/"},{"name":"JVM","slug":"JVM","permalink":"http://huermosi.xyz/tags/JVM/"}]},{"title":"【JVM从小白学成大佬】4.Java虚拟机何谓垃圾及垃圾回收算法","slug":"java/java-gc-algorithm","date":"2019-08-26T13:16:30.000Z","updated":"2021-04-08T08:49:31.581Z","comments":true,"path":"2019/95270311211630/","link":"","permalink":"http://huermosi.xyz/2019/95270311211630/","excerpt":"","text":"在Java中内存是由虚拟机自动管理的，虚拟机在内存中划出一片区域，作为满足程序内存分配请求的空间。内存的创建仍然是由程序猿来显示指定的，但是对象的释放却对程序猿是透明的。就是解放了程序猿手动回收内存的工作，交给垃圾回收器来自动回收。 在虚拟机中，释放哪些不再被使用的对象所占空间的过程称为垃圾收集（Garbage Collection，GC）。负责垃圾收集的程序模块，成为垃圾收集器（Garbage Collector）。 既然虚拟机已经帮我们把垃圾自动处理了，为什么还要去了解GC和内存分配呢？ 当需要排查各种内存溢出、内存泄漏问题时，当垃圾收集成为系统达到更高并发量的瓶颈时，我们就需要对虚拟机的自动管理技术实施必要的监控和调节了。这也是JVM调优，故障排查，重点需要掌握的知识了。 本篇我们的重点是介绍何谓垃圾及垃圾回收算法，那我们就要弄清到底什么是垃圾？能不能设计一种强大的垃圾回收算法来解决垃圾回收的所有问题？肯定是没有的，后面介绍的每一种垃圾回收算法都有它得天独厚的优点，也有它避之不及的缺点。针对具体的场景，灵活运用方是上策。 希望大家能带着如下问题进行学习，会收获更大。 什么是垃圾？ 如何回收垃圾？ 有没有一种垃圾回收算法能像银弹一样解决所有垃圾所有？ GC的分类是什么样的？（Minor GC、Major GC、Full GC） Stop-the-world是什么？ 如何避免全堆扫描？ 1 垃圾回收在堆里面存放着Java世界中几乎所有的对象实例，垃圾收集器在堆进行回收前，第一件事就是要确定这些对象之中哪些还“存活”着，哪些已经“死亡”（即不可能再被任何途径使用的对象）。垃圾回收，其实就是将已经分配出去的，但不再使用的内存回收，以便能够再次分配。在Java虚拟机的规范中，垃圾指的就是死亡的对象所占据的堆空间。 那怎么确定一个对象是存活还是死亡呢？ 1.1 引用计数算法给对象中添加一个引用计数器，每当有一个地方引用它时，计数器值就加1；当引用失效时，计数器值就减1；任何时刻计数器为0的对象就是不可能再被使用的。也就是说，需要截获所有的引用更新操作，并且相应地增减目标对象的计数器。 题外话：记得研一那段时间对iOS开发感兴趣，找个公司去实习，现学现搞iOS开发，当时是做了一个模拟炒股的app。用的就是Objective-C，这门语言起初管理内存的方式就是用的这种引用计数算法，不过后面也有了自动管理内存。接触的对象多了，发现很多东西在本质的原理有非常多的相似之处。 引用计数算法缺点： 需要额外的空间来存储计数器，以及繁琐的更新操作。 无法处理循环引用对象。 其中无法处理循环引用对象，算是引用计数法的一个重大漏洞。 1.2 可达性分析算法可达性是指，如果一个对象会被至少一个在程序中的变量通过直接或间接的方式被其他可达的对象引用，则称该对象是可达的（reachable）。更准确的说，一个对象只有满足下述两个条件之一，就会被判断为可达的： 本身是根对象。根（Root）是指由堆以外空间访问的对象。JVM中会将一组对象标记为根，包括全局变量、部分系统类，以及栈中引用的对象，如当前栈帧中的局部变量和参数。 被一个可达的对象引用。 这个算法的基本思路就是通过一系列的成为”GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链（Reference Chain），当一个对象到GC Roots没有任何引用链相连（即从GC Roots到这个对象不可达），则证明此对象是不可用的。 GC Roots又是什么呢？可以暂时理解为由堆外指向堆内的引用。 在Java语言中，可以作为GC Roots的对象包括下面几种： 虚拟机栈（栈帧中的本地变量表）中引用的对象。 方法区中类静态属性引用的对象。 方法区中常量引用的对象。 本地方法栈中JNI（即一般说的Native方法）引用的对象。 已启动且未停止的Java线程。 可达性分析算法可以解决引用计数算法不能解决的循环引用问题。举个例子，即便对象a和b相互引用，只要从GC Roots出发无法到达a或者b，那么可达性分析便不会将它们加入存活对象合集之中。 关于Java中的引用的定义及分类（强引用、软引用、弱引用、虚引用）会在单独出一篇进行详细介绍，Java引用的内容虽然有点冷门，但是很多公司面试的常考点。 可达性分析算法本身虽然很简明，但是在实践中还是有不少其他问题需要解决的。比如，在多线程环境下，其他线程可能会更新已经访问过的对象中的引用，从而造成误报（将引用设置为null）或者漏报（将引用设置为未被访问过的对象）。误杀还可以接受，Java虚拟机至多损失了部分垃圾回收的机会。漏报就问题大了，因为垃圾回收器可能回收事实上仍被引用的对象内存。一旦从原引用访问已经被回收了的对象，则很有可能会直接导致Java虚拟机奔溃。 2 垃圾回收算法上面我们介绍什么是Java中的垃圾，接下来我们就开始介绍如何高效的回收这些垃圾。 2.1 标记-清除算法标记-清除（Mark-Sweep）算法可以分为两个阶段： 标记阶段：标记出所有可以回收的对象。 清除阶段：回收所有已被标记的对象，释放这部分空间。 该算法存在如下不足： 内存碎片。由于Java虚拟机的堆中对象必须是连续分布的，因此可能出现总空闲内存足够，但是无法分配的极端情况。无法找到足够的连续内存，而不得不提前触发一次垃圾收集动作。 分配效率较低。如果是一块连续的内存空间，那么我们可以通过指针加法（pointer bumping）来做分配。而对于空闲列表，Java虚拟机则需要逐个访问列表中的项，来查询能够放入新建对象的空闲内存。 标记-清除算法的示意图如下： 2.2 复制算法复制算法的过程如下： 划分区域：将内存区域按比例划分为1个Eden区作为分配对象的“主战场”和2个幸存区（即Survivor空间，划分为2个等比例的from区和to区）。 复制：收集时，打扫“战场”，将Eden区中仍存活的对象复制到某一块幸存区中。 清除：由于上一阶段已确保仍存活的对象已被妥善安置，现在可以“清理战场”了，释放Eden区和另一块幸存区。 晋升：如在“复制”阶段，一块幸存区接纳不了所有的“幸存”对象。则直接晋升到老年代。 该算法解决了内存碎片化问题，但堆空间的使用效率极其低下。在对象存活率较高时，需要进行较多的复制操作，效率会变得很低。 2.3 标记-整理算法该算法分为两个阶段： 标记阶段：标记出所有可以回收的对象。 压缩阶段：将标记阶段的对象移动到空间的一端，释放剩余的空间。 该算法的标记过程与标记-清除算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。 解决了内存碎片的问题，也规避了复制算法只能利用一半内存区域的弊端。看起来很美好，但它对内存变动更频繁，需要整理所有存活对象的引用地址，在效率上比复制算法要差很多。 标记-整理算法的示意图如下： 2.4 分代收集算法分代收集算法倒并没有什么新的思想，只是根据对象存活周期的不同将内存划分为几块。一般是把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。 在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用标记-清理算法或标记-整理算法来进行回收。 3 HotSpot算法实现3.1 枚举根节点以可达性分析中从GC Roots节点找引用链这个操作为例，可作为GC Roots的节点主要在全局性的引用（例如常量或类静态属性）与执行上下文（例如栈帧中的本地变量表）中。上面介绍可达性分析算法时有详细介绍GC Roots，可以参看上面。 3.2 安全点（Safepoint）安全点，即程序执行时并非在所有地方都能停顿下来开始GC，只有在到达安全点时才能暂停。Safepoint的选定既不能太少以至于让GC等待时间太长，也不能过于频繁以致于过分增大运行时的负荷。 安全点的初始目的并不是让其他线程停下，而是找到一个稳定的执行状态。在这个执行状态下，Java虚拟机的堆栈不会发生变化。这么一来，垃圾回收器便能够“安全”地执行可达性分析。只要不离开这个安全点，Java虚拟机便能够在垃圾回收的同时，继续运行这段本地代码。 程序运行时并非在所有地方都能停顿下来开始GC，只有在到达安全点时才能暂停。安全点的选定基本上是以程序“是否具有让程序长时间执行的特征”为标准进行选定的。“长时间执行”的最明显特征就是指令序列复用，例如方法调用、循环跳转、异常跳转等，所以具有这些功能的指令才会产生Safepoint。 对于安全点，另一个需要考虑的问题就是如何在GC发生时让所有线程（这里不包括执行JNI调用的线程）都“跑”到最近的安全点上再停顿下来。 两种解决方案： 抢先式中断（Preemptive Suspension）抢先式中断不需要线程的执行代码主动去配合，在GC发生时，首先把所有线程全部中断，如果发现有线程中断的地方不在安全点上，就恢复线程，让它“跑”到安全点上。现在几乎没有虚拟机采用这种方式来暂停线程从而响应GC事件。 主动式中断（Voluntary Suspension）主动式中断的思想是当GC需要中断线程的时候，不直接对线程操作，仅仅简单地设置一个标志，各个线程执行时主动去轮询这个标志，发现中断标志为真时就自己中断挂起。轮询标志地地方和安全点是重合的，另外再加上创建对象需要分配内存的地方。 3.3 安全区域指在一段代码片段中，引用关系不会发生变化。在这个区域中任意地方开始GC都是安全的。也可以把Safe Region看作是被扩展了的Safepoint。 4 扩展知识4.1 GC分类Minor GC： 针对新生代。 指发生在新生代的垃圾收集动作，因为java对象大多都具备朝生夕死的特性，所以Minor GC非常频繁，一般回收速度也比较快。 触发条件：Eden空间满时。 Major GC： 针对老年代。 指发生在老年代的GC，出现了Major GC，经常会伴随至少一次的Minor GC（但非绝对的，在Parallel Scavenge收集器的收集策略里就有直接进行Major GC的策略选择过程）。Major GC的速度一般会比Minor GC慢10倍以上。 触发条件：Minor GC 会将对象移到老年代中，如果此时老年代空间不够，那么触发 Major GC。 Full GC： 清理整个堆空间。一定意义上Full GC 可以说是 Minor GC 和 Major GC 的结合。 触发条件：调用System.gc()；老年代空间不足；空间分配担保失败。 4.2 Stop-the-worldGC进行时必须停顿所有Java执行线程，这就是Stop-the-world。 可达性分析时必须在一个能确保一致性的快照中进行，这里“一致性”的意思是指在整个分析期间整个执行系统看起来就像被冻结在某个时间点上，不可以出现分析过程中对象引用关系还在不断变化的情况，这一点不满足的话分析结果准确性就无法得到保证。 Stop-the-world是通过安全点机制来实现的。当Java虚拟机接收到Stop-the-world请求，它便会等待所有的线程都到达安全点，才允许请求Stop-the-world的线程进行独占的工作。 4.3 卡表有个场景，老年代的对象可能引用新生代的对象，那标记存活对象的时候，需要扫描老年代中的所有对象。因为该对象拥有对新生代对象的引用，那么这个引用也会被称为GC Roots。那不是得又做全堆扫描？成本太高了吧。 HotSpot给出的解决方案是一项叫做卡表（Card Table）的技术。该技术将整个堆划分为一个个大小为512字节的卡，并且维护一个卡表，用来存储每张卡的一个标识位。这个标识位代表对应的卡是否可能存有指向新生代对象的引用。如果可能存在，那么我们就认为这张卡是脏的。 在进行Minor GC的时候，我们便可以不用扫描整个老年代，而是在卡表中寻找脏卡，并将脏卡中的对象加入到Minor GC的GC Roots里。当完成所有脏卡的扫描之后，Java虚拟机便会将所有脏卡的标识位清零。 想要保证每个可能有指向新生代对象引用的卡都被标记为脏卡，那么Java虚拟机需要截获每个引用型实例变量的写操作，并作出对应的写标识位操作。 卡表能用于减少老年代的全堆空间扫描，这能很大的提升GC效率。 本文转载自猿人谷博客，原文连接点这里，版权归原作者所有。微信公众号：猿人谷如果您认为阅读这篇博客让您有些收获，不妨点击一下右下角的【推荐】如果您希望与我交流互动，欢迎关注微信公众号本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文连接。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/categories/JAVA/"},{"name":"JVM","slug":"JAVA/JVM","permalink":"http://huermosi.xyz/categories/JAVA/JVM/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/tags/JAVA/"},{"name":"JVM","slug":"JVM","permalink":"http://huermosi.xyz/tags/JVM/"}]},{"title":"【JVM从小白学成大佬】3.深入解析强引用、软引用、弱引用、幻象引用","slug":"java/java-reference","date":"2019-08-24T14:20:24.000Z","updated":"2021-04-08T08:49:31.626Z","comments":true,"path":"2019/95270310215343/","link":"","permalink":"http://huermosi.xyz/2019/95270310215343/","excerpt":"","text":"关于强引用、软引用、弱引用、幻象引用的区别，在很多公司的面试题中经常出现，可能有些小伙伴觉得这个知识点比较冷门，但其实大家在开发中经常用到，如new一个对象的时候就是强引用的应用。 在java语言中，除了原始数据类型（boolean、byte、short、char、int、float、double、long）的变量，其他所有都是所谓的引用类型，指向各种不同的对象。理解这些引用的区别，对于掌握java对象生命周期和JVM内部相关机制非常有帮助。也有助于更深刻的理解底层对象生命周期、垃圾收集机制等，对设计可靠的缓存框架、诊断应用OOM等问题也大有裨益。 这四种应用主要的区别体现在对象不同的可达性状态和对垃圾收集的影响，他们之间的可达性状态可以参看下图： 1.强引用（strong reference）强引用就是我们最常见的普通对象引用（如new 一个对象），只要还有强引用指向一个对象，就表明此对象还“活着”。在强引用面前，即使JVM内存空间不足，JVM宁愿抛出OutOfMemoryError运行时错误（OOM），让程序异常终止，也不会靠回收强引用对象来解决内存不足的问题。对于一个普通的对象，如果没有其他的引用关系，只要超过了引用的作用域或者显式地将相应（强）引用赋值为null，就意味着此对象可以被垃圾收集了。但要注意的是，并不是赋值为null后就立马被垃圾回收，具体的回收时机还是要看垃圾收集策略的。 1Object obj = new Object(); 2.软引用（soft reference）软引用相对强引用要弱化一些，可以让对象豁免一些垃圾收集。当内存空间足够的时候，垃圾回收器不会回收它。只有当JVM认定内存空间不足时才会去回收软引用指向的对象。JVM会确保在抛出OOM前清理软引用指向的对象，而且JVM是很聪明的，会尽可能优先回收长时间闲置不用的软引用指向的对象，对那些刚构建的或刚使用过的软引用指向的对象尽可能的保留。基于软引用的这些特性，软引用可以用来实现很多内存敏感点的缓存场景，即如果内存还有空闲，可以暂时缓存一些业务场景所需的数据，当内存不足时就可以清理掉，等后面再需要时，可以重新获取并再次缓存。这样就确保在使用缓存提升性能的同时，不会导致耗尽内存。 软引用通常可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，java虚拟机就会把这个软引用加入到与之关联的引用队列中。 12345Object obj = new Object();SoftReference&lt;Object&gt; sf = new SoftReference&lt;Object&gt;(obj);obj = null;//有时候会返回nullsf.get(); 通过上面的代码可以看出sf是对obj的一个软引用，当sf对象还没有被销毁前，sf.get()可以获取到这个对象，如果已被销毁，则返回null。 正确使用软引用的示例代码如下： 123456789101112SoftReference&lt;List&lt;Foo&gt;&gt; ref = new SoftReference&lt;List&lt;Foo&gt;&gt;(new LinkedList&lt;Foo&gt;()); // somewhere else in your code, you create a Foo that you want to add to the listList&lt;Foo&gt; list = ref.get();if (list != null)&#123; list.add(foo);&#125;else&#123; // list is gone; do whatever is appropriate&#125; 在使用软引用的时候必须检查引用是否为null。因为垃圾收集器可能在任意时刻回收软引用，如果不做是否null的判断，可能会出现NullPointerException的异常。 总的来说，软引用是用来描述一些还有用但并非必需的对象。对于软引用关联着的对象，在系统将要发生内存溢出异常之前，将会把这些对象列进回收范围之中进行第二次回收。如果这次回收还没有足够的内存，才会抛出内存溢出异常。 3.弱引用（weak reference）弱引用指向的对象是一种十分临近finalize状态的情况，当弱引用被清除的时候，就符合finalize的条件了。弱引用与软引用最大的区别就是弱引用比软引用的生命周期更短暂。垃圾回收器会扫描它所管辖的内存区域的过程中，只要发现弱引用的对象，不管内存空间是否有空闲，都会立刻回收它。如同前面我说过的，具体的回收时机还是要看垃圾回收策略的，因此那些弱引用的对象并不是说只要达到弱引用状态就会立马被回收。 基于弱引用的这些特性，弱引用同样可以应用在很多需要缓存的场景。 1234567Object obj = new Object();WeakReference&lt;Object&gt; wf = new WeakReference&lt;Object&gt;(obj);obj = null;//有时候会返回nullwf.get();//返回是否被垃圾回收器标记为即将回收的垃圾wf.isEnQueued(); 4.幻象引用（phantom reference）幻象引用，也有被说成是虚引用或幽灵引用。幻象引用并不会决定对象的生命周期。即如果一个对象仅持有虚引用，就相当于没有任何引用一样，在任何时候都可能被垃圾回收器回收。不能通过它访问对象，幻象引用仅仅是提供了一种确保对象被finalize以后，做某些事情的机制（如做所谓的Post-Mortem清理机制），也有人利用幻象引用监控对象的创建和销毁。 1234567Object obj = new Object();PhantomReference&lt;Object&gt; pf = new PhantomReference&lt;Object&gt;(obj);obj=null;//永远返回nullpf.get();//返回是否从内存中已经删除pf.isEnQueued(); 幻象引用的get方法永远返回null，主要用于检查对象是否已经从内存中删除。 5.生存还是死亡通过上面对四种引用类型的分析，你可能发现有些对象即使不可达，但也并非是“非死不可”的，这个时候它们暂时处于“缓刑”阶段，要真正宣告一个对象死亡，至少要经历两次标记过程：如果对象在进行可达性分析后发现没有与GC Roots相连接的引用链，那它将会被第一次标记并且进行一次筛选，筛选的条件是此对象是否有必要执行finalize()方法。当对象没有覆盖finalize()方法，或者finalize()方法已经被虚拟机调用过，虚拟机将这两种情况都视为“没有必要执行”。 如果这个对象被判定为有必要执行finalize()方法，那么这个对象将会放置在一个叫做F-Queue的队列之中，并在稍后被一个由虚拟机自动建立的、低优先级的Finalizer线程去执行它。这里所谓的“执行”是指虚拟机会触发这个方法，但并不承诺会等待它运行结束，这样做的原因是，如果一个对象在finalize()方法中执行缓慢，或者发生了死循环（更极端的情况），将很可能会导致F-Queue队列中其他对象永久处于等待，甚至导致整个内存回收系统奔溃。finalize()方法是对象逃脱死亡命运的最后一次机会，稍后GC将对F-Queue中的对象进行第二次小规模的标记，如果对象要在finalize()中成功拯救自己——只要重新与引用链上的任何一个对象建立关联即可。譬如把自己（this关键字）赋值给某个类变量或者对象的成员变量，那在第二次标记时它将被移除出“即将回收”的集合；如果对象这时候还没有逃脱，那基本上它就真的被回收了。 任何一个对象的finalize()方法都只会被系统自动调用一次，如果对象面临下一次回收，它的finalize()方法不会被再次执行。 6.总结对象的可达性是JVM垃圾收集器决定如何处理对象的一个重要考虑指标。 所有引用类型都是抽象类java.lang.ref.Reference的子类，子类里提供了get()方法。通过上面的分析中可以得知，除了幻象引用（因为get永远返回null），如果对象还没有被销毁，都可以通过get方法获取原有对象。其实有个非常关键的注意点，利用软引用和弱引用，我们可以将访问到的对象，重新指向强引用，也就是人为的改变了对象的可达性状态。所以对于软引用、弱引用之类，垃圾收集器可能会存在二次确认的问题，以确保处于弱引用状态的对象没有改变为强引用。 但是有个问题，如果我们错误的保持了强引用（比如，赋值给了static变量），那么对象可能就没有机会变回类似弱引用的可达性状态了，就会产生内存泄露。所以，检查弱引用指向对象是否被垃圾收集，也是诊断是否有特定内存泄露的一个思路，我们的框架使用到弱引用又怀疑有内存泄露，就可以从这个角度检查。 对于软引用、弱引用、幻象引用可以配合引用队列（ReferenceQueue）来使用，特别是幻象引用，get方法只返回null，如果再不指定引用队列，基本就没有任何意义了。 上面分析了四种引用类型的使用，熟悉这几种应用类型对深入理解JVM也大有裨益。 参考： 《深入理解Java虚拟机》 http://www.kdgregory.com/index.php?page=java.refobj 本文转载自猿人谷博客，原文连接点这里，版权归原作者所有。微信公众号：猿人谷如果您认为阅读这篇博客让您有些收获，不妨点击一下右下角的【推荐】如果您希望与我交流互动，欢迎关注微信公众号本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文连接。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/categories/JAVA/"},{"name":"JVM","slug":"JAVA/JVM","permalink":"http://huermosi.xyz/categories/JAVA/JVM/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/tags/JAVA/"},{"name":"JVM","slug":"JVM","permalink":"http://huermosi.xyz/tags/JVM/"},{"name":"引用","slug":"引用","permalink":"http://huermosi.xyz/tags/%E5%BC%95%E7%94%A8/"}]},{"title":"【JVM从小白学成大佬】2.Java虚拟机运行时数据区","slug":"java/jvm-runtime-data-area","date":"2019-08-20T13:53:43.000Z","updated":"2021-04-08T08:49:31.638Z","comments":true,"path":"2019/95270310222024/","link":"","permalink":"http://huermosi.xyz/2019/95270310222024/","excerpt":"","text":"这里我们先说句题外话，相信大家在面试中经常被问到介绍Java内存模型，我在面试别人时也会经常问这个问题。但是，往往都会令我比较尴尬，我还话音未落，面试者就会“背诵”一段（Java虚拟机时有堆、方法去、虚拟机栈，吧啦吧啦。。。），估计心里还一脸自豪的想幸好哥提前在网上搜过，早有准备。每每这个时候，我都不忍心打断，因为“背诵”的真的太顺畅了！ 这也怪不得面试者，首先Java虚拟机方面的知识，对中高级程序猿来说，工作中正面接触Java虚拟机的东西不多。其次，这个其次咱得好好唠唠，网上搜个Java内存模型，度娘推的第一页大都是介绍Java运行时数据区的，起到了一定的误导作用，大写的尴尬。 本篇将给各位小伙伴先详细介绍Java运行时数据区的组成，Java内存模型也是虚拟机里面的重点，后面会单独抽出一篇来进行介绍。 1.运行时数据区介绍程序运行所需的内存空间，有些是不能在编译期就能确定，得要在运行期根据实际运行状况动态地在系统中创建。Java虚拟机在执行Java程序的过程中会把它所管理的内存划分为若干个不同的数据区域。这些区域都有各自的用途，以及创建和销毁的时间，有的区域随着虚拟机进程的启动而存在，有些区域则依赖用户线程的启动和结束而建立和销毁。 如图所示，堆和方法区是所有线程共享的公共区域，堆和方法区所占的内存空间是由JVM负责管理的，在该区域内的内存分配是由HotSpot的内存管理模块维护的，而内存的释放工作则由垃圾收集器自动完成。虚拟机栈、本地方法栈、程序计数器是线程的私有区域，每个线程都关联着唯一的栈和程序计数器，并仅能使用属于自己的那份栈空间和程序计算器来执行程序。 2.堆（Heap）对于大多数应用来说，Java堆（Java Heap）是Java虚拟机所管理的内存中最大的一块。堆是可供各个线程共享的运行时内存区域，在虚拟机启动的时候就被创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。这一点在Java虚拟机规范中的描述就是：所有的对象实例以及数组对象都要在堆上分配。但是随着JIT编译器的发展与逃逸分析技术逐渐成熟，栈上分配、标量替换优化技术将会导致一些微妙的变化发生，所有的对象都分配在堆上也渐渐变得不是那么“绝对”了。 Java堆的容量可以是固定的，也可以随着程序执行的需求动态扩展，并在不需要过多空间时自动收缩。Java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可。如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError异常。 Java堆是垃圾收集器管理的主要区域，因此很多时候也被称做“GC堆”（Garbage Collected Heap）。从内存回收的角度来看，由于现在收集器基本都采用分代收集算法，Java虚拟机将堆划分为新生代和老年代。其中，新生代又被分为Eden区，以及两个大小相同的Survivor区（From Survivor，To Survivor）。默认情况下，Java虚拟机采取的是一种动态分配的策略（JVM参数-XX:+UsePSAdaptiveSurvivorSizePolicy），根据生成对象的速率，以及Survivor区的使用情况，动态调整Eden区和Survivor区的比例。也可以通过参数（SurvivorRatio）来调整这个比例，SurvivorRatio这个参数就是新生代中Eden区与Survivor区的容量比值，默认是8，代表Eden：Survivor=8:1。 是否可能有两个对象共用一段内存的事故？当调用new指令时，会在Eden区划出一块作为存储对象的内存。由于堆空间是线程共享的，因此直接在这里边划空间是需要进行同步的。否则，将有可能出现两个对象共用一段内存的事故。解决方法就是，Java堆中可能划出多个线程私有的分配缓冲区TLAB（Thread Local Allocation Buffer，对应的虚拟机参数-XX：+UseTLAB，默认开启）。 具体来说，每个线程可以向Java虚拟机申请一段连续内存，比如2048字节，作为线程私有的TLAB。这个操作需要加锁，线程需要维护两个指针（实际上可能更多，但重要也就两个），一个指向TLAB中空余内存的起始位置，一个则指向TLAB末尾。接下来的new指令，便可以直接通过指针加法（bump the pointer），也有人叫做指针碰撞来实现，即把指向空余内存位置的指针加上所请求的字节数。如果加法后空余内存指针的值仍小于或等于指向末尾的指针，则代表分配成功。否则，TLAB已经没有足够的空间来满足本次新建操作。这个时候，便需要当前线程重新申请新的TLAB。 3.方法区（Method Area）方法区与堆一样是线程共享的，在虚拟机启动的时候创建，方法区可视为堆的一个逻辑部分，但是它却有一个别名叫做Non-Heap（非堆），目的应该是与Java堆区分开来。 方法区类似于传统语言编译后的代码存储区域，它存储每个类的结构信息，如： 常量池 域 方法数据 方法和构造函数的字节码 类、实例、接口初始化时用到的特殊方法备注：《深入理解Java虚拟机》里将方法区归纳为用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 Java虚拟机规范对方法区的限制非常宽松，除了和Java堆一样不需要连续的内存和可以选择固定大小或者可扩展外，还可以选择不实现垃圾收集。这区域的内存回收目标主要是针对常量池的回收和对类型的卸载。 4.程序计数器（Program Counter Register）Java虚拟机可以支持多条线程同时执行，每一条Java虚拟机线程都有自己的程序计数器。在任意时刻，一条Java虚拟机线程只会执行一个方法的代码，这个正在被线程执行的方法称为该线程的当前方法（current methon）。如果这个方法不是native的，那程序计数器保存的就是Java虚拟机正在执行的字节码指令的地址。如果该方法是native方法，那程序计数器的值为空（undefined）。程序计数器的容量至少应当保存一个returnAddress类型的数据或者一个与平台相关的本地指针的值。 程序计数器是一块较小的内存空间，它可以看作是当前线程所执行的字节码的行号指示器。此内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError情况的区域。 5.虚拟机栈（VM Stack）每一条Java虚拟机线程都有自己私有的Java虚拟机栈，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法在执行的同时都会创建一个栈帧（stack frame）用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。 Java虚拟机栈可能发生如下异常情况： 如果线程请求分配的栈容量超过Java虚拟机栈允许的最大容量，Java虚拟机将会抛出一个StackOverflowError异常。 如果Java虚拟机栈可以动态扩展，并且在尝试扩展的时候无法申请到足够的内存，或者在创建新的线程时没有足够的内存区创建对应的虚拟机栈，那Java虚拟机将会抛出一个OutOfMemoryError异常 6.本地方法栈（Native Method Stack）本地方法栈与虚拟机栈所发挥的作用是非常相似的，它们之间的区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则为虚拟机使用到的native方法服务。 Java虚拟机规范允许本地方法栈实现成固定大小或者根据计算来动态扩展和收缩。如果采用固定大小的本地方法栈，那么每一个线程的本地方法栈容量可以在创建栈的时候独立选定。 与虚拟机栈一样，本地方法栈区域也会抛出StackOverflowError和OutOfMemoryError异常。 7.扩展知识点7.1 栈上分配和逃逸分析在栈中分配的基本思路是这样的：分析局部变量的作用域仅限于方法内部，则JVM直接在栈帧内分配对象空间，避免在堆中分配。这个分析过程称为逃逸分析（也有叫逸出分析），而栈帧内分配对象的方式称为栈上分配。 这样做的目的是减少新生代的收集次数，间接提高JVM性能。虚拟机是允许堆逃逸分析开关进行配置的，从Sun Java 6u23以后，HotSpot默认开启逃逸分析。 7.2 栈帧栈帧是用于支持虚拟机进行方法调用和方法执行的数据结构，它是虚拟机运行时数据区中的虚拟机栈的栈元素。栈帧存储了方法的局部变量表、操作数栈、动态连接和方法返回地址等信息。每一个方法从调用开始至执行完成的过程，都对应着一个栈帧在虚拟机栈里面从入栈到出栈的过程。 在编译程序代码的时候，栈帧中需要多大的局部变量表，多深的操作数栈都已经完全确定了，并且写入到方法表的Code属性之中。因此一个栈帧需要分配多少内存，不会收到程序运行期变量数据的影响，而仅仅取决于具体的虚拟机实现。 一个线程中的方法调用链可能会很长，很多方法都同时处于执行状态。对于执行引擎来说，在活动线程中，只有位于栈顶的栈帧才是有效的，称为当前栈帧（Current Stack Frame），与这个栈帧相关联的方法称为当前方法（Current Method）。执行引擎运行的所有字节码指令都只针对当前栈帧进行操作。栈帧的概念结构如下： 8.运行时数据区脑图 本文转载自猿人谷博客，原文连接点这里，版权归原作者所有。微信公众号：猿人谷如果您认为阅读这篇博客让您有些收获，不妨点击一下右下角的【推荐】如果您希望与我交流互动，欢迎关注微信公众号本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文连接。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/categories/JAVA/"},{"name":"JVM","slug":"JAVA/JVM","permalink":"http://huermosi.xyz/categories/JAVA/JVM/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/tags/JAVA/"},{"name":"JVM","slug":"JVM","permalink":"http://huermosi.xyz/tags/JVM/"}]},{"title":"【JVM从小白学成大佬】1.开篇","slug":"java/jvm-start","date":"2019-08-20T13:50:08.000Z","updated":"2021-04-08T08:49:31.644Z","comments":true,"path":"2019/95270313215608/","link":"","permalink":"http://huermosi.xyz/2019/95270313215608/","excerpt":"","text":"JVM的重要性毋庸置疑，可以毫不夸张的说Java虚拟机是整个Java平台的基石。 JVM方面的知识，也一直是BAT等大厂面试考核的重点。特别是JVM调优，故障排查性能调优，你知道该从哪些方面入手吗？ 此专栏结合自己对JVM的理解，从java运行时数据区、java内存模型(JMM)、什么是垃圾回收，垃圾回收算法、垃圾收集器、内存分配与回收策略等，也会介绍安全点、安全区域等知识点，还有什么是“卡带”？ 相信学习完专栏，在面试时被问到JVM，你定能跟面试官侃侃而谈，话聊人生！ 关于JVM的历史以及重要性我就不做介绍，网上能搜到一大堆。我们就直接上干货，希望能最简单直白语言来深入浅出，一步步揭开JVM的面纱。 本专栏将分为如下几个大模块进行分析： 开篇介绍 Java运行时数据区。 什么是垃圾回收？ 常用垃圾回收算法及HotSpot的算法实现。 垃圾收集器。 内存分配与回收策略。此模块也会延展一些内存回收时的坑。 Java内存模型（JMM）。 头脑风暴（即JVM必备题）。 专栏将会围绕如下脑图进行展开（此脑图会持续进行完善）： 备注：此脑图会持续维护并进行完善，高清无码大图请留言告知。 鬼知道我看了多少相关的书籍和博客，都没找到适合我的，不得不自己动手撸几篇，请叫我知识的搬运工。此专栏算是我在学习Java虚拟机的读书笔记及经验总结，在小伙伴们阅读专栏的过程中，如有哪一块的知识想了解，可以留言告知。 参考书籍：《深入理解Java虚拟机》、《Java虚拟机规范》（Java SE 8版）、《HotSpot实战》。 本文转载自猿人谷博客，原文连接点这里，版权归原作者所有。微信公众号：猿人谷如果您认为阅读这篇博客让您有些收获，不妨点击一下右下角的【推荐】如果您希望与我交流互动，欢迎关注微信公众号本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文连接。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/categories/JAVA/"},{"name":"JVM","slug":"JAVA/JVM","permalink":"http://huermosi.xyz/categories/JAVA/JVM/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/tags/JAVA/"},{"name":"JVM","slug":"JVM","permalink":"http://huermosi.xyz/tags/JVM/"}]},{"title":"ExecutorService详解","slug":"java/executorservice-01","date":"2018-08-14T06:57:36.000Z","updated":"2021-06-04T07:16:09.131Z","comments":true,"path":"2018/9527888016/","link":"","permalink":"http://huermosi.xyz/2018/9527888016/","excerpt":"","text":"0. 前言在我们的日常开发中，难免会使用到线程，部分还会用到多线程并发问题。我们知道，线程的创建和释放，需要占用不小的内存和资源。如果每次需要使用线程时，都new 一个Thread的话，难免会造成资源的浪费，而且可以无限制创建，之间相互竞争，会导致过多占用系统资源导致系统瘫痪。不利于扩展，比如如定时执行、定期执行、线程中断，所以很有必要了解下ExecutorService的使用。 ExecutorService是Java提供的线程池，也就是说，每次我们需要使用线程的时候，可以通过ExecutorService获得线程。它可以有效控制最大并发线程数，提高系统资源的使用率，同时避免过多资源竞争，避免堵塞，同时提供定时执行、定期执行、单线程、并发数控制等功能，也不用使用TimerTask了。 1. ExecutorService的创建方式1234567public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) 所有线程池最终都是通过这个方法来创建的。 corePoolSize :核心线程数，一旦创建将不会再释放。如果创建的线程数还没有达到指定的核心线程数量，将会继续创建新的核心线程，直到达到最大核心线程数后，核心线程数将不在增加；如果没有空闲的核心线程，同时又未达到最大线程数，则将继续创建非核心线程；如果核心线程数等于最大线程数，则当核心线程都处于激活状态时，任务将被挂起，等待空闲线程来执行。 maximumPoolSize :最大线程数，允许创建的最大线程数量。如果最大线程数等于核心线程数，则无法创建非核心线程；如果非核心线程处于空闲时，超过设置的空闲时间，则将被回收，释放占用的资源。 keepAliveTime :也就是当线程空闲时，所允许保存的最大时间，超过这个时间，线程将被释放销毁，但只针对于非核心线程。 unit :时间单位，TimeUnit.SECONDS等。 workQueue :任务队列，存储暂时无法执行的任务，等待空闲线程来执行任务。 threadFactory : 线程工程，用于创建线程。 handler :当线程边界和队列容量已经达到最大时，用于处理阻塞时的程序 2. 线程池的类型2.1 可缓存线程池1ExecutorService cachePool = Executors.newCachedThreadPool(); 看看它的具体创建方式： 12345public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; 通过它的创建方式可以知道，创建的都是非核心线程，而且最大线程数为Interge的最大值，空闲线程存活时间是1分钟。如果有大量耗时的任务，则不适该创建方式。它只适用于生命周期短的任务。 2.2 单线程池1ExecutorService singlePool = Executors.newSingleThreadExecutor(); 顾名思义，也就是创建一个核心线程： 123456public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125; 只用一个线程来执行任务，保证任务按FIFO顺序一个个执行。 2.3 固定线程数线程池1Executors.newFixedThreadPool(3); 12345public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; 也就是创建固定数量的可复用的线程数，来执行任务。当线程数达到最大核心线程数，则加入队列等待有空闲线程时再执行。 2.4 固定线程数，支持定时和周期性任务1ExecutorService scheduledPool = Executors.newScheduledThreadPool(5); 12345public ScheduledThreadPoolExecutor(int corePoolSize) &#123; super(corePoolSize, Integer.MAX_VALUE, DEFAULT_KEEPALIVE_MILLIS, MILLISECONDS, new DelayedWorkQueue());&#125; 可用于替代handler.postDelay和Timer定时器等延时和周期性任务。 12public ScheduledFuture&lt;?&gt; schedule(Runnable command, long delay, TimeUnit unit); 1234public ScheduledFuture&lt;?&gt; scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit); 1234public ScheduledFuture&lt;?&gt; scheduleWithFixedDelay(Runnable command, long initialDelay, long delay, TimeUnit unit); scheduleAtFixedRate和sheduleWithFixedDelay有什么不同呢？ scheduleAtFixedRate：创建并执行一个在给定初始延迟后的定期操作，也就是将在 initialDelay 后开始执行，然后在initialDelay+period 后下一个任务执行，接着在 initialDelay + 2 * period 后执行，依此类推 ，也就是只在第一次任务执行时有延时。 sheduleWithFixedDelay：创建并执行一个在给定初始延迟后首次启用的定期操作，随后，在每一次执行终止和下一次执行开始之间都存在给定的延迟，即总时间是（initialDelay + period）*n 2.5 手动创建线程池1234private ExecutorService pool = new ThreadPoolExecutor(3, 10, 10L, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(512), Executors.defaultThreadFactory(), new ThreadPoolExecutor.AbortPolicy()); 可以根据自己的需求创建指定的核心线程数和总线程数。 3. 如何终止一个周期性任务呢？直接上源码你就懂了： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182public final class LgExecutorService &#123; private ConcurrentHashMap&lt;String, Future&gt; futureMap = new ConcurrentHashMap&lt;&gt;(); private ScheduledExecutorService executorService = new ScheduledThreadPoolExecutor(5); private LgExecutorService() &#123; &#125; private static final class InnerExecutorService &#123; private static final LgExecutorService INSTANCE = new LgExecutorService(); &#125; public static LgExecutorService getInstance() &#123; return InnerExecutorService.INSTANCE; &#125; public ConcurrentHashMap&lt;String, Future&gt; getFutureMap() &#123; return futureMap; &#125; public void execute(Runnable runnable) &#123; if (runnable != null) &#123; executorService.execute(runnable); &#125; &#125; /** * @param runnable * @param delay 延迟时间 * @param timeUnit 时间单位 */ public void sheduler(Runnable runnable, long delay, TimeUnit timeUnit) &#123; if (runnable != null) &#123; executorService.schedule(runnable, delay, timeUnit); &#125; &#125; /** * 执行延时周期性任务 * * @param runnable &#123;@code LgExecutorSercice.JobRunnable&#125; * @param initialDelay 延迟时间 * @param period 周期时间 * @param timeUnit 时间单位 */ public &lt;T extends JobRunnable&gt; void sheduler(T runnable, long initialDelay, long period, TimeUnit timeUnit) &#123; if (runnable != null) &#123; Future future = executorService.scheduleAtFixedRate(runnable, initialDelay, period, timeUnit); futureMap.put(runnable.getJobId(), future); &#125; &#125; public static abstract class JobRunnable implements Runnable &#123; private String jobId; public JobRunnable(@NonNull String jobId) &#123; this.jobId = jobId; &#125; /** * 强制终止定时线程 */ public void terminal() &#123; try &#123; Future future = LgExecutorService.getInstance().getFutureMap().remove(jobId); future.cancel(true); &#125; finally &#123; System.out.println(\"jobId \" + jobId + \" had cancel\"); &#125; &#125; public String getJobId() &#123; return jobId; &#125; &#125; &#125; 自己写个demo测试后就知道结果了，还是自己动下手吧。 本文转载自CSDN，原文连接点这里，版权归原作者所有，转载目的是为了避免以后找不到。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/categories/JAVA/"},{"name":"多线程","slug":"JAVA/多线程","permalink":"http://huermosi.xyz/categories/JAVA/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/tags/JAVA/"},{"name":"ExecutorService","slug":"ExecutorService","permalink":"http://huermosi.xyz/tags/ExecutorService/"},{"name":"Executors","slug":"Executors","permalink":"http://huermosi.xyz/tags/Executors/"},{"name":"Executor","slug":"Executor","permalink":"http://huermosi.xyz/tags/Executor/"}]},{"title":"说一下公平锁和非公平锁的区别？","slug":"java/java-fair-nonfair-lock","date":"2018-03-27T02:00:12.000Z","updated":"2021-04-08T08:49:31.568Z","comments":true,"path":"2018/95270327100012/","link":"","permalink":"http://huermosi.xyz/2018/95270327100012/","excerpt":"","text":"前言上次我们提到了乐观锁和悲观锁，那我们知道锁的类型还有很多种，我们今天简单聊一下，公平锁和非公平锁两口子，以及他们在我们代码中的实践。 正文开始聊之前，我先大概说一下他们两者的定义，帮大家回顾或者认识一下。 公平锁：多个线程按照申请锁的顺序去获得锁，线程会直接进入队列去排队，永远都是队列的第一位才能得到锁。 优点：所有的线程都能得到资源，不会饿死在队列中。 缺点：吞吐量会下降很多，队列里面除了第一个线程，其他的线程都会阻塞，cpu唤醒阻塞线程的开销会很大。 非公平锁：多个线程去获取锁的时候，会直接去尝试获取，获取不到，再去进入等待队列，如果能获取到，就直接获取到锁。 优点：可以减少CPU唤醒线程的开销，整体的吞吐效率会高点，CPU也不必取唤醒所有线程，会减少唤起线程的数量。 缺点：你们可能也发现了，这样可能导致队列中间的线程一直获取不到锁或者长时间获取不到锁，导致饿死。 其实在大家经常使用的ReentrantLock中就有相关公平锁，非公平锁的实现了。 大家还记得我在乐观锁、悲观锁章节提到的Sync类么，是ReentrantLock他本身的一个内部类，他继承了AbstractQueuedSynchronizer，我们在操作锁的大部分操作，都是Sync本身去实现的。 Sync呢又分别有两个子类：FairSync和NofairSync 他们子类的名字就可以见名知意了，公平和不公平那又是怎么在代码层面体现的呢？ 公平锁： 你可以看到，他加了一个hasQueuedPredecessors的判断，那他判断里面有些什么玩意呢？ 代码的大概意思也是判断当前的线程是不是位于同步队列的首位，是就是返回true，否就返回false。 我总觉得写到这里就应该差不多了，但是我坐下来，静静的思考之后发现，还是差了点什么。 上次聊过ReentrantLock了，但是AQS什么的我都只是提了一嘴，一个线程进来，他整个处理链路到底是怎样的呢？ 公平锁到底公平不公平呢？让我们一起跟着丙丙走进ReentrantLock的内心世界。 上面提了这么多，我想你应该是有所了解了，那一个线程进来ReentrantLock这个渣男是怎么不公平的呢？（默认是非公平锁） 我先画个图，帮助大家了解下细节： ReentrantLock的Sync继承了AbstractQueuedSynchronizer也就是我们常说的AQS 他也是ReentrantLock加锁释放锁的核心，大致的内容我之前一期提到了，我就不过多赘述了，他们看看一次加锁的过程吧。 A线程准备进去获取锁，首先判断了一下state状态，发现是0，所以可以CAS成功，并且修改了当前持有锁的线程为自己。 这个时候B线程也过来了，也是一上来先去判断了一下state状态，发现是1，那就CAS失败了，真晦气，只能乖乖去等待队列，等着唤醒了，先去睡一觉吧。 A持有久了，也有点腻了，准备释放掉锁，给别的仔一个机会，所以改了state状态，抹掉了持有锁线程的痕迹，准备去叫醒B。 这个时候有个带绿帽子的仔C过来了，发现state怎么是0啊，果断CAS修改为1，还修改了当前持有锁的线程为自己。 B线程被A叫醒准备去获取锁，发现state居然是1，CAS就失败了，只能失落的继续回去等待队列，路线还不忘骂A渣男，怎么骗自己，欺骗我的感情。 诺以上就是一个非公平锁的线程，这样的情况就有可能像B这样的线程长时间无法得到资源，优点就是可能有的线程减少了等待时间，提高了利用率。 现在都是默认非公平了，想要公平就得给构造器传值true。 1ReentrantLock lock = new ReentrantLock(true); 说完非公平，那我也说一下公平的过程吧： 线A现在想要获得锁，先去判断下state，发现也是0，去看了看队列，自己居然是第一位，果断修改了持有线程为自己。 线程b过来了，去判断一下state，嗯哼？居然是state=1，那cas就失败了呀，所以只能乖乖去排队了。 线程A暖男来了，持有没多久就释放了，改掉了所有的状态就去唤醒线程B了，这个时候线程C进来了，但是他先判断了下state发现是0，以为有戏，然后去看了看队列，发现前面有人了，作为新时代的良好市民，果断排队去了。 线程B得到A的召唤，去判断state了，发现值为0，自己也是队列的第一位，那很香呀，可以得到了。 总结：总结我不说话了，但是去获取锁判断的源码，箭头所指的位置，现在是不是都被我合理的解释了，当前线程，state，是否是0，是否是当前线程等等，都去思考下。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/categories/JAVA/"},{"name":"锁","slug":"JAVA/锁","permalink":"http://huermosi.xyz/categories/JAVA/%E9%94%81/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/tags/JAVA/"},{"name":"锁","slug":"锁","permalink":"http://huermosi.xyz/tags/%E9%94%81/"},{"name":"ReentrantLock","slug":"ReentrantLock","permalink":"http://huermosi.xyz/tags/ReentrantLock/"},{"name":"乐观锁","slug":"乐观锁","permalink":"http://huermosi.xyz/tags/%E4%B9%90%E8%A7%82%E9%94%81/"},{"name":"悲观锁","slug":"悲观锁","permalink":"http://huermosi.xyz/tags/%E6%82%B2%E8%A7%82%E9%94%81/"}]},{"title":"Cookie/Session/Token的区别","slug":"sessioncookie/cookie-session-token","date":"2018-03-22T06:50:54.000Z","updated":"2021-04-26T12:38:11.179Z","comments":true,"path":"2018/95270322145054/","link":"","permalink":"http://huermosi.xyz/2018/95270322145054/","excerpt":"","text":"在Web开发领域，相信大家对于Cookie和Session都很熟悉，Cookie和Session都是会话保持技术的解决方案。随着技术的发展，Token机制出现在我们面前，不过很多开发者对于Token和Cookie、Session的区别及使用场景分辨不清。 Cookie和Session的用途要知道我们访问网站都是通过HTTP协议或HTTPS协议来完成的，HTTP协议它本身是无状态的协议（即：服务器无法分辨哪些请求是来源于同个客户）。而业务层面会涉及到客户端与服务器端的交互（同网站下多个页面间能共享数据），此时服务器端必须要保持会话状态，这样才能进行用户身份的鉴别。 由于HTTP无状态的特性，如果要实话客户端和服务器端的会话保持，那就需要其它机制来实现，于是Cookie和Session应运而生。 通常情况下，Session和Cookie是搭配在一起使用的。 Token是什么上面说到的Session和Cookie机制来保持会话，会存在一个问题：客户端浏览器只要保存自己的SessionID即可，而服务器却要保存所有用户的Session信息，这对于服务器来说开销较大，而且不利用服务器的扩展（比如服务器集群时，Session如何同步存储就是个问题）！ 于是有人思考，如果把Session信息让客户端来保管而且无法伪造不就可以解决这个问题了？进而有了Token机制。 Token俗称为“令牌”，它的构成是： uid：用户唯一身份标识 timestamp：当前时间戳 sign：签名字符串，防止第三方伪造数据；签名密钥是存储在服务器端的，其它人无法知道 其它附加参数。 Token机制下的认证流程Token机制其实和Cookie机制极其相似，主要有以下流程： 用户登录进行身份认证，认证成功后服务器端生成Token返回给客户端； 客户端接收到Token后保存在客户端（可保存在Cookie、LocalStorage、SessionStorage中）； 客户端再次请求服务器端时，将Token作为请求头放入Headers中； 服务器端接收请求头中的Token，将用户参数按照既定规则再进行一次签名，两次签名若一致则认为成功，反之数据存在篡改请求失败。 Token与Cookie+Session的区别Cookie其实也充当的是令牌作用，但它是“有状态”的；而Token令牌是无状态的，更利于分布式部署。 Session和Cookie在讲Token之前，先简单说说什么是Session和Cookie。 首先要知道HTTP请求是无状态的，也就是不知道这一次的请求和上一次请求是否有关系，比如我们登录一个系统的时候，验证用户名密码之后，打开系统各个页面的时候就不需要再进行登录操作了，直到我们主动退出登录或超时退出登录；这里为了避免访问每个都登录一下，就要用到Session、Cookie。 Cookie是在客户端(浏览器)保存用户信息的一种机制；而且每种浏览器存储大小会有一些差异，一般不超过4KB； Session是在服务端保存，可以用于记录客户状态，比如我们经常会用Session保存客户的基本信息、权限信息等；用户第一次登录之后，服务器就会创建一个session，浏览器再次访问时，只需要从该Session中查找该客户的信息就可以了。 Token但是这里会有个问题，服务器要保存所有用户的Session信息，开销会很大，如果在分布式的架构下，就需要考虑Session共享的问题，需要做额外的设计和开发，例如把session中的信息保存到Redis中进行共享；所以因为这个原因，有人考虑这些信息是否可以让客户端保存，可以保存到任何地方，并且保证其安全性，于是就有了Token。 Token是服务端生成的一串字符串，可以看做客户端进行请求的一个令牌。 当客户端第一次访问服务端，服务端会根据传过来的唯一标识userId，运用一些加密算法，生成一个Token，客户端下次请求时，只需要带上Token，服务器收到请求后，会验证这个Token。 有些公司会建设统一登录系统（单点登录），客户端先去这个系统获取Token，验证通过再拿着这些Token去访问其他系统；API Gateway也可以提供类似的功能，我们公司就是这样，客户端接入的时候，先向网关获取Token，验证通过了才能访问被授权的接口，并且一段时间后要重新或者Token。 基于Token的认证流程整体的流程是这样的： 客户端使用用户名、密码做身份验证； 服务端收到请求后进行身份验证；(也可能是统一登录平台、网关) 验证成功后，服务端会签发一个Token返回给客户端； 客户端收到Token以后可以把它存储起来（可以放在）；每次向服务端发送请求的时候，都要带着Token； Token会有过期时间，过期后需要重新进行验证； 服务端收到请求，会验证客户端请求里面的Token，验证成功，才会响应客户端的请求； 总结 Cookie：保存在浏览器种，有大小限制，有状态； Session：保存在服务器中，服务器有资源开销，分布式、跨系统不好实现； Token：客户端可以将Token保存到任何地方，无限制，无状态，利于分布式部署。 由Session到Token的身份验证演变过程理解Session、Cookie、Token本文将从Web应用 由传统身份验证到基于Token的身份验证的演变过程的角度，介绍Session、Cookie、Token。 很久以前，Web 应用基本用作文档的浏览，如网络黄页。既然仅仅是浏览，因此服务器不需要记录具体用户在某一段时间里都浏览了哪些文档，每次请求都是一个新的HTTP协议，对服务器来说都是全新的。 基于Session的身份验证随着交互式Web应用的兴起，比如，购物等需要登录的网站。引出了一个新的问题，那就是要记录哪些用户登录了系统进行了哪些操作，即要管理会话（什么是会话？简单的讲如果用户需要登录，那么就可以简单的理解为会话，如果不需要登录，那么就是简单的连接。），比如，不同用户将不同商品加入到购物车中， 也就是说必须把每个用户区分开。因为HTTP请求是无状态的，所以想出了一个办法，那就是给每个用户配发一个会话标识(Session id)，简单的讲就是一个既不会重复，又不容易被找到规律以仿造的随机字符串，使得每个用户的收到的会话标识都不一样， 每次用户从客户端向服务端发起HTTP请求的时候，把这个字符串给一并发送过来， 这样服务端就能区分开谁是谁了，至于客户端（浏览器）如何保存这个“身份标识”，一般默认采用 Cookie 的方式，这个会话标识(Session id)会存在客户端的Cookie中。 虽然这样解决了区分用户的问题，但又引发了一个新的问题，那就是每个用户（客户端）只需要保存自己的会话标识(Session id)，而服务端则要保存所有用户的会话标识(Session id)。 如果访问服务端的用户逐渐变多， 就需要保存成千上万，甚至几千万个，这对服务器说是一个难以接受的开销 。 再比如，服务端是由2台服务器组成的一个集群， 小明通过服务器A登录了系统， 那session id会保存在服务器A上， 假设小明的下一次请求被转发到服务器B怎么办？ 服务器B可没有小明 的 session id。 可能会有人讲，如果使小明登录时，始终在服务器A上进行登录（sticky session），岂不解决了这个问题？那如果服务器A挂掉怎么办呢？ 还是会将小明的请求转发到服务器B上。 如此一来，那只能做集群间的 session 复制共享了， 就是把 session id 在两个机器之间进行复制，如下图，但这对服务器的性能和内存提出了巨大的挑战。 因此，又想到如果将所有用户的Session集中存储呢，也就想到了缓存服务Memcached——由于 Memcached 是分布式的内存对象缓存系统，因此可以用来实现 Session 同步。把session id 集中存储到一台服务器上， 所有的服务器都来访问这个地方的数据， 如此就避免了复制的方式， 但是这种“集万千宠爱于一身”使得又出现了单点故障的可能， 就是说这个负责存储 session 的服务器挂了， 所有用户都得重新登录一遍， 这是用户难以接受的。 那么索性存储Session的服务器也搞成集群，增加其可靠性，避免单点故障，但不管如何，Session 引发出来的问题层出不穷。 于是有人就在思考， 为什么服务端必须要保存这session呢， 只让每个客户端去保存不行吗？可是服务端如果不保存这些session id ，又将如何验证客户端发送的 session id 的确是服务端生成的呢？ 如果不验证，服务端无法判断是否是合法登录的用户，对，这里的问题是验证， session 只是解决这个验证问题的而产生的一个解决方案，是否还有其它方案呢？ 基于Token 的身份验证例如， 小明已经登录了系统，服务端给他发一个令牌(Token)， 里边包含了小明的 user id， 后续小明再次通过 Http 请求访问服务器的时候， 把这个 Token 通过 Http header 带过来不就可以了。 服务端需要验证 Token是自己生成的，而非伪造的。假如不验证任何人都可以伪造，那么这个令牌(token)和 session id没有本质区别，如何让别人伪造不了？那就对数据做一个签名（Sign）吧， 比如说服务端用 HMAC-SHA256 加密算法，再加上一个只有服务端才知道的密钥， 对数据做一个签名， 把这个签名和数据一起作为 Token 发给客户端， 客户端收到 Token 以后可以把它存储起来，比如存储在 Cookie 里或者 Local Storage 中，由于密钥除了服务端任何其他用户都不知道， 就无法伪造令牌(Token)。 如此一来，服务端就不需要保存 Token 了， 当小明把这个Token发给服务端时，服务端使用相同的HMAC-SHA256 算法和相同的密钥，对数据再计算一次签名， 和 Token 中的签名做个对比， 如果相同，说明小明已经登录过了， 即验证成功。若不相同， 那么说明这个请求是伪造的。 这样一来，服务端只需要生成 Token，而不需要保存Token，只是验证Token就好了，也就实现了时间换取空间（CPU计算时间换取session 存储空间）。没了session id的限制，当用户访问量增大，直接加机器就可以轻松地做水平扩展，也极大的提高了可扩展性。","categories":[{"name":"更多","slug":"更多","permalink":"http://huermosi.xyz/categories/%E6%9B%B4%E5%A4%9A/"},{"name":"HTTP","slug":"更多/HTTP","permalink":"http://huermosi.xyz/categories/%E6%9B%B4%E5%A4%9A/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://huermosi.xyz/tags/HTTP/"},{"name":"COOKIE","slug":"COOKIE","permalink":"http://huermosi.xyz/tags/COOKIE/"},{"name":"SESSION","slug":"SESSION","permalink":"http://huermosi.xyz/tags/SESSION/"},{"name":"TOKEN","slug":"TOKEN","permalink":"http://huermosi.xyz/tags/TOKEN/"}]},{"title":"Spring面试必问题","slug":"spring/spring-interview-01","date":"2018-03-02T11:20:00.000Z","updated":"2021-04-15T08:37:13.052Z","comments":true,"path":"2018/9527888013/","link":"","permalink":"http://huermosi.xyz/2018/9527888013/","excerpt":"","text":"1、谈谈你对spring的理解？ Spring是个java企业级应用的开源开发框架，轻量级，基本的版本大约2MB，Spring的优点主要体现在IOC和AOP，他两说白了就是对工厂模式和代理模式的一个封装！ 2、解释Spring中IOC, DI, AOP IOC就是控制反转或依赖注入。简单点说就是以前我们需要一个对象的时候都是new出来，而spring的IOC就帮我们实现了这一点，通过Spring的bean工厂为你生成所需要的对象 AOP就是面向切面编程，比如说你每做一次对数据库操作，都要生成一句日志。如果，你对数据库的操作有很多类，那你每一类中都要写关于日志的方法。但是如果你用aop，那么你可以写一个方法，在这个方法中有关于数据库操作的方法，每一次调用这个方法的时候，就加上生成日志的操作。 3、Spring的IOC是解耦,AOP是干什么的？ AOP面向切面编程 将程序中的交叉业务逻辑（比如安全，日志，事务等），封装成一个切面，然后注入到目标对象（具体业务逻辑）中去。 比如： 很多方法可能会抛异常，你要记录这个异常到日志中去，可以写个拦截器类，在这个类中记录日志 在spring.xml中配置一个对这些要记录日志的方法的aop拦截器 在这个方法执行后调用这个拦截器，记录日志。这样就不用每次抛异常都要手动记录日志。 spring的事务管理用到的就是aop 这样也可以提高程序的内聚性。 4、 Bean 工厂和 Application contexts 有什么区别？ Application contexts提供一种方法处理文本消息，一个通常的做法是加载文件资源（比如镜像），它们可以向注册为监听器的bean发布事件。另外，在容器或容器内的对象上执行的那些不得不由bean工厂以程序化方式处理的操作，可以Application contexts中以声明的方式处理。Application contexts实现了MessageSource接口，该接口的实现以可插拔的方式提供获取本地化消息的方法。 5、有哪些不同类型的IOC（依赖注入）方式？ （1）构造器依赖注入：构造器依赖注入通过容器触发一个类的构造器来实现的，该类有一系列参数，每个参数代表一个对其他类的依赖 （2）Setter方法注入：Setter方法注入是容器通过调用无参构造器或无参static工厂 方法实例化bean之后，调用该bean的setter方法，即实现了基于setter的依赖注入。 6、Spring beans是什么? Spring beans 是那些形成Spring应用的主干的java对象。它们被Spring IOC容器初始化，装配，和管理。这些beans通过容器中配置的元数据创建。比如，以XML文件中 的形式定义。Spring 框架定义的beans都是单件beans。在bean tag中有个属性”singleton”，如果它被赋为TRUE，bean 就是单件，否则就是一个 prototype bean。默认是TRUE，所以所有在Spring框架中的beans 缺省都是单件。 7、 一个 Spring Bean 定义 包含什么？ 一个Spring Bean 的定义包含容器必知的所有配置元数据，包括如何创建一个bean，它的生命周期详情及它的依赖 8、Spring框架中的单例bean是线程安全的吗? Spring框架中的单例bean不是线程安全的 9、Spring的注解有哪些？ @Component：是所有受Spring 管理组件的通用形式，@Component注解可以放在类的头上，@Component不推荐使用。 @Controller： @Controller对应表现层的Bean @Service： @Service对应的是业务层Bean @ Repository：@Repository对应数据访问层Bean @Autowired： @Autowired 根据bean 类型从spring 上线文中进行查找，注册类型必须唯一 @RequestMapping：@RequestMapping 可以声明到类或方法上 @RequestParam 10、Spring有几种配置方式？ （1）基于xml的配置 （2）基于注解额配置 （3）基于java的配置 本文转载自Java搬运工头条，原文连接点这里，版权归原作者所有，转载目的是为了避免以后找不到。","categories":[{"name":"SPRING","slug":"SPRING","permalink":"http://huermosi.xyz/categories/SPRING/"}],"tags":[{"name":"SPRING","slug":"SPRING","permalink":"http://huermosi.xyz/tags/SPRING/"},{"name":"面试","slug":"面试","permalink":"http://huermosi.xyz/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"我所了解的Restful","slug":"restful/restful-principal","date":"2018-02-28T07:07:43.000Z","updated":"2021-04-15T08:27:11.793Z","comments":true,"path":"2018/9527888012/","link":"","permalink":"http://huermosi.xyz/2018/9527888012/","excerpt":"","text":"REST是英文representational state transfer(表象性状态转变)或者表述性状态转移;Rest是web服务的一种架构风格;使用HTTP,URI,XML,JSON,HTML等广泛流行的标准和协议;轻量级,跨平台,跨语言的架构设计;它是一种设计风格,不是一种标准,是一种思想 Rest架构的主要原则网络上的所有事物都被抽象为资源 每个资源都有一个唯一的资源标识符 同一个资源具有多种表现形式(xml,json等) 对资源的各种操作不会改变资源标识符 所有的操作都是无状态的 符合REST原则的架构方式即可称为RESTful REST的五大特性 资源（Resource） 资源的表述（Representation） 状态转移（State Transfer） 统一接口（Uniform Interface） 超文本驱动（Hypertext Driven） 什么是Restful: 对应的中文是rest式的;Restful web service是一种常见的rest的应用,是遵守了rest风格的web服务;rest式的web服务是一种ROA(The Resource-Oriented Architecture)(面向资源的架构). 为什么会出现Restful 在Restful之前的操作： http://127.0.0.1/user/query/1 GET 根据用户id查询用户数据 http://127.0.0.1/user/save POST 新增用户 http://127.0.0.1/user/update POST 修改用户信息 http://127.0.0.1/user/delete GET/POST 删除用户信息 RESTful用法： http://127.0.0.1/user/1 GET 根据用户id查询用户数据 http://127.0.0.1/user POST 新增用户 http://127.0.0.1/user PUT 修改用户信息 http://127.0.0.1/user DELETE 删除用户信息 之前的操作是没有问题的,大神认为是有问题的,有什么问题呢?你每次请求的接口或者地址,都在做描述,例如查询的时候用了query,新增的时候用了save,其实完全没有这个必要,我使用了get请求,就是查询.使用post请求,就是新增的请求,我的意图很明显,完全没有必要做描述,这就是为什么有了restful. PUT和PATCH的功能都可以代表更新，但略有不同，PUT大多时候表示更新该资源的全部信息，而PATCH则更新部分信息。 PUT和POST又一些功能的重叠，都可以是新建一个资源，POST时，新建资源的地址是由服务器返回给客户端的。也就是说客户端在发送POST请求资源之前还无法预知该资源的地址，这在我们的 Api 开发中非常常见，新建一个帖子，新建一条评论，都如此。 本文转载自我所知道的头条，原文连接点这里，版权归原作者所有，转载目的是为了避免以后找不到。","categories":[{"name":"MICROSERVICE","slug":"MICROSERVICE","permalink":"http://huermosi.xyz/categories/MICROSERVICE/"}],"tags":[{"name":"RESTFUL","slug":"RESTFUL","permalink":"http://huermosi.xyz/tags/RESTFUL/"}]},{"title":"如何通过优化sql语句提高数据库查询效率","slug":"mysql/mysql-statement-optimize-01","date":"2018-02-26T23:50:00.000Z","updated":"2021-04-15T08:04:56.191Z","comments":true,"path":"2018/9527888011/","link":"","permalink":"http://huermosi.xyz/2018/9527888011/","excerpt":"","text":"可以通过以下多个方面优化sql语句提高数据库查询效率 应尽量避免在 where 子句中使用!=或&lt;&gt;操作符，否则将引擎放弃使用索引而进行全表扫描。 应尽量避免在 where 子句中使用 or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描，如： select id from t where num=10 or num=20 可以这样查询： select id from t where num=10 union all select id from t where num=20 in 和 not in 也要慎用，否则会导致全表扫描，如： select id from t where num in(1,2,3) 对于连续的数值，能用 between 就不要用 in 了： select id from t where num between 1 and 3 下面的查询也将导致全表扫描： select id from t where name like ‘%abc%’ 如果在 where 子句中使用参数，也会导致全表扫描。因为SQL只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择推迟到运行时；它必须在编译时进行选择。然而，如果在编译时建立访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。如下面语句将进行全表扫描： select id from t where num=@num 可以改为强制查询使用索引： select id from t with(index(索引名)) where num=@num 应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。如： select id from t where num/2=100 应改为: select id from t where num=100*2 应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。如： select id from t where substring(name,1,3)=’abc’–name以abc开头的id select id from t where datediff(day,createdate,’2005-11-30′)=0–‘2005-11-30’生成的id 应改为: select id from t where name like ‘abc%’ select id from t where createdate&gt;=’2005-11-30′ and createdate&lt;’2005-12-1′ 不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。 不要写一些没有意义的查询，如需要生成一个空表结构： select col1,col2 into #t from t where 1=0 这类代码不会返回任何结果集，但是会消耗系统资源的，应改成这样： create table #t(…) 很多时候用 exists 代替 in 是一个好的选择： select num from a where num in(select num from b) 用下面的语句替换： select num from a where exists(select 1 from b where num=a.num) 任何地方都不要使用 select * from t ，用具体的字段列表代替“*”，不要返回用不到的任何字段。 尽量避免使用游标，因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。 尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理。 尽量避免大事务操作，提高系统并发能力。 本文转载自VenusKong头条，原文连接点这里，版权归原作者所有，转载目的是为了避免以后找不到。","categories":[{"name":"MYSQL","slug":"MYSQL","permalink":"http://huermosi.xyz/categories/MYSQL/"},{"name":"优化","slug":"MYSQL/优化","permalink":"http://huermosi.xyz/categories/MYSQL/%E4%BC%98%E5%8C%96/"}],"tags":[{"name":"MYSQL","slug":"MYSQL","permalink":"http://huermosi.xyz/tags/MYSQL/"},{"name":"查询效率","slug":"查询效率","permalink":"http://huermosi.xyz/tags/%E6%9F%A5%E8%AF%A2%E6%95%88%E7%8E%87/"}]},{"title":"解构电商产品之订单系统","slug":"oms/order-system-arch","date":"2018-02-24T16:00:00.000Z","updated":"2021-04-26T12:36:29.369Z","comments":true,"path":"2018/9527888010/","link":"","permalink":"http://huermosi.xyz/2018/9527888010/","excerpt":"","text":"随着阿里、京东的崛起，中国电子商务的大门渐渐打开，越来越多的行业使用线上支付，无一例外地会用到电商系统，今天为大家解构一下订单系统。 今天分享将会分为以下三个环节来阐述： 1. 订单系统的介绍 2. 订单系统的结构 3. 订单系统设计思路 一、什么是订单系统？订单管理系统(OMS)是物流管理系统的一部分，通过对客户下达的订单进行管理及跟踪，动态掌握订单的进展和完成情况，提升物流过程中的作业效率，从而节省运作时间和作业成本，提高物流企业的市场竞争力。顾名思义，电商系统就是用户、平台、商户等对于订单的管控、跟踪的系统，衔接着商品中心、wms、促销系统、物流系统等，是电子商务的基础模块； 简单地说订单管理系统作为整个电商的核心，管理着所有的交易进出，可以说没有订单系统电商就无法流畅地运转； 一个好的订单管理系统需要有很好地扩展性和流畅性，在一个电商产品从0-1的过程，订单系统作为其基础模块需要提前考虑到各系统的扩展，订单系统如果在前期就能考虑到后面的扩展，相信对于电商的壮大会非常有帮助； 流畅性指的是整个交易链路需要很流畅，早期我司的订单系统做的非常庞大，但是却没有考虑到流程的通畅性，导致连基础的订单流程都没有办法正常走下去，所以，在从0到1地做一套订单系统时，需要有一些前瞻性，但落地时，以MVP去试错； 二、订单系统解构订单字段订单的主要信息包括支付信息 、配送信息、状态信息、促销信息、商品信息、用户信息等； 支付信息：涉及支付的字段信息，主要包括支付方式、支付金额、订单金额、优惠金额等； 促销信息：涉及促销的字段信息，主要包括优惠方式、优惠面额、折扣等； 商品信息：涉及订单中的商品字段，主要包括商品名称、单价、数量、所属店铺等； 时间信息：涉及订单流转中各个时间戳的字段，包括下单时间、支付时间、发货时间、完成时间等 状态信息：涉及订单流转中状态变更的字段，主要包括订单状态、物流状态及退款状态等； 用户信息：涉及用户的信息，比如买家姓名、注册手机号、收件人等信息； 配送信息：涉及订单配送的基本信息，比如配送方式、物流单号等； 以上这些字段构成了订单所需要的大部分信息； 订单体系 可以从三个层面来了解电商的订单管理体系，分别是用户层、系统层和底层； 用户层 这个比较好理解，就是用户日常使用的功能和页面，主要有订单列表、订单详情和退款详情等C端用户购买时会使用到的页面，系统层和底层模块为其提供支持； 系统层 在订单管理体系中，和订单最息息相关的交互系统主要有支付系统、订单系统、仓储系统； 1.支付系统 主要作用就是为订单提供支付支持，方便用户使用各种支付方式进行支付，用户支付后会将支付信息给到订单系统； 2.订单系统 作为订单管理体系的核心，起着至关重要的作用，在订单系统中会生成订单，审核订单，取消订单，还涉及到复杂的订单金额计算以及移库操作； 仓储系统：主要用来管理库存以及发货，订单到达一定状态后给到仓储系统，用于管理对应订单的打包、分拣、备货、出库等； 底层模块 主要包括商品、支付、用户、营销、订单和消息等模块，这些模块共同组成了对上层业务、系统的支持； 大公司一般会将底层框架模块化，比如商品，会构建对应的商品中心，代码、数据库等相对独立，由商品中心开接口和soa，其他模块需要使用商品中心相关功能的时候调取接口，这样做的好处是使各个模块底层相对独立，便于管理及改动； 状态机 下面来说说状态机，一般电商平台用户直观能看到的状态有上图中列举的几个，包括待支付、待配送、待收货、交易完成、退款中； O2O没有电商中庞大的仓储系统，自然比电商的流程简单些，我将从正流程分别从正流程和逆流程来介绍； 主流程 在电商中，无论是买家端还是卖家端，都会将交易主状态分为待付款、待发货、待确认收货、交易完成，但是买家端与买家端的展示逻辑稍有不同； 在买家端，买家关心的状态无非就那么几个，即待付款、待发货、待收货和待评价，所以淘宝并未像商家端那样将全部的状态一一罗列出，而是保留了买家最关心的状态，保持整个买家端的简洁性； 而买家端中，主要解决的是商家效率的问题，所以在订单列表中会将所有的状态（即待付款、待发货、已发货、退款中、需要评价、交易完成、交易关闭）的订单全部拉出，考虑到商家订单较多的情况，出于对服务器查询的考虑以及并发的考虑，增加了三个月内订单与三个月前订单的查询区分； 首先说说待付款状态，待付款状态主要是买家下单但是没有支付的情况，待付款状态下淘宝的商家也可以进行一系列操作如改价等，买家也可以申请代付、批量操作； 待发货，该状态下会展示所有已支付，待发货的订单，淘宝目前支持的发货方式主要有四种，在线下单、手填快递单号、无纸化物流以及无需物流，操作配送之后交易状态会变更为待确认收货，大型电商平台已经采用无纸化发货的形式进行发货，即使用中端叫单，成功后会展示在已发货（商家端）和待收货（买家端）中； 待确认收货，该状态出于物流阶段，一般会根据业务、活动等来设定自动确认收货的时间，一般电商默认值是在发货后的10天为自动确认收货时间，在双十一、双十二等节日，这个时间会延长到15天，另外海外购、天猫国际等海外购物的订单自动确认时间也会相对较长，为25天； 交易完成，该状态由系统或者用户触发，在订单确认收货后，订单状态变更为交易成功，此时系统会根据是否评价过判断是否将订单展示在买家端的待评价下来引导用户对商家进行评价反馈； 退款/退货流程 一般电商中订单的逆流程主要分为退款流程和退货流程，这里简单地介绍下，后续会有专题来讲述； 发货前的逆流程 发货前的状态一般有待支付和待发货两个，待支付的订单发起逆流程后无需商家确认，直接关闭订单； 而待发货的订单发起后需要走商家的审核，商家同意后订单变为交易关闭，触发退款； 发货后的逆流程 发货后的逆流程主要包括待确认收货和交易成功的逆流程； 大致分为需要仅退款和退货退款； 仅退款：未收到货或与卖家协商同意后的申请，卖家同意后无需物流； 退货退款：已收到货需要退换的情况，卖家同意后需要走物流； Po上我司的退款流程作为后续专题的引子吧，敬请期待… 3、某垂直电商设计思路笔者的公司属于某个垂直行业的电商，主要以B2B转单为主，将线上的订单转给线下门店进行配送，所以暂时不涉及商品、库存、仓库等； 以下是我司的订单流程，线上商家将订单转给线下门店，涉及的状态有待派单、待支付、待接单、待配送、待转账和交易完成； 在设计主流程的时候并不复杂，根据业务场景进行设计即可，真正复杂的部分在订单的逆流程与系统间的交互； 由于旧版的系统过于臃肿，没有办法在其上进行迭代，加之流程上有很多问题，所以打算从业务流程、系统框架、视觉设计等方面做个大改版，即解决用户使用流程中的问题，也便于后期业务功能的实现； 本文转载自人人都是产品经理头条，原文连接点这里，版权归原作者所有，转载目的是为了避免以后找不到。","categories":[{"name":"更多","slug":"更多","permalink":"http://huermosi.xyz/categories/%E6%9B%B4%E5%A4%9A/"},{"name":"杂项","slug":"更多/杂项","permalink":"http://huermosi.xyz/categories/%E6%9B%B4%E5%A4%9A/%E6%9D%82%E9%A1%B9/"}],"tags":[{"name":"电商","slug":"电商","permalink":"http://huermosi.xyz/tags/%E7%94%B5%E5%95%86/"},{"name":"订单","slug":"订单","permalink":"http://huermosi.xyz/tags/%E8%AE%A2%E5%8D%95/"}]},{"title":"NGINX负载均衡","slug":"nginx/nginx-loadbalance-01","date":"2018-02-24T12:41:50.000Z","updated":"2021-04-26T12:37:37.640Z","comments":true,"path":"2018/9527888009/","link":"","permalink":"http://huermosi.xyz/2018/9527888009/","excerpt":"","text":"当用户量很大时，一台服务器单位时间内访问量很大的时候，服务器压力就会很大，当达到这台服务器的极限，就会崩溃；怎么解决？可以通过nginx的反向代理设置，添加几台同样功能的服务器 分担压力。 nginx实现负载均衡原理，用户访问首先访问到nginx服务器，然后nginx服务器再从应用服务器集群中选择压力比较小的服务器，然后将该访问请求引向该服务器。如应用服务器集群中某一台服务器崩溃，那么从待选择服务器列表中将该服务器删除，也就是说一个服务器崩溃了，那么nginx服务器不会把请求引向到该服务器。 nginx通过指令upstream实现负载均衡，如： 123456789101112upstream mypro &#123; #mypro代表此负载均衡的规则名称 server 192.168.5.140:8080; #服务器1的ip地址以及端口 server 192.168.5.141:8080; #服务器2的ip地址以及端口&#125;server &#123; listen 80; server_name 192.168.5.138; location &#x2F; &#123; proxy_pass http:&#x2F;&#x2F;mypro; &#125;&#125; 上面的配置代表，用户访问服务器192.168.5.138，端口80时，nginx会把请求传发给mypro的负载均衡规则去处理；nginx会随机取一个mypro下的服务器来处理用户请求。 nginx在选取upstream服务器列表的规则，默认是随机；当然nginx还支持其他的选取规则 根据权重选取1234upstream mypro &#123; #weight权重越大，选取占比越高 server 192.168.5.140:8080 weight&#x3D;5; server 192.168.5.141:8080 weight&#x3D;10;&#125; 根据ip_hash选取12345upstream mypro &#123; ip_hash; #加入ip_hash指令 server 192.168.5.140:8080; server 192.168.5.141:8080;&#125; ip_hash选取规则，是根据请求客户端的ip的hash值配对选择一个服务器，下次此客户端再次访问时，会配对相同的服务器处理请求；这样的好处在于有些带有状态的业务，就可以一直在同一个服务器上进行处理。 ip_hash这种方式在很多场景会用到，如：session会话，状态转发服务等业务。 随机选取（默认）1234upstream mypro &#123; server 192.168.5.140:8080; server 192.168.5.141:8080;&#125; nginx的负载均衡就先介绍到这里，大家有什么问题可以回复老顾 本文转载自老顾聊技术头条，原文连接点这里，版权归原作者所有，转载目的是为了避免以后找不到。","categories":[{"name":"更多","slug":"更多","permalink":"http://huermosi.xyz/categories/%E6%9B%B4%E5%A4%9A/"},{"name":"NGINX","slug":"更多/NGINX","permalink":"http://huermosi.xyz/categories/%E6%9B%B4%E5%A4%9A/NGINX/"}],"tags":[{"name":"NGINX","slug":"NGINX","permalink":"http://huermosi.xyz/tags/NGINX/"},{"name":"负载均衡","slug":"负载均衡","permalink":"http://huermosi.xyz/tags/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"}]},{"title":"基于Zookeeper的分布式锁的简单介绍及实现","slug":"zookeeper/zookeeper-distribute-lock","date":"2018-02-24T12:34:38.000Z","updated":"2021-04-10T12:40:17.300Z","comments":true,"path":"2018/9527888008/","link":"","permalink":"http://huermosi.xyz/2018/9527888008/","excerpt":"","text":"实现分布式锁目前有三种流行方案，分别为基于数据库、Redis、Zookeeper的方案，其中前两种方案网络上有很多资料可以参考，本文不做展开。我们来看下使用Zookeeper如何实现分布式锁。 什么是Zookeeper？ Zookeeper（业界简称zk）是一种提供配置管理、分布式协同以及命名的中心化服务，这些提供的功能都是分布式系统中非常底层且必不可少的基本功能，但是如果自己实现这些功能而且要达到高吞吐、低延迟同时还要保持一致性和可用性，实际上非常困难。因此zookeeper提供了这些功能，开发者在zookeeper之上构建自己的各种分布式系统。 虽然zookeeper的实现比较复杂，但是它提供的模型抽象却是非常简单的。Zookeeper提供一个多层级的节点命名空间（节点称为znode），每个节点都用一个以斜杠（/）分隔的路径表示，而且每个节点都有父节点（根节点除外），非常类似于文件系统。例如，/foo/doo这个表示一个znode，它的父节点为/foo，父父节点为/，而/为根节点没有父节点。与文件系统不同的是，这些节点都可以设置关联的数据，而文件系统中只有文件节点可以存放数据而目录节点不行。Zookeeper为了保证高吞吐和低延迟，在内存中维护了这个树状的目录结构，这种特性使得Zookeeper不能用于存放大量的数据，每个节点的存放数据上限为1M。 而为了保证高可用，zookeeper需要以集群形态来部署，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么zookeeper本身仍然是可用的。客户端在使用zookeeper时，需要知道集群机器列表，通过与集群中的某一台机器建立TCP连接来使用服务，客户端使用这个TCP链接来发送请求、获取结果、获取监听事件以及发送心跳包。如果这个连接异常断开了，客户端可以连接到另外的机器上。 架构简图如下所示： 客户端的读请求可以被集群中的任意一台机器处理，如果读请求在节点上注册了监听器，这个监听器也是由所连接的zookeeper机器来处理。对于写请求，这些请求会同时发给其他zookeeper机器并且达成一致后，请求才会返回成功。因此，随着zookeeper的集群机器增多，读请求的吞吐会提高但是写请求的吞吐会下降。 有序性是zookeeper中非常重要的一个特性，所有的更新都是全局有序的，每个更新都有一个唯一的时间戳，这个时间戳称为zxid（Zookeeper Transaction Id）。而读请求只会相对于更新有序，也就是读请求的返回结果中会带有这个zookeeper最新的zxid。 如何使用zookeeper实现分布式锁？ 在描述算法流程之前，先看下zookeeper中几个关于节点的有趣的性质： 下面描述使用zookeeper实现分布式锁的算法流程，假设锁空间的根节点为/lock： 步骤1中创建的临时节点能够保证在故障的情况下锁也能被释放，考虑这么个场景：假如客户端a当前创建的子节点为序号最小的节点，获得锁之后客户端所在机器宕机了，客户端没有主动删除子节点；如果创建的是永久的节点，那么这个锁永远不会释放，导致死锁；由于创建的是临时节点，客户端宕机后，过了一定时间zookeeper没有收到客户端的心跳包判断会话失效，将临时节点删除从而释放锁。 另外细心的朋友可能会想到，在步骤2中获取子节点列表与设置监听这两步操作的原子性问题，考虑这么个场景：客户端a对应子节点为/lock/lock-0000000000，客户端b对应子节点为/lock/lock-0000000001，客户端b获取子节点列表时发现自己不是序号最小的，但是在设置监听器前客户端a完成业务流程删除了子节点/lock/lock-0000000000，客户端b设置的监听器岂不是丢失了这个事件从而导致永远等待了？这个问题不存在的。因为zookeeper提供的API中设置监听器的操作与读操作是原子执行的，也就是说在读子节点列表时同时设置监听器，保证不会丢失事件。 最后，对于这个算法有个极大的优化点：假如当前有1000个节点在等待锁，如果获得锁的客户端释放锁时，这1000个客户端都会被唤醒，这种情况称为“羊群效应”；在这种羊群效应中，zookeeper需要通知1000个客户端，这会阻塞其他的操作，最好的情况应该只唤醒新的最小节点对应的客户端。应该怎么做呢？在设置事件监听时，每个客户端应该对刚好在它之前的子节点设置事件监听，例如子节点列表为/lock/lock-0000000000、/lock/lock-0000000001、/lock/lock-0000000002，序号为1的客户端监听序号为0的子节点删除消息，序号为2的监听序号为1的子节点删除消息。 所以调整后的分布式锁算法流程如下： 本文转载自3DMAX建模渲染头条，原文连接点这里，版权归原作者所有，转载目的是为了避免以后找不到。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/categories/JAVA/"},{"name":"锁","slug":"JAVA/锁","permalink":"http://huermosi.xyz/categories/JAVA/%E9%94%81/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/tags/JAVA/"},{"name":"ZOOKEEPER","slug":"ZOOKEEPER","permalink":"http://huermosi.xyz/tags/ZOOKEEPER/"},{"name":"分布式锁","slug":"分布式锁","permalink":"http://huermosi.xyz/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/"}]},{"title":"JAVA简单爬虫-JSOUP","slug":"jsoup/java-simple-crawler-jsoup","date":"2018-02-21T11:47:53.000Z","updated":"2021-04-10T11:53:18.172Z","comments":true,"path":"2018/9527888007/","link":"","permalink":"http://huermosi.xyz/2018/9527888007/","excerpt":"","text":"Jsoup 是一个 Java 的开源HTML解析器，可直接解析某个URL地址、HTML文本内容。 Jsoup主要有以下功能： 从一个URL，文件或字符串中解析HTML 使用DOM或CSS选择器来查找、取出数据 对HTML元素、属性、文本进行操作 基本步骤： //下载网页String URL=”输入网址”；Document document=Jsoup.cnnect(“URL”); //在下载的document里进行检索的语句elements test=document.select(“#div_JK”).select(“div.item_list”).select(“div:nth-child(1)”).select(“div.dTit.tracking-ad”).select(“a”); //这样test标签就是我们最开始右键单击检查的标签String Str=test.toString(); //将标签转化成字符串String text=test.text(); //将标签里的文本提取出来 实例演练 Jsoup在解析html方面还有很多功能，这里就不一一列举了。Jsoup的中文api地址：http://www.open-open.com/jsoup/ Jsoup在做爬虫方面，有着非常大优势。后续会选择一些现有开源的爬虫框架进行分析，与大家共享。欢迎大家拍砖。 本文转载自头条，原文连接点这里，版权归原作者所有，转载目的是为了避免以后找不到。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/categories/JAVA/"},{"name":"JSOUP","slug":"JAVA/JSOUP","permalink":"http://huermosi.xyz/categories/JAVA/JSOUP/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/tags/JAVA/"},{"name":"JSOUP","slug":"JSOUP","permalink":"http://huermosi.xyz/tags/JSOUP/"},{"name":"爬虫","slug":"爬虫","permalink":"http://huermosi.xyz/tags/%E7%88%AC%E8%99%AB/"}]},{"title":"一张图了解微服务架构核心知识点","slug":"microservice/one-pic-for-microservice","date":"2018-02-19T10:03:18.000Z","updated":"2021-04-10T10:11:28.195Z","comments":true,"path":"2018/9527888004/","link":"","permalink":"http://huermosi.xyz/2018/9527888004/","excerpt":"","text":"互联网时代，企业在瞬息万变的市场赢得和保持竞争优势的核心在于持续创新。业界前沿互联网公司的实践表明，微服务架构 (Microservices Architecture) 是企业应对业务复杂性，支持大规模持续创新行之有效的架构手段。 微服务架构作为一种渐进式的演进架构，自提出以来便被互联网企业和传统企业所重视和采用。微服务架构所涉及的知识广泛，其学习曲线相对陡峭，其中架构落地、网关、监控等技术问题是常见的挑战。技术人员光靠自学摸索，通常需要耗费不少时间精力。由于微服务架构可操作学习的案例相对较少，尤其是大型业务微服务架构应用案例，所以大家对微服务架构理念虽然有一定理解，但是对微服务架构如何落地缺乏可靠的最佳实践经验。 技术基础和平台工具易学，但架构思维和落地经验难建。一个合格的架构师除了最核心的技术理论基础之外，必须具备良好的架构视野和思维模式，以及通过技术与业务结合的落地实践所总结的行之有效的经验和方法论。 为了帮助技术人员在微服务架构落地实践中提高效率，少走弯路，InfoQ 特邀请微服务技术专家、资深架构师杨波老师共同策划和制作了《微服务架构核心 20 讲》视频课程，就大家最关注的微服务落地关键问题做深入浅出的讲解。 关于微服务架构，杨波老师总结了一张长图，从整体架构到服务发布体系，再到持续交付流水线，让你详细了解微服务架构核心知识点。 本文转载自InfoQ头条，原文连接点这里，版权归原作者所有，转载目的是为了避免以后找不到。","categories":[{"name":"MICROSERVICE","slug":"MICROSERVICE","permalink":"http://huermosi.xyz/categories/MICROSERVICE/"}],"tags":[{"name":"MICROSERVICE","slug":"MICROSERVICE","permalink":"http://huermosi.xyz/tags/MICROSERVICE/"},{"name":"微服务","slug":"微服务","permalink":"http://huermosi.xyz/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"架构","slug":"架构","permalink":"http://huermosi.xyz/tags/%E6%9E%B6%E6%9E%84/"}]},{"title":"什么是单点登陆","slug":"sso/what-is-sso","date":"2018-02-19T09:48:04.000Z","updated":"2021-04-26T12:38:24.105Z","comments":true,"path":"2018/9527888003/","link":"","permalink":"http://huermosi.xyz/2018/9527888003/","excerpt":"","text":"计算机网络登录方式改革变化很大，从一开始登录一个网站或者一个软件就要输入用户名和密码到现在一个账户实现全登陆，比如你要登录支付宝、闲鱼、淘宝、千牛用一个账户即全部登录，下面就对登陆方式探讨一下！ 1. 单系统登录机制1.1 http无状态协议web应用采用browser/server架构，http作为通信协议。http是无状态协议，浏览器的每一次请求，服务器会独立处理，不与之前或之后的请求产生关联，但这也同时意味着任何用户都能通过浏览器访问服务器资源，如果想保护服务器的某些资源，必须限制浏览器请求；要限制浏览器请求，必须鉴别浏览器请求，响应合法请求，忽略非法请求；要鉴别浏览器请求，必须清楚浏览器请求状态，这就牵涉到下一个问题会话机制。 1.2 会话机制浏览器第一次请求服务器，服务器创建一个会话，并将会话的id作为响应的一部分发送给浏览器，浏览器存储会话id，并在后续第二次和第三次请求中带上会话id，服务器取得请求中的会话id就知道是不是同一个用户了，用到两种方式： 第一种方式是请求参数cookie将会话id作为每一个请求的参数，服务器接收请求自然能解析参数获得会话id，并借此判断是否来自同一会话，很明显这种方式不安全。那就用第二种方式，浏览器自己来维护这个会话id吧，每次发送http请求时浏览器自动发送会话id，cookie机制正好适用这种情况。cookie是浏览器用来存储少量数据的一种机制，数据以”key/value“形式存储，浏览器发送http请求时自动附带cookie信息。 1.3 登录状态有了会话机制，登录状态就好明白了，假设浏览器第一次请求服务器需要输入用户名与密码验证身份，服务器拿到用户名密码去数据库比对，正确的话说明当前持有这个会话的用户是合法用户，应该将这个会话标记为“已授权”或者“已登录”等等之类的状态，既然是会话的状态，自然要保存在会话对象中，tomcat在会话对象中设置登录状态如下：用户再次访问时，tomcat在会话对象中查看登录状态： 实现每次请求受保护资源时都会检查会话对象中的登录状态，只有 isLogin=true 的会话才能访问，登录机制因此而实现。 2. 多系统的复杂性web系统早已从久远的单系统发展成为如今由多系统组成的应用群，面对如此众多的系统，用户难道要一个一个登录、然后一个一个注销吗？系统由单系统发展成多系统组成的应用群，复杂性应该由系统内部承担，而不是用户。无论web系统内部多么复杂，对用户而言都是一个统一的整体，也就是说用户访问web系统的整个应用群与访问单个系统一样，登录/注销只要一次就够了，虽然单系统的登录解决方案很完美，但对于多系统应用群已经不再适用了，因为单系统登录解决方案的核心是cookie，cookie携带会话id在浏览器与服务器之间维护会话状态，但cookie是有限制的，这个限制就是cookie的域（通常对应网站的域名），浏览器发送http请求时会自动携带与该域匹配的cookie，而不是所有cookie。 那为什么不将web应用群中所有子系统的域名统一在一个顶级域名下，例如“*.baidu.com”，然后将它们的cookie域设置为“baidu.com”，这种做法理论上是可以的，甚至早期很多多系统登录就采用这种同域名共享cookie的方式。然而可行并不代表好，共享cookie的方式存在众多局限性。 2.1 应用群域名必须统一。2.2 应用群各系统使用的技术（至少是web服务器）要相同，不然cookie的key值（tomcat为JSESSIONID）不同，无法维持会话，共享cookie的方式是无法实现跨语言技术平台登录的，比如java、php、.net系统之间。2.3 cookie本身不安全，因此我们需要一种全新的登录方式来实现多系统应用群的登录，这就是我们的主题单点登录。 3. 单点登录单点登陆是指在多系统应用群中登录一个系统，便可在其他所有系统中得到授权而无需再次登录，包括单点登录与单点注销两部分。 3.1 单点登录相比于单系统登录，sso需要一个独立的认证中心，只有认证中心能接受用户的用户名密码等安全信息，其他系统不提供登录入口，只接受认证中心的间接授权。间接授权通过令牌实现，sso认证中心验证用户的用户名密码没问题，创建授权令牌，在接下来的跳转过程中，授权令牌作为参数发送给各个子系统，子系统拿到令牌，即得到了授权，可以借此创建局部会话，局部会话登录方式与单系统的登录方式相同。这个过程也就是单点登录的原理。 举例说明，用户访问系统1的受保护资源，系统1发现用户未登录，跳转至sso认证中心，并将自己的地址作为参数sso认证中心发现用户未登录，将用户引导至登录页面，用户输入用户名密码提交登录申请sso认证中心校验用户信息，创建用户与sso认证中心之间的会话，称为全局会话，同时创建授权令牌sso认证中心带着令牌跳转回最初的请求地址（系统1）系统1拿到令牌，去sso认证中心校验令牌是否有效sso认证中心校验令牌，返回有效，注册系统1，系统1使用该令牌创建与用户的会话，称为局部会话，返回受保护资源，用户访问系统2的受保护资源，系统2发现用户未登录，跳转至sso认证中心，并将自己的地址作为参数sso认证中心发现用户已登录，跳转回系统2的地址，并附上令牌系统2拿到令牌，去sso认证中心校验令牌是否有效，sso认证中心校验令牌，返回有效，注册系统2，系统2使用该令牌创建与用户的局部会话，返回受保护资源，用户登录成功之后，会与sso认证中心及各个子系统建立会话，用户与sso认证中心建立的会话称为全局会话，用户与各个子系统建立的会话称为局部会话，局部会话建立之后，用户访问子系统受保护资源将不再通过sso认证中心。 全局会话与局部会话有如下约束关系：局部会话存在，全局会话一定存在，全局会话存在，局部会话不一定存在，全局会话销毁，局部会话必须销毁，你可以通对支付宝系列等网站的登录过程加深对单点登录的理解。 3.2 单点注销单点登录自然也要单点注销，在一个子系统中注销，所有子系统的会话都将被销毁，认证中心一直监听全局会话的状态，一旦全局会话销毁，监听器将通知所有注册系统执行注销操作。 举例说明，如用户向系统1发起注销请求，系统1根据用户与系统1建立的会话id拿到令牌，向sso认证中心发起注销请求，sso认证中心校验令牌有效，销毁全局会话，同时取出所有用此令牌注册的系统地址，sso认证中心向所有注册系统发起注销请求，各注册系统接收sso认证中心的注销请求，销毁局部会话，sso认证中心引导用户至登录页面。 3.3 部署单点登录涉及sso认证中心与众子系统，子系统与sso认证中心需要通信以交换令牌、校验令牌及发起注销请求，因而子系统必须集成sso的客户端，sso认证中心则是sso服务端，整个单点登录过程实质是sso客户端与服务端通信的过程，认证中心与sso客户端通信方式有多种，这里以简单好用的httpClient为例，web service、rpc、restful api都可以。 3.4 实现简要介绍下基于java的实现过程，明白了原理，可以自己实现，sso采用客户端/服务端架构，举例说明sso-client与sso-server要实现的功能，sso-client拦截子系统未登录用户请求，跳转至sso认证中心接收并存储sso认证中心发送的令牌与sso-server通信，校验令牌的有效性，建立局部会话，拦截用户注销请求，向sso认证中心发送注销请求，接收sso认证中心发出的注销请求，销毁局部会话，sso-server验证用户的登录信息，创建全局会话，创建授权令牌，与sso-client通信发送令牌，校验sso-client令牌有效性，系统注册，接收sso-client注销请求，注销所有会话 。 4. 举例说明实如何现sso4.1 sso-client拦截未登录请求java拦截请求的方式有servlet、filter、listener三种方式，我们采用filter。在sso-client中新建LoginFilter.java类并实现Filter接口，在doFilter()方法中加入对未登录用户的拦截。4.2 sso-server拦截未登录请求，拦截从sso-client跳转至sso认证中心的未登录请求，跳转至登录页面，这个过程与sso-client完全一样。4.3 sso-server验证用户登录信息，用户在登录页面输入用户名密码，请求登录，sso认证中心校验用户信息，校验成功，将会话状态标记为“已登录”。4.4 sso-server创建授权令牌，授权令牌是一串随机字符，以什么样的方式生成都没有关系，只要不重复、不易伪造即可。4.5 sso-client取得令牌并校验，sso认证中心登录后，跳转回子系统并附上令牌，子系统（sso-client）取得令牌，然后去sso认证中心校验。4.6 sso-server接收并处理校验令牌请求，用户在sso认证中心登录成功后，sso-server创建授权令牌并存储该令牌，所以sso-server对令牌的校验就是去查找这个令牌是否存在以及是否过期，令牌校验成功后sso-server将发送校验请求的系统注册到sso认证中心（就是存储起来的意思），令牌与注册系统地址通常存储在key-value数据库（如redis）中，redis可以为key设置有效时间也就是令牌的有效期。redis运行在内存中，速度非常快，正好sso-server不需要持久化任何数据。令牌与注册系统地址可以用下图描述的结构存储在redis中，可能你会问为什么要存储这些系统的地址？如果不存储，注销的时候就麻烦了，用户向sso认证中心提交注销请求，sso认证中心注销全局会话，但不知道哪些系统用此全局会话建立了自己的局部会话，也不知道要向哪些子系统发送注销请求注销局部会话。4.7 sso-client校验令牌成功创建局部会话，令牌校验成功后，sso-client将当前局部会话标记为“已登录”，sso-client还需将当前会话id与令牌绑定，表示这个会话的登录状态与令牌相关，此关系可以用java的hashmap保存，保存的数据用来处理sso认证中心发来的注销请求。4.8 注销过程，用户向子系统发送带有“logout”参数的请求（注销请求），sso-client拦截器拦截该请求，向sso认证中心发起注销请求。sso认证中心也用同样的方式识别出sso-client的请求是注销请求（带有“logout”参数），sso认证中心注销全局会话。sso认证中心有一个全局会话的监听器，一旦全局会话注销，将通知所有注册系统注销。亲，你对单点登陆有了比较清晰的了解了吗！ 本文转载自黑猫大V头条，原文连接点这里，版权归原作者所有，转载目的是为了避免以后找不到。","categories":[{"name":"更多","slug":"更多","permalink":"http://huermosi.xyz/categories/%E6%9B%B4%E5%A4%9A/"},{"name":"SSO","slug":"更多/SSO","permalink":"http://huermosi.xyz/categories/%E6%9B%B4%E5%A4%9A/SSO/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/tags/JAVA/"},{"name":"SSO","slug":"SSO","permalink":"http://huermosi.xyz/tags/SSO/"},{"name":"单点登录","slug":"单点登录","permalink":"http://huermosi.xyz/tags/%E5%8D%95%E7%82%B9%E7%99%BB%E5%BD%95/"}]},{"title":"从Elasticsearch来看分布式系统架构设计","slug":"elasticsearch/elasticsearch-distribute-arch-design","date":"2018-02-04T13:03:25.000Z","updated":"2021-04-26T12:40:20.629Z","comments":true,"path":"2018/9527888014/","link":"","permalink":"http://huermosi.xyz/2018/9527888014/","excerpt":"","text":"分布式系统类型多，涉及面非常广，不同类型的系统有不同的特点，批量计算和实时计算就差别非常大。这篇文章中，重点会讨论下分布式数据系统的设计，比如分布式存储系统，分布式搜索系统，分布式分析系统等。 我们先来简单看下Elasticsearch的架构。 Elasticsearch 集群架构Elasticsearch是一个非常著名的开源搜索和分析系统，目前被广泛应用于互联网多种领域中，尤其是以下三个领域特别突出。一是搜索领域，相对于solr，真正的后起之秀，成为很多搜索系统的不二之选。二是Json文档数据库，相对于MongoDB，读写性能更佳，而且支持更丰富的地理位置查询以及数字、文本的混合查询等。三是时序数据分析处理，目前是日志处理、监控数据的存储、分析和可视化方面做得非常好，可以说是该领域的引领者了。 Elasticsearch的详细介绍可以到官网查看。我们先来看一下Elasticsearch中几个关键概念： 节点（Node）：物理概念，一个运行的Elasticearch实例，一般是一台机器上的一个进程。 索引（Index），逻辑概念，包括配置信息mapping和倒排正排数据文件，一个索引的数据文件可能会分布于一台机器，也有可能分布于多台机器。索引的另外一层意思是倒排索引文件。 分片（Shard）：为了支持更大量的数据，索引一般会按某个维度分成多个部分，每个部分就是一个分片，分片被节点(Node)管理。一个节点(Node)一般会管理多个分片，这些分片可能是属于同一份索引，也有可能属于不同索引，但是为了可靠性和可用性，同一个索引的分片尽量会分布在不同节点(Node)上。分片有两种，主分片和副本分片。 副本（Replica）：同一个分片(Shard)的备份数据，一个分片可能会有0个或多个副本，这些副本中的数据保证强一致或最终一致。 用图形表示出来可能是这样子的： Index 1：蓝色部分，有3个shard，分别是P1，P2，P3，位于3个不同的Node中，这里没有Replica。 Index 2：绿色部分，有2个shard，分别是P1，P2，位于2个不同的Node中。并且每个shard有一个replica，分别是R1和R2。基于系统可用性的考虑，同一个shard的primary和replica不能位于同一个Node中。这里Shard1的P1和R1分别位于Node3和Node2中，如果某一刻Node2发生宕机，服务基本不会受影响，因为还有一个P1和R2都还是可用的。因为是主备架构，当主分片发生故障时，需要切换，这时候需要选举一个副本作为新主，这里除了会耗费一点点时间外，也会有丢失数据的风险。 Index流程建索引（Index）的时候，一个Doc先是经过路由规则定位到主Shard，发送这个doc到主Shard上建索引，成功后再发送这个Doc到这个Shard的副本上建索引，等副本上建索引成功后才返回成功。 在这种架构中，索引数据全部位于Shard中，主Shard和副本Shard各存储一份。当某个副本Shard或者主Shard丢失（比如机器宕机，网络中断等）时，需要将丢失的Shard在其他Node中恢复回来，这时候就需要从其他副本（Replica）全量拷贝这个Shard的所有数据到新Node上构造新Shard。这个拷贝过程需要一段时间，这段时间内只能由剩余主副本来承载流量，在恢复完成之前，整个系统会处于一个比较危险的状态，直到failover结束。 这里就体现了副本（Replica）存在的一个理由，避免数据丢失，提高数据可靠性。副本（Replica）存在的另一个理由是读请求量很大的时候，一个Node无法承载所有流量，这个时候就需要一个副本来分流查询压力，目的就是扩展查询能力。 角色部署方式接下来再看看角色分工的两种不同方式： Elasticsearch支持上述两种方式： 混合部署（左图）： 默认方式。 不考虑MasterNode的情况下，还有两种Node，Data Node和Transport Node，这种部署模式下，这两种不同类型Node角色都位于同一个Node中，相当于一个Node具备两种功能：Data和Transport。 当有index或者query请求的时候，请求随机（自定义）发送给任何一个Node，这台Node中会持有一个全局的路由表，通过路由表选择合适的Node，将请求发送给这些Node，然后等所有请求都返回后，合并结果，然后返回给用户。一个Node分饰两种角色。 好处就是使用极其简单，易上手，对推广系统有很大价值。最简单的场景下只需要启动一个Node，就能完成所有的功能。 缺点就是多种类型的请求会相互影响，在大集群如果某一个Data Node出现热点，那么就会影响途经这个Data Node的所有其他跨Node请求。如果发生故障，故障影响面会变大很多。 Elasticsearch中每个Node都需要和其余的每一个Node都保持13个连接。这种情况下，每个Node都需要和其他所有Node保持连接，而一个系统的连接数是有上限的，这样连接数就会限制集群规模。 还有就是不能支持集群的热更新。 分层部署（右图）： 通过配置可以隔离开Node。 设置部分Node为Transport Node，专门用来做请求转发和结果合并。 其他Node可以设置为DataNode，专门用来处理数据。 缺点是上手复杂，需要提前设置好Transport的数量，且数量和Data Node、流量等相关，否则要么资源闲置，要么机器被打爆。 好处就是角色相互独立，不会相互影响，一般Transport Node的流量是平均分配的，很少出现单台机器的CPU或流量被打满的情况，而DataNode由于处理数据，很容易出现单机资源被占满，比如CPU，网络，磁盘等。独立开后，DataNode如果出了故障只是影响单节点的数据处理，不会影响其他节点的请求，影响限制在最小的范围内。 角色独立后，只需要Transport Node连接所有的DataNode，而DataNode则不需要和其他DataNode有连接。一个集群中DataNode的数量远大于Transport Node，这样集群的规模可以更大。另外，还可以通过分组，使Transport Node只连接固定分组的DataNode，这样Elasticsearch的连接数问题就彻底解决了。 可以支持热更新：先一台一台的升级DataNode，升级完成后再升级Transport Node，整个过程中，可以做到让用户无感知。 上面介绍了Elasticsearch的部署层架构，不同的部署方式适合不同场景，需要根据自己的需求选择适合的方式。 Elasticsearch 数据层架构接下来我们看看当前Elasticsearch的数据层架构。 数据存储Elasticsearch的Index和meta，目前支持存储在本地文件系统中，同时支持niofs，mmap，simplefs，smb等不同加载方式，性能最好的是直接将索引LOCK进内存的MMap方式。默认，Elasticsearch会自动选择加载方式，另外可以自己在配置文件中配置。这里有几个细节，具体可以看官方文档。 索引和meta数据都存在本地，会带来一个问题：当某一台机器宕机或者磁盘损坏的时候，数据就丢失了。为了解决这个问题，可以使用Replica（副本）功能。 副本（Replica） 可以为每一个Index设置一个配置项：副本（Replicda）数，如果设置副本数为2，那么就会有3个Shard，其中一个是PrimaryShard，其余两个是ReplicaShard，这三个Shard会被Mater尽量调度到不同机器，甚至机架上，这三个Shard中的数据一样，提供同样的服务能力。 副本（Replica）的目的有三个：保证服务可用性：当设置了多个Replica的时候，如果某一个Replica不可用的时候，那么请求流量可以继续发往其他Replica，服务可以很快恢复开始服务。 保证数据可靠性：如果只有一个Primary，没有Replica，那么当Primary的机器磁盘损坏的时候，那么这个Node中所有Shard的数据会丢失，只能reindex了。 提供更大的查询能力：当Shard提供的查询能力无法满足业务需求的时候， 可以继续加N个Replica，这样查询能力就能提高N倍，轻松增加系统的并发度。 问题上面说了一些优势，这种架构同样在一些场景下会有些问题。 Elasticsearch采用的是基于本地文件系统，使用Replica保证数据可靠性的技术架构，这种架构一定程度上可以满足大部分需求和场景，但是也存在一些遗憾： Replica带来成本浪费。为了保证数据可靠性，必须使用Replica，但是当一个Shard就能满足处理能力的时候，另一个Shard的计算能力就会浪费。 Replica带来写性能和吞吐的下降。每次Index或者update的时候，需要先更新Primary Shard，更新成功后再并行去更新Replica，再加上长尾，写入性能会有不少的下降。 当出现热点或者需要紧急扩容的时候动态增加Replica慢。新Shard的数据需要完全从其他Shard拷贝，拷贝时间较长。 上面介绍了Elasticsearch数据层的架构，以及副本策略带来的优势和不足，下面简单介绍了几种不同形式的分布式数据系统架构。 分布式系统第一种：基于本地文件系统的分布式系统 上图中是一个基于本地磁盘存储数据的分布式系统。Index一共有3个Shard，每个Shard除了Primary Shard外，还有一个Replica Shard。当Node 3机器宕机或磁盘损坏的时候，首先确认P3已经不可用，重新选举R3位Primary Shard，此Shard发生主备切换。然后重新找一台机器Node 7，在Node7 上重新启动P3的新Replica。由于数据都会存在本地磁盘，此时需要将Shard 3的数据从Node 6上拷贝到Node7上。如果有200G数据，千兆网络，拷贝完需要1600秒。如果没有replica，则这1600秒内这些Shard就不能服务。 为了保证可靠性，就需要冗余Shard，会导致更多的物理资源消耗。 这种思想的另外一种表现形式是使用双集群，集群级别做备份。 在这种架构中，如果你的数据是在其他存储系统中生成的，比如HDFS/HBase，那么你还需要一个数据传输系统，将准备好的数据分发到相应的机器上。 这种架构中为了保证可用性和可靠性，需要双集群或者Replica才能用于生产环境，优势和副作用在上面介绍Elasticsearch的时候已经介绍过了，这里就就不赘述了。 Elasticsearch使用的就是这种架构方式。 第二种：基于分布式文件系统的分布式系统（共享存储） 针对第一种架构中的问题，另一种思路是：存储和计算分离。 第一种思路的问题根源是数据量大，拷贝数据耗时多，那么有没有办法可以不拷贝数据？为了实现这个目的，一种思路是底层存储层使用共享存储，每个Shard只需要连接到一个分布式文件系统中的一个目录/文件即可，Shard中不含有数据，只含有计算部分。相当于每个Node中只负责计算部分，存储部分放在底层的另一个分布式文件系统中，比如HDFS。 上图中，Node 1 连接到第一个文件；Node 2连接到第二个文件；Node3连接到第三个文件。当Node 3机器宕机后，只需要在Node 4机器上新建一个空的Shard，然后构造一个新连接，连接到底层分布式文件系统的第三个文件即可，创建连接的速度是很快的，总耗时会非常短。 这种是一种典型的存储和计算分离的架构，优势有以下几个方面： 在这种架构下，资源可以更加弹性，当存储不够的时候只需要扩容存储系统的容量；当计算不够的时候，只需要扩容计算部分容量。 存储和计算是独立管理的，资源管理粒度更小，管理更加精细化，浪费更少，结果就是总体成本可以更低。 负载更加突出，抗热点能力更强。一般热点问题基本都出现在计算部分，对于存储和计算分离系统，计算部分由于没有绑定数据，可以实时的扩容、缩容和迁移，当出现热点的时候，可以第一时间将计算调度到新节点上。 这种架构同时也有一个不足： 访问分布式文件系统的性能可能不及访问本地文件系统。在上一代分布式文件系统中，这是一个比较明显的问题，但是目前使用了各种用户态协议栈后，这个差距已经越来越小了。 HBase使用的就是这种架构方式。 Solr也支持这种形式的架构。 总结上述两种架构，各有优势和不足，对于某些架构中的不足或缺陷，思路不同，解决的方案也大相径庭，但是思路跨度越大，收益一般也越大。 上面只是介绍了分布式数据（存储/搜索/分析等等）系统在存储层的两种不同架构方式，希望能对大家有用。但是分布式系统架构设计所涉及的内容广，细节多，权衡点众，如果大家对某些领域或者方面有兴趣，也可以留言，后面再探讨。 本文转载自阿里云云栖号头条，原文连接点这里，版权归原作者所有，转载目的是为了避免以后找不到。","categories":[{"name":"ES","slug":"ES","permalink":"http://huermosi.xyz/categories/ES/"}],"tags":[{"name":"ELASTICSEARCH","slug":"ELASTICSEARCH","permalink":"http://huermosi.xyz/tags/ELASTICSEARCH/"},{"name":"ES","slug":"ES","permalink":"http://huermosi.xyz/tags/ES/"}]},{"title":"Java NIO 基础知识","slug":"java/java-nio-basic","date":"2018-01-24T10:24:41.000Z","updated":"2021-04-10T10:33:20.562Z","comments":true,"path":"2018/9527888006/","link":"","permalink":"http://huermosi.xyz/2018/9527888006/","excerpt":"","text":"1. 前言前言部分是科普，读者可自行选择是否阅读这部分内容。 为什么我们需要关心 NIO？我想很多业务猿都会有这个疑问。 我在工作的前两年对这个问题也很不解，因为那个时候我认为自己已经非常熟悉 IO 操作了，读写文件什么的都非常溜了，IO 包无非就是 File、RandomAccessFile、字节流、字符流这些，感觉没什么好纠结的。最混乱的当属 InputStream/OutputStream 一大堆的类不知道谁是谁，不过了解了装饰者模式以后，也都轻松破解了。 在 Java 领域，一般性的文件操作确实只需要和 java.io 包打交道就可以了，尤其对于写业务代码的程序员来说。不过，当你写了两三年代码后，你的业务代码可能已经写得很溜了，蒙着眼睛也能写增删改查了。这个时候，也许你会想要开始了解更多的底层内容，包括并发、JVM、分布式系统、各个开源框架源码实现等，处于这个阶段的程序员会开始认识到 NIO 的用处，因为系统间通讯无处不在。 可能很多人不知道 Netty 或 Mina 有什么用？和 Tomcat 有什么区别？为什么我用 HTTP 请求就可以解决应用间调用的问题却要使用 Netty？ 当然，这些问题的答案很简单，就是为了提升性能。那意思是 Tomcat 性能不好？当然不是，它们的使用场景就不一样。当初我也不知道 Nginx 摆在 Tomcat 前面有什么用，也是经过实践慢慢领悟到了那么些意思。 Nginx 是 web 服务器，Tomcat/Jetty 是应用服务器，Netty 是通讯工具。 也许你现在还不知道 NIO 有什么用，但是一定不要放弃学习它。 2. 缓冲区操作缓冲区是 NIO 操作的核心，本质上 NIO 操作就是缓冲区操作。 写操作是将缓冲区的数据排干，如将数据从缓冲区持久化到磁盘中。 读操作是将数据填充到缓冲区中，以便应用程序后续使用数据。 当然，我们这里说的缓冲区是指用户空间的缓冲区。 简单分析下上图。应用程序发出读操作后，内核向磁盘控制器发送命令，要求磁盘返回相应数据，磁盘控制器通过 DMA 直接将数据发送到内核缓冲区。一旦内核缓冲区满了，内核即把数据拷贝到请求数据的进程指定的缓冲区中。 DMA: Direct Memory AccessWikipedia：直接内存访问是计算机科学中的一种内存访问技术。它允许某些电脑内部的硬件子系统（电脑外设），可以独立地直接读写系统内存，而不需中央处理器（CPU）介入处理 。在同等程度的处理器负担下，DMA 是一种快速的数据传送方式。很多硬件的系统会使用 DMA，包含硬盘控制器、绘图显卡、网卡和声卡。也就是说，磁盘控制器可以在不用 CPU 的帮助下就将数据从磁盘写到内存中，毕竟让 CPU 等待 IO 操作完成是一种浪费 很容易看出来，数据先到内核，然后再从内核复制到用户空间缓冲区的做法并不高效，下面简单说说为什么需要这么设计。 首先，用户空间运行的代码是不可以直接访问硬件的，需要由内核空间来负责和硬件通讯，内核空间由操作系统控制。 其次，磁盘存储的是固定大小的数据块，磁盘按照扇区来组织数据，而用户进程请求的一般都是任意大小的数据块，所以需要由内核来负责协调，内核会负责组装、拆解数据。 内核空间会对数据进行缓存和预读取，所以，如果用户进程需要的数据刚好在内核空间中，直接拷贝过来就可以了。如果内核空间没有用户进程需要的数据的话，需要挂起用户进程，等待数据准备好。 3. 虚拟内存这个概念大家都懂，这里就继续啰嗦一下了，虚拟内存是计算机系统内存管理的一种技术。前面说的缓存区操作看似简单，但是具体到底层细节，还是蛮复杂的。 下面的描述，我尽量保证准确，但是不会展开得太具体，因为虚拟内存还是蛮复杂的，要完全介绍清楚，恐怕需要很大的篇幅，如果读者对这方面的内容感兴趣的话，建议读者寻找更加专业全面的介绍资料，如《深入理解计算机系统》。 物理内存被组织成一个很大的数组，每个单元是一个字节大小，然后每个字节都有一个唯一的物理地址，这应该很好理解。 虚拟内存是对物理内存的抽象，它使得应用程序认为它自己拥有连续可用的内存（一个连续完整的地址空间），而实际上，应用程序得到的全部内存其实是一个假象，它通常会被分隔成多个物理内存碎片（后面说的页），还有部分暂时存储在外部磁盘存储器上，在需要时进行换入换出。 举个例子，在 32 位系统中，每个应用程序能访问到的内存是 4G（32 位系统的最大寻址空间 2^32），这里的 4G 就是虚拟内存，每个程序都以为自己拥有连续的 4G 空间的内存，即使我们的计算机只有 2G 的物理内存。也就是说，对于机器上同时运行的多个应用程序，每个程序都以为自己能得到连续的 4G 的内存。这中间就是使用了虚拟内存。 我们从概念上看，虚拟内存也被组织成一个很大的数组，每个单元也是一个字节大小，每个字节都有唯一的虚拟地址。它被存储于磁盘上，物理内存是它的缓存。 物理内存作为虚拟内存的缓存，当然不是以字节为单位进行组织的，那样效率太低了，它们之间是以页（page）进行缓存的。虚拟内存被分割为一个个虚拟页，物理内存也被分割为一个个物理页，这两个页的大小应该是一致的，通常是 4KB - 2MB。 举个例子，看下图： 进程 1 现在有 8 个虚拟页，其中有 2 个虚拟页缓存在主存中，6 个还在磁盘上，需要的时候再读入主存中；进程 2 有 7 个虚拟页，其中 4 个缓存在主存中，3 个还在磁盘上。 在 CPU 读取内存数据的时候，给出的是虚拟地址，将一个虚拟地址转换为物理地址的任务我们称之为地址翻译。在主存中的查询表存放了虚拟地址到物理地址的映射关系，表的内容由操作系统维护。CPU 需要访问内存时，CPU 上有一个叫做内存管理单元的硬件会先去查询真实的物理地址，然后再到指定的物理地址读取数据。 上面说的那个查询表，我们称之为页表，虚拟内存系统通过页表来判断一个虚拟页是否已经缓存在了主存中。如果是，页表会负责到物理页的映射；如果不命中，也就是我们经常会见到的概念缺页，对应的英文是 page fault，系统首先判断这个虚拟页存放在磁盘的哪个位置，然后在物理内存中选择一个牺牲页，并将虚拟页从磁盘复制到内存中，替换这个牺牲页。 在磁盘和内存之间传送页的活动叫做交换（swapping）或者页面调度（paging）。 下面，简单介绍下虚拟内存带来的好处。 SRAM缓存：表示位于 CPU 和主存之间的 L1、L2 和 L3 高速缓存。 DRAM缓存：表示虚拟内存系统的缓存，缓存虚拟页到主存中。 物理内存访问速度比高速缓存要慢 10 倍左右，而磁盘要比物理内存慢大约 100000 倍。所以，DRAM 的缓存不命中比 SRAM 缓存不命中代价要大得多，因为 DRAM 缓存一旦不命中，就需要到磁盘加载虚拟页。而 SRAM 缓存不命中，通常由 DRAM 的主存来服务。而从磁盘的一个扇区读取第一个字节的时间开销比起读这个扇区中连续的字节要慢大约 100000 倍。 了解 Kafka 的读者应该知道，消息在磁盘中的顺序存储对于 Kafka 的性能至关重要。 结论就是，IO 的性能主要是由 DRAM 的缓存是否命中决定的。 4. 内存映射文件英文名是 Memory Mapped Files，相信大家也都听过这个概念，在许多对 IO 性能要求比较高的 java 应用中会使用到，它是操作系统提供的支持，后面我们在介绍 NIO Buffer 的时候会碰到的 MappedByteBuffer 就是用来支持这一特性的。 是什么： 我们可以认为内存映射文件是一类特殊的文件，我们的 Java 程序可以直接从内存中读取到文件的内容。它是通过将整个文件或文件的部分内容映射到内存页中实现的，操作系统会负责加载需要的页，所以它的速度是非常快的。 优势： 一旦我们将数据写入到了内存映射文件，即使我们的 JVM 挂掉了，操作系统依然会帮助我们将这部分内存数据持久化到磁盘上。当然了，如果是断电的话，还是有可能会丢失数据的。 另外，它比较适合于处理大文件，因为操作系统只会在我们需要的页不在内存中时才会去加载页数据，而用其处理大量的小文件反而可能会造成频繁的缺页。 另一个重要的优势就是内存共享。我们可以在多个进程中同时使用同一个内存映射文件，也算是一种进程间协作的方式吧。想像下进程间的数据通讯平时我们一般采用 Socket 来请求，而内存共享至少可以带来 10 倍以上的性能提升。 我们还没有接触到 NIO 的 Buffer，下面就简单地示意一下： 123456789101112131415161718import java.io.RandomAccessFile;import java.nio.MappedByteBuffer;import java.nio.channels.FileChannel;public class MemoryMappedFileInJava &#123; private static int count = 10485760; //10 MB public static void main(String[] args) throws Exception &#123; RandomAccessFile memoryMappedFile = new RandomAccessFile(\"largeFile.txt\", \"rw\"); // 将文件映射到内存中，map 方法 MappedByteBuffer out = memoryMappedFile.getChannel().map(FileChannel.MapMode.READ_WRITE, 0, count); // 这一步的写操作其实是写到内存中，并不直接操作文件 for (int i = 0; i &lt; count; i++) &#123; out.put((byte) 'A'); &#125; System.out.println(\"Writing to Memory Mapped File is completed\"); // 这一步的读操作读的是内存 for (int i = 0; i &lt; 10 ; i++) &#123; System.out.print((char) out.get(i)); &#125; System.out.println(\"Reading from Memory Mapped File is completed\"); &#125;&#125; 我们需要注意的一点就是，用于加载内存映射文件的内存是堆外内存。 参考资料：Why use Memory Mapped File or MapppedByteBuffer in Java 5. 分散/聚集 IOscatter/gather IO，个人认为这个看上去很酷炫，实践中比较难使用到。 分散/聚集 IO（另一种说法是 vectored I/O 也就是向量 IO）是一种可以在单次操作中对多个缓冲区进行输入输出的方法，可以把多个缓冲区的数据写到单个数据流，也可以把单个数据流读到多个缓冲区中。 这个功能是操作系统提供的支持，Java NIO 包中已经给我们提供了操作接口 。这种操作可以提高一定的性能，因为一次操作相当于多次的线性操作，同时这也带来了原子性的支持，因为如果用多线程来操作的话，可能存在对同一文件的操作竞争。 6. 非阻塞 IO相信读者在很多地方都看到过说 NIO 其实不是代表 New IO，而是 Non-Blocking IO，我们这里不纠结这个。我想之所以会有这个说法，是因为在 Java 1.4 第一次推出 NIO 的时候，提供了 Non-Blocking IO 的支持。 在理解非阻塞 IO 前，我们首先要明白，它的对立面 阻塞模式为什么不好。 比如说 InputStream.read 这个方法，一旦某个线程调用这个方法，那么就将一直阻塞在这里，直到数据传输完毕，返回 -1，或者由于其他错误抛出了异常。 我们再拿 web 服务器来说，阻塞模式的话，每个网络连接进来，我们都需要开启一个线程来读取请求数据，然后到后端进行处理，处理结束后将数据写回网络连接，这整个流程需要一个独立的线程来做这件事。那就意味着，一旦请求数量多了以后，需要创建大量的线程，大量的线程必然带来创建线程、切换线程的开销，更重要的是，要给每个线程都分配一部分内存，会使得内存迅速被消耗殆尽。我们说多线程是性能利器，但是这就是过多的线程导致系统完全消化不了了。 通常，我们可以将 IO 分为两类：面向数据块（block-oriented）的 IO 和面向流（stream-oriented）的 IO。比如文件的读写就是面向数据块的，读取键盘输入或往网络中写入数据就是面向流的。 注意，这节混着用了流和通道这两个词，提出来这点是希望不会对读者产生困扰。 面向流的 IO 往往是比较慢的，如网络速度比较慢、需要一直等待用户新的输入等。 这个时候，我们可以用一个线程来处理多个流，让这个线程负责一直轮询这些流的状态，当有的流有数据到来后，进行相应处理，也可以将数据交给其他子线程来处理，这个线程继续轮询。 问题来了，不断地轮询也会带来资源浪费呀，尤其是当一个线程需要轮询很多的数据流的时候。 现代操作系统提供了一个叫做 readiness selection 的功能，我们让操作系统来监控一个集合中的所有的通道，当有的通道数据准备好了以后，就可以直接到这个通道获取数据。当然，操作系统不会通知我们，但是我们去问操作系统的时候，它会知道告诉我们通道 N 已经准备好了，而不需要自己去轮询（后面我们会看到，还要自己轮询的 select 和 poll）。 后面我们在介绍 Java NIO 的时候会说到 Selector，对应类java.nio.channels.Selector，这个就是 java 对 readiness selection 的支持。这样一来，我们的一个线程就可以更加高效地管理多个通道了。 上面这张图我想大家也都可能看过，就是用一个 Selector 来管理多个 Channel，实现了一个线程管理多个连接。说到底，其实就是解决了我们前面说的阻塞模式下线程创建过多的问题。 在 Java 中，继承自 SelectableChannel 的子类就是实现了非阻塞 IO 的，我们可以看到主要有 socket IO 中的 DatagramChannel 和 SocketChannel，而 FileChannel 并没有继承它。所以，文件 IO 是不支持非阻塞模式的。 在系统实现上，POSIX 提供了 select 和 poll 两种方式。它们两个最大的区别在于持有句柄的数量上，select 最多只支持到 FD_SETSIZE（一般常见的是 1024），显然很多场景都会超过这个数量。而 poll 我们想创建多少就创建多少。它们都有一个共同的缺点，那就是当有任务完成后，我们只能知道有几个任务完成了，而不知道具体是哪几个句柄，所以还需要进行一次扫描。 正是由于 select 和 poll 的不足，所以催生了以下几个实现。BSD&amp; OS X 中的 kqueue，Solaris 中的 /dev/poll，还有 Linux 中的 epoll。 Windows 没有提供额外的实现，只能使用 select。 在不同的操作系统上，JDK 分别选择相应的系统支持的非阻塞实现方式。 7. 异步 IO我们知道 Java 1.4 引入了 New IO，从 Java 7 开始，就不再是 New IO 了，而是 More New IO 来临了，我们也称之为 NIO2。 Java7 在 NIO 上带来的最大的变化应该就属引入了 Asynchronous IO（异步 IO）。本来吧，异步 IO 早就提上日程了，可是大佬们没有时间完成，所以才一直拖到了 java 7 的。废话不多说，简单来看看异步 IO 是什么。 要说异步 IO 是什么，当然还得从 Non-Blocking IO 没有解决的问题入手。非阻塞 IO 很好用，它解决了阻塞式 IO 的等待问题，但是它的缺点是需要我们去轮询才能得到结果。 而异步 IO 可以解决这个问题，线程只需要初始化一下，提供一个回调方法，然后就可以干其他的事情了。当数据准备好以后，系统会负责调用回调方法。 异步 IO 最主要的特点就是回调，其实回调在我们日常的代码中也是非常常见的。 最简单的方法就是设计一个线程池，池中的线程负责完成一个个阻塞式的操作，一旦一个操作完成，那么就调用回调方法。比如 web 服务器中，我们前面已经说过不能每来一个请求就新开一个线程，我们可以设计一个线程池，在线程池外用一个线程来接收请求，然后将要完成的任务交给线程池中的线程并提供一个回调方法，这样这个线程就可以去干其他的事情了，如继续处理其他的请求。等任务完成后，池中的线程就可以调用回调方法进行通知了。 另外一种方式就是自己不设计线程池，让操作系统帮我们实现。流程也是基本一样的，提供给操作系统回调方法，然后就可以干其他事情了，等操作完成后，操作系统会负责回调。这种方式的缺点就是依赖于操作系统的具体实现，不过也有它的一些优势。 首先，我们自己设计处理任务的线程池的话，我们需要掌握好线程池的大小，不能太大，也不能太小，这往往需要凭我们的经验；其次，让操作系统来做这件事情的话，操作系统可以在一些场景中帮助我们优化性能，如文件 IO 过程中帮助更快找到需要的数据。 操作系统对异步 IO 的实现也有很多种方式，主要有以下 3 中： Linux AIO：由 Linux 内核提供支持 POSIX AIO：Linux，Mac OS X（现在该叫 Mac OS 了），BSD，solaris 等都支持，在 Linux 中是通过 glibc 来提供支持的。 Windows：提供了一个叫做 completion ports 的机制。 这篇文章 asynchronous disk I/O 的作者表示，在类 unix 的几个系统实现中，限制太多，实现的质量太差，还不如自己用线程池进行管理异步操作。 而 Windows 系统下提供的异步 IO 的实现方式有点不一样。它首先让线程池中的线程去自旋调用GetQueuedCompletionStatus.aspx) 方法，判断是否就绪。然后，让任务跑起来，但是需要提供特定的参数来告诉执行任务的线程，让线程执行完成后将结果通知到线程池中。一旦任务完成，操作系统会将线程池中阻塞在 GetQueuedCompletionStatus 方法的线程唤醒，让其进行后续的结果处理。 Windows 智能地唤醒那些执行 GetQueuedCompletionStatus 方法的线程，以让线程池中活跃的线程数始终保持在合理的水平。这样就不至于创建太多的线程，降低线程切换的开销。 Java 7 在异步 IO 的实现上，如果是 Linux 或者其他类 Unix 系统上，是采用自建线程池实现的，如果是 Windows 系统上，是采用系统提供的 completion ports 来实现的。 所以，在非阻塞 IO 和异步 IO 之间，我们应该怎么选择呢？ 如果是文件 IO，我们没得选，只能选择异步 IO。 如果是 Socket IO，在类 unix 系统下我们应该选择使用非阻塞 IO，Netty 是基于非阻塞模式的；在 Windows 中我们应该使用异步 IO。 当然了，Java 的存在就是为了实现平台无关化，所以，其实不需要我们选择，了解这些权当让自己涨点知识吧。 8. 总结和其他几篇文章一样，也没什么好总结的，要说的都在文中了，希望读者能学到点东西吧。 如果哪里说得不对了，我想也是正常的，我这些年写的都是 Java，对于底层了解得愈发的少了，所以如果读者发现有什么不合理的内容，非常希望读者可以提出来。 本文转载自云南全攻略头条，原文连接点这里，版权归原作者所有，转载目的是为了避免以后找不到。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/categories/JAVA/"},{"name":"NIO","slug":"JAVA/NIO","permalink":"http://huermosi.xyz/categories/JAVA/NIO/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/tags/JAVA/"},{"name":"NIO","slug":"NIO","permalink":"http://huermosi.xyz/tags/NIO/"},{"name":"IO","slug":"IO","permalink":"http://huermosi.xyz/tags/IO/"}]},{"title":"微服务架构时代，运维体系建设要以“应用”为核心","slug":"microservice/microservice-operation-core","date":"2018-01-21T10:13:58.000Z","updated":"2021-04-10T10:34:43.337Z","comments":true,"path":"2018/9527888005/","link":"","permalink":"http://huermosi.xyz/2018/9527888005/","excerpt":"","text":"今天我来讲一下微服务架构模式下的一个核心概念：应用。 我会从这几个方面来讲：应用的起源、应用模型和应用关系模型建模以及为什么要这样做。最终希望，在微服务的架构模式下，我们的运维视角一定转到应用这个核心概念上来，一切要从应用的角度来分析和看待问题。 应用的起源 我们知道，微服务架构一般都是从单体架构或分层架构演进过来的。软件架构服务化的过程，就是我们根据业务模型进行细化的过程，在这个过程中切分出一个个具备不同职责的业务逻辑模块，然后每个微服务模块都会提供相对应业务逻辑的服务化接口。 如果解释得简单点，就一个字，拆！如下图，从一个单体工程，拆分出 N 个独立模块。 这些模块可以独立部署和运行，并提供对应的业务能力。拆分后的模块数量与业务体量和复杂度相关，少则几个、十几个，多则几十、几百个，所以为了统一概念，我们通常称这些模块为应用。 为了确保每个应用的唯一性，我们给每个应用定义一个唯一的标识符，如上图的 APP-1、APP-2 等，这个唯一标识符我们称之为应用名。 接下来，这个定义为应用的概念，将成为我们后续一系列微服务架构管理的核心概念。 应用模型及关系模型的建立 上面我们定义出来的一个个应用，都是从业务角度入手进行拆分细化出来的业务逻辑单元。它虽然可以独立部署和运行，但是每一个应用都只具备相对单一的业务职能。如果要完成整体的业务流程和目标，就需要和周边其它的服务化应用交互。同时，这个过程中还需要依赖各种与业务无直接关系、相对独立的基础设施和组件，比如机器资源、域名、DB、缓存、消息队列等等。 所以，除了应用这个实体之外，还会存在其他各类基础组件实体。同时，在应用运行过程中，还需要不断地与它们产生和建立各种各样复杂的关联关系，这也为我们后续的运维带来很多困难。 那接下来，我们要做的就是应用模型以及各种关系模型的梳理和建立，因为只有模型和关系梳理清楚了，才能为我们后面一系列的运维自动化、持续交付以及稳定性保障打下一个良好的基础。 应用业务模型 应用业务模型，也就是每个应用对外提供的业务服务能力，并以 API 的方式暴露给外部，如下图商品的应用业务模型示例： 这个业务模型通常都是业务架构师在进行业务需求分析和拆解时进行设计，更多的是聚焦在业务逻辑上，所以从运维的角度，我们一般不会关注太多。 而接下来的几部分，将是运维要重点关注的内容。 应用管理模型 应用管理模型，也就是应用自身的各种属性，如应用名、应用功能信息、责任人、Git 地址、部署结构（代码路径、日志路径以及各类配置文件路径等）、启停方式、健康检测方式等等。这其中，应用名是应用的唯一标识，我们用 AppName 来表示。 这里我们可以把应用想象成一个人，通常一个人会具备身份证号码、姓名、性别、家庭住址、联系方式等等属性，这里身份证号码，就是一个人的唯一标识。 应用运行时所依赖的基础设施和组件 资源层面：应用运行所必需的资源载体有物理机、虚拟机或容器等，如果对外提供 HTTP 服务，就需要虚 IP 和 DNS 域名服务； 基础组件：这一部分其实就是我们所说的中间件体系，比如应用运行过程中必然要存储和访问数据，这就需要有数据库和数据库中间件；想要更快地访问数据，同时减轻 DB 的访问压力，就需要缓存；应用之间如果需要数据交互或同步，就需要消息队列；如果进行文件存储和访问，就需要存储系统等等。 从这里我们可以挖掘出一条规律，那就是 这些基础设施和组件都是为上层的一个个业务应用所服务的。也正是因为业务和应用上的需求，才开启了它们各自的生命周期。如果脱离了这些业务应用，它们自己并没有单纯存在的意义。所以，从始至终基础设施和组件都跟应用这个概念保持着紧密的联系。 理清了这个思路，我们再去梳理它们之间的关系就会顺畅很多，分为两步。 第一步，建立各个基础设施和组件的数据模型，同时识别出它们的唯一标识。这个套路跟应用管理模型的梳理类似，以典型的缓存为例，每当我们申请一个缓存空间时，通常会以 NameSpace 来标识唯一命名，同时这个缓存空间会有空间容量和 Partition 分区等信息。 第二步，也是最关键的一步，就是识别出基础设施及组件可以与应用名 AppName 建立关联关系的属性，或者在基础组件的数据模型中增加所属应用这样的字段。 还是以上面的缓存为例，既然是应用申请的缓存空间，并且是一对一的关联关系，既可以直接将 NameSpace 字段取值设置为 AppName，也可以再增加一个所属应用这样的字段，通过外键关联模式建立起应用与缓存空间的关联关系。 相应地，对于消息队列、DB、存储空间等，都可以参考上面这个思路去做。 通过上面的梳理，我们就可以建立出类似下图这样的以应用为核心的应用模型和关联关系模型了，基于这个统一的应用概念，系统中原本分散杂乱的信息，最终都被串联了起来，应用也将成为整个运维信息管理及流转的纽带。 真实的情况是怎么样的？ 上面讲了这么多理论和道理，但是我们业界真实的现状是怎样的呢？ 从我个人实际观察和经历的场景来看，大部分公司在这块的统筹规划是不够的，或者说是不成熟的。也就是软件架构上引入了微服务，但是后续的一系列运维措施和管理手段没跟上，主要还是思路没有转变过来。虽然说要做 DevOps，但实际的执行还是把开发和运维分裂了去对待，不信你看下面两个常见的场景。 场景一 这个场景是关于线上的缓存和消息队列的。 开发使用的时候就去申请一下，一开始还能记住自己使用了哪些，但是时间一长，或者申请得多了，就记不住了。久而久之，线上就存在一堆无用的 NameSpace 和 Topic，但是集群的维护者又不敢随意清理，因为早就搞不清楚是谁用的，甚至申请人已经离职，以后会不会再用也已经没人讲得清楚了，越往后就越难维护。 根本原因，就是前面我们讲到的，太片面地对待基础组件，没有与应用的访问建立起关联关系，没有任何的生命周期管理措施。 场景二 这个典型场景就体现了应用名不统一的问题。 按照我们前面讲的，按说应用名应该在架构拆分出一个个独立应用的时候就明确下来，并贯穿整个应用生命周期才对。 但是大多数情况下，我们的业务架构师或开发在早期只考虑应用开发，并不会过多地考虑整个应用的生命周期问题，会下意识地默认后面的事情是运维负责的，所以开发期间，只要将应用开发完和将服务注册到服务配置中心上就 OK 了。 而到了运维这里，也只从软件维护的角度，为了便于资源和应用配置的管理，会独立定义一套应用名体系出来，方便自己的管理。 这时不统一的问题就出现了，如果持续交付和监控系统这些运维平台也是独立去开发的，脱节问题就会更严重。 如下图所示，一个个的孤岛，无法成为体系，当这些系统需要对接时，就会发现需要做大量的应用名转化适配工作，带来非常多无谓的工作量，所谓的效率提升就是一句空话。 所以，今天一开头我就提到，微服务架构模式下的运维思路一定要转变，一定要将视角转换到应用这个维度，从一开始就要统一规划，从一开始就要将架构、开发和运维的工作拉通了去看，这一点是与传统运维的思路完全不同的。 当然，这里面也有一个经验的问题。虽然微服务在国内被大量应用，但是我们绝大多数技术团队的经验还集中在开发设计层面。微服务架构下的运维经验，确实还需要一个总结积累的过程。我自己也是痛苦地经历了上面这些反模式，才总结积累下这些经验教训。 这也是为什么我今天分享了这样一个思路，我们要转换视角，规划以应用为核心的运维管理体系。 不知道你目前是否也遇到了类似的问题，如果今天的内容对你有帮助，也请你分享给身边的朋友。 本文节选自赵成教授在极客时间 App 开设的“赵成的运维体系管理课”，已获授权。更多相关文章，请下载极客时间 App，订阅专栏获取。 赵成专栏目录 本文转载自InfoQ头条，原文连接点这里，版权归原作者所有，转载目的是为了避免以后找不到。","categories":[{"name":"MICROSERVICE","slug":"MICROSERVICE","permalink":"http://huermosi.xyz/categories/MICROSERVICE/"}],"tags":[{"name":"MICROSERVICE","slug":"MICROSERVICE","permalink":"http://huermosi.xyz/tags/MICROSERVICE/"},{"name":"微服务","slug":"微服务","permalink":"http://huermosi.xyz/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"运维","slug":"运维","permalink":"http://huermosi.xyz/tags/%E8%BF%90%E7%BB%B4/"}]},{"title":"Thread的方法（源代码）和线程的状态","slug":"java/java-thread1","date":"2017-08-16T14:33:04.000Z","updated":"2021-04-08T08:49:31.632Z","comments":true,"path":"2017/95270316223304/","link":"","permalink":"http://huermosi.xyz/2017/95270316223304/","excerpt":"","text":"1. Thread中重要的属性1234567891011121314151617181920212223242526publicclass Thread implements Runnable &#123; //继承自Runnable接口private char name[]; // 以char数组保存线程的名字 private int priority; // 线程优先级别 /* Whether or not the thread is a daemon thread. */ private boolean daemon = false; //是否为守护线程 /* What will be run. */ private Runnable target; //构造方法中传递一个Runnable对象 最终由target指向 /* The group of this thread */ private ThreadGroup group; //线程组 //预先定义好的优先级 public final static int MIN_PRIORITY = 1; public final static int NORM_PRIORITY = 5; public final static int MAX_PRIORITY = 10; // 这个类是在ThreadLocal中定义 类似于Map的key-value的数据结构(后面会有该类的叙述) // 特殊之处:key的值是固定的 就是当前线程 ThreadLocal.ThreadLocalMap threadLocals = null; // 当不为线程命名的时候 默认名称是Thread-编号 编号从0开始增长 就是依靠这个 前面已经详细讲述 private static int threadInitNumber; private static synchronized int nextThreadNum() &#123; return threadInitNumber++; &#125; ... ...&#125; 2. 构造方法在Thread重载了很多构造方法 我们挑选几个常用的进行列举 123456789101112131415public Thread() &#123; init(null, null, \"Thread-\" + nextThreadNum(), 0);&#125;public Thread(String name) &#123; init(null, null, name, 0);&#125; public Thread(Runnable target) &#123; init(null, target, \"Thread-\" + nextThreadNum(), 0);&#125; public Thread(Runnable target, String name) &#123; init(null, target, name, 0);&#125; 可以看出Thread的构造方法最终都会调用init方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445//4个参数分别表示 线程所属组 Runnable对象 线程名字 线程栈大小 //一般只会用到第2、3个参数private void init(ThreadGroup g, Runnable target, String name, long stackSize) &#123; Thread parent = currentThread(); //获取当前运行的线程为父线程 一些属性将会基础自parent SecurityManager security = System.getSecurityManager(); if (g == null) &#123; if (security != null) &#123; g = security.getThreadGroup(); &#125; if (g == null) &#123; g = parent.getThreadGroup(); &#125; &#125; g.checkAccess(); if (security != null) &#123; if (isCCLOverridden(getClass())) &#123; security.checkPermission(SUBCLASS_IMPLEMENTATION_PERMISSION); &#125; &#125; g.addUnstarted(); this.group = g; //指定要开启线程的组 this.daemon = parent.isDaemon();//指定要开启线程是否为守护线程 来自于parent this.priority = parent.getPriority();//设置优先级的值 来自于parent this.name = name.toCharArray();//设置线程名字 if (security == null || isCCLOverridden(parent.getClass())) this.contextClassLoader = parent.getContextClassLoader(); else this.contextClassLoader = parent.contextClassLoader; this.inheritedAccessControlContext = AccessController.getContext(); this.target = target;//设置要执行的目标 Runnable对象 setPriority(priority);//设置优先级 if (parent.inheritableThreadLocals != null) this.inheritableThreadLocals = ThreadLocal.createInheritedMap(parent.inheritableThreadLocals); /* Stash the specified stack size in case the VM cares */ this.stackSize = stackSize; /* Set thread ID */ tid = nextThreadID(); this.me = this;&#125; 3. 线程的状态1234/* Java thread status for tools, * initialized to indicate thread 'not yet started'*/ private int threadStatus = 0; //描述线程状态的属性 线程有四种状态： 新状态：线程已被创建但尚未执行（start() 尚未被调用）。 可执行状态：线程可以执行，虽然不一定正在执行。CPU 时间随时可能被分配给该线程，从而使得它执行。 死亡状态：正常情况下 run() 返回使得线程死亡。调用 stop()或 destroy() 亦有同样效果，但是不被推荐，前者会产生异常，后者是强制终止，不会释放锁。 阻塞状态：线程不会被分配 CPU 时间，无法执行。 这些状态 Thread中是以枚举来描述的： 12345678public enum State &#123; NEW, RUNNABLE, BLOCKED, WAITING, TIMED_WAITING, TERMINATED;&#125; Thread中的不同方法的执行 会使线程进入不同的状态，如图： 将blocked、waiting、time waiting统称为阻塞状态，这个也是可以的 只不过这里我想将线程的状态和Java中的方法调用联系起来，所以将waiting和time waiting两个状态分离出来。 4. Thread中的成员方法1）start方法start()用来启动一个线程，当调用start方法后，系统才会开启一个新的线程来执行用户定义的子任务，在这个过程中，会为相应的线程分配需要的资源。 一个线程只能start 1次 以为一个只需要分配一次资源就够了 如果启动多次 就会出错：非法的线程状态异常 IllegalThreadStateException 123456789101112public synchronized void start() &#123; if (threadStatus != 0 || this != me) //只有处于未开启状态的线程才可以继续执行 throw new IllegalThreadStateException(); group.add(this); start0(); if (stopBeforeStart) &#123; stop0(throwableFromStop);&#125;&#125;private native void start0();private native void stop0(Object o); 2）run方法run()方法是不需要用户来调用的，当通过start方法启动一个线程之后，当线程获得了CPU执行时间，便进入run方法体去执行具体的任务。 注意，继承Thread类必须重写run方法，在run方法中定义具体要执行的任务。 如果我们覆盖了run方法 就会执行我们覆盖的方法 如果我们向Thread传递了一个Runnable对象 就会执行该对象的run方法 12345public void run() &#123; if (target != null) &#123; //判断是否有Runnable对象传入 target.run(); &#125;&#125; 那么请思考下面程序的输出： 1234567891011121314public static void main(String[] args) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(\"run in Runnable\"); &#125; &#125;) &#123; public void run() &#123; System.out.println(\"run in Thread\"); &#125; &#125;.start();&#125; 答案：run in Thread 这就表明既复写run方法又传递Runnable时执行的是覆盖的方法 因为执行原有的run方法已经失效 根本不会判断if (target != null) 更不会执行target.run(); 3）sleep方法sleep方法有两个重载版本： sleep(long millis) //参数为毫秒 sleep(long millis,int nanoseconds) //第一参数为毫秒，第二个参数为纳秒 sleep相当于让线程睡眠，交出CPU，让CPU去执行其他的任务。 但是有一点要非常注意，sleep方法不会释放锁，也就是说如果当前线程持有对某个对象的锁，则即使调用sleep方法，其他线程也无法访问这个对象。 注意，如果调用了sleep方法，必须捕获InterruptedException异常或者将该异常向上层抛出。 当线程睡眠时间满后，不一定会立即得到执行，因为此时可能CPU正在执行其他的任务。所以说调用sleep方法相当于让线程进入阻塞状态。 4）yield方法调用yield方法会让当前线程交出CPU权限，让CPU去执行其他的线程。它跟sleep方法类似，同样不会释放锁。 但是yield不能控制具体的交出CPU的时间，另外，yield方法只能让拥有相同优先级的线程有获取CPU执行时间的机会。 注意，调用yield方法并不会让线程进入阻塞状态，而是让线程重回就绪状态，它只需要等待重新获取CPU执行时间，这一点是和sleep方法不一样的。 5）join方法join方法有三个重载版本： join() join(long millis) //参数为毫秒 join(long millis,int nanoseconds) //第一参数为毫秒，第二个参数为纳秒 假如在main线程中，调用thread.join方法，则main方法会等待thread线程执行完毕或者等待一定的时间。 如果调用的是无参join方法，则等待thread执行完毕，如果调用的是指定了时间参数的join方法，则等待一定的时间。 6）interrupt方法interrupt，顾名思义，即中断的意思。单独调用interrupt方法可以使得处于阻塞状态的线程抛出一个异常， 也就说，它可以用来中断一个正处于阻塞状态的线程直接调用interrupt方法不能中断正在运行中的线程；另外，通过interrupt方法和isInterrupted()方法来停止正在运行的线程。 但是一般情况下不建议通过这种方式来中断线程，一般会在MyThread类中增加一个属性 isStop来标志是否结束while循环，然后再在while循环中判断isStop的值。 那么就可以在外面通过调用setStop方法来终止while循环。 12345678910111213141516class MyThread extends Thread&#123; private volatile boolean isStop = false; public void run() &#123; int i = 0; while(!isStop)&#123; i++; &#125; &#125; public void setStop(boolean stop)&#123; this.isStop = stop; &#125;&#125; 7）stop方法stop方法已经是一个废弃的方法，它是一个不安全的方法。 因为调用stop方法会直接终止run方法的调用，并且会抛出一个ThreadDeath错误，如果线程持有某个对象锁的话，会完全释放锁，导致对象状态不一致。所以stop方法基本是不会被用到的。 8）destroy方法destroy方法也是废弃的方法。基本不会被使用到。 9）以下是关系到线程属性的几个方法： getId 用来得到线程ID getName和setName 用来得到或者设置线程名称。 getPriority和setPriority 用来获取和设置线程优先级。 setDaemon和isDaemon 用来设置线程是否成为守护线程和判断线程是否是守护线程。 守护线程和用户线程的区别在于：守护线程依赖于创建它的线程，而用户线程则不依赖。 举个简单的例子：如果在main线程中创建了一个守护线程，当main方法运行完毕之后，守护线程也会随着消亡。而用户线程则不会，用户线程会一直运行直到其运行完毕。在JVM中，像垃圾收集器线程就是守护线程。 10）Thread类有一个比较常用的静态方法currentThread()用来获取当前线程。 11）holdsLock 判断线程是否有锁 Returns true if and only if the current thread holds the monitor lock on the specified object. 参考：Java并发编程：Thread类的使用 本文转载自yweihainan博客，原文连接点这里，版权归原作者所有。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/categories/JAVA/"},{"name":"多线程","slug":"JAVA/多线程","permalink":"http://huermosi.xyz/categories/JAVA/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/tags/JAVA/"},{"name":"THREAD","slug":"THREAD","permalink":"http://huermosi.xyz/tags/THREAD/"},{"name":"多线程","slug":"多线程","permalink":"http://huermosi.xyz/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"JAVA单例模式的多种实现方式","slug":"java/java-singleton","date":"2017-08-16T12:13:54.000Z","updated":"2021-04-08T08:49:31.628Z","comments":true,"path":"2017/95270316201354/","link":"","permalink":"http://huermosi.xyz/2017/95270316201354/","excerpt":"","text":"一、意图保证一个类仅有一个实例，并提供一个访问它的全局访问点。 我们怎么样才能保证一个类只有一个实例并且这个实例易于被访问呢？ 如果将对象赋值给一个java静态变量，那么你必须在程序一开始就创建好对象。万一这个对象非常耗费资源，而程序在这次的执行过程中又一直没有使用到它，不就形成浪费吗？ 一个更好的办法是，让类自身负责保存它的唯一实例。这个类可以保证没有其他实例可以被创建（通过截取创建新对象的请求） ，并且它可以提供一个访问该实例的方法。 这就是Singleton模式，我们可以在需要时才创建对象。在计算机系统中，线程池、缓存、日志对象、对话框、打印机、显卡的驱动程序对象常被设计成单例。 单例模式就是要确保类在内存中只有一个对象，该实例必须自动创建，并且对外提供访问接口。 优点:在系统内存中只存在一个对象，因此可以节约系统资源，对于一些需要频繁创建和销毁的对象单例模式无疑可以提高系统的性能。 缺点:没有抽象层，因此扩展很难。 职责过重，在一定程序上违背了单一职责 二、单例模式的实现第一种：懒汉式，线程不安全 1234567891011121314151617public class LazyNotSecurtySingleton &#123; //使用静态变量来记录Singleton类的唯一实例 private static LazyNotSecurtySingleton instance; // 私有构造，确保只有自类内部才能访问 private LazyNotSecurtySingleton() &#123; &#125; //返回该类的实例， 有线程同步问题 public static LazyNotSecurtySingleton getInstance() &#123; if (instance == null) &#123; instance = new LazyNotSecurtySingleton(); &#125; return instance; &#125;&#125; Singleton通过将构造方法限定为private避免了类在外部被实例化，在同一个虚拟机范围内，Singleton的唯一实例只能通过getInstance()方法访问。（事实上，通过Java反射机制是能够实例化构造方法为private的类的，那基本上会使所有的Java单例实现失效。此问题在此处不做讨论，姑且掩耳盗铃地认为反射机制不存在。） 但是以上实现没有考虑线程安全问题。所谓线程安全是指：如果你的代码所在的进程中有多个线程在同时运行，而这些线程可能会同时运行这段代码。如果每次运行结果和单线程运行的结果是一样的，而且其他的变量的值也和预期的是一样的，就是线程安全的。或者说：一个类或者程序所提供的接口对于线程来说是原子操作或者多个线程之间的切换不会导致该接口的执行结果存在二义性,也就是说我们不用考虑同步的问题。显然以上实现并不满足线程安全的要求，在并发环境下很可能出现多个Singleton实例。 如下场景就会导致多个实例： 当线程A进入到第12行时，检查instance是否为空，此时是空的。 此时，线程B也进入到12行。切换到线程B执行。同样检查instance为空，于是往下执行13行，创建了一个实例。接着返回了。 在切换回线程A，由于之前检查到instance为空。所以也会执行13行创建实例。返回。 至此，已经有两个实例被创建了，这不是我们所希望的。 第二种：饿汉式，线程安全12345678910public class EagerSingleton &#123; //私有的类成员常量 private static final EagerSingleton SINGLETON = new EagerSingleton(); //私有的默认构造方法，此类不能被继承 private EagerSingleton()&#123;&#125; //静态工厂方法 public static EagerSingleton getInstance()&#123; return SINGLETON; &#125; &#125; 这种方式，我们依赖JVM在加载这个类时马上创建该类的唯一实例，避免了线程安全问题。不过，instance在类装载时就实例化，没有达到lazy loading的效果。 JAVA中的Runtime类就是采用这种方式 123456789101112131415161718192021222324/* * Runtime:每个 Java 应用程序都有一个 Runtime 类实例，使应用程序能够与其运行的环境相连接。 * exec(String command) */public class RuntimeDemo &#123; public static void main(String[] args) throws IOException &#123; Runtime r = Runtime.getRuntime(); // r.exec(\"notepad\"); // r.exec(\"calc\"); // r.exec(\"shutdown -s -t 10000\"); r.exec(\"shutdown -a\"); &#125;&#125;/* * class Runtime &#123; * private Runtime() &#123;&#125; * private static Runtime currentRuntime = new Runtime(); * public static Runtime getRuntime() &#123; * return currentRuntime; * &#125; * &#125; */ 第三种：懒汉式，线程安全1234567891011121314151617181920public class LazySecurtySingleton &#123; private static LazySecurtySingleton instance; /** * 私有构造子，确保无法在类外实例化该类 */ private LazySecurtySingleton() &#123; &#125; /** * synchronized关键字解决多个线程的同步问题 */ public static synchronized LazySecurtySingleton getInstance() &#123; if (instance == null) &#123; instance = new LazySecurtySingleton(); &#125; return instance; &#125; &#125; 静态工厂方法中synchronized关键字提供的同步是必须的，否则当多个线程同时访问该方法时，无法确保获得的总是同一个实例。然而我们也看到，在所有的代码路径中，虽然只有第一次引用的时候需要对instance变量进行实例化，但是synchronized同步机制要求所有的代码执行路径都必须先获取类锁。在并发访问比较低时，效果并不显著，但是当并发访问量上升时，这里有可能会成为并发访问的瓶颈。 但给方法加上synchronized后。所有getInstance()的调用都要同步了。其实我们只是在第一次调用的时候要同步。而同步需要消耗性能。这就是问题。 第四种：双重校验锁可以使用“双重检查加锁”的方式来实现，就可以既实现线程安全，又能够使性能不受很大的影响。那么什么是“双重检查加锁”机制呢？ 所谓“双重检查加锁”机制，指的是：并不是每次进入getInstance方法都需要同步，而是先不同步，进入方法后，先检查实例是否存在，如果不存在才进行下面的同步块，这是第一重检查，进入同步块过后，再次检查实例是否存在，如果不存在，就在同步的情况下创建一个实例，这是第二重检查。这样一来，就只需要同步一次了，从而减少了多次在同步情况下进行判断所浪费的时间。 “双重检查加锁”机制的实现会使用关键字volatile，它的意思是：被volatile修饰的变量的值，将不会被本地线程缓存，所有对该变量的读写都是直接操作共享内存，从而确保多个线程能正确的处理该变量。关于volatile参看。 注意：在java1.4及以前版本中，很多JVM对于volatile关键字的实现的问题，会导致“双重检查加锁”的失败，因此“双重检查加锁”机制只能用在JAVA5及以上的版本。 123456789101112131415161718public class TwoLockSingleton &#123; private volatile static TwoLockSingleton singleton; private TwoLockSingleton() &#123; &#125; public static TwoLockSingleton getInstance() &#123; //先检查实例是否存在，如果不存在才进入下面的同步块 if (singleton == null) &#123; //同步块，线程安全的创建实例 synchronized (TwoLockSingleton.class) &#123; //再次检查实例是否存在，如果不存在才真正的创建实例 if (singleton == null) &#123; singleton = new TwoLockSingleton(); &#125; &#125; &#125; return singleton; &#125;&#125; 这种实现方式既可以实现线程安全地创建实例，而又不会对性能造成太大的影响。它只是第一次创建实例的时候同步，以后就不需要同步了，从而加快了运行速度。 第五种：使用内部类 Initialization-on-demand holder – idiomIn software engineering, the Initialization on Demand Holder (design pattern) idiom is a lazy-loaded singleton.In all versions of Java, the idiom enables a safe, highly concurrent lazy initialization with good performance. 1234567891011public class Something &#123; private Something() &#123;&#125; private static class LazyHolder &#123; private static final Something INSTANCE = new Something(); &#125; public static Something getInstance() &#123; return LazyHolder.INSTANCE; &#125;&#125; 关于idiom详见http://en.wikipedia.org/wiki/Initialization_on_demand_holder_idiom 解释一下，因为java机制规定，内部类SingletonHolder只有在getInstance()方法第一次调用的时候才会被加载（实现了lazy）， 而且其加载过程是线程安全的（实现线程安全）。内部类加载的时候实例化一次instance。 有兴趣可以阅读下面的文字：The implementation of the idiom relies on the initialization phase of execution within the Java Virtual Machine (JVM) as specified by the Java Language Specification (JLS).[2] When the class Something is loaded by the JVM, the class goes through initialization. Since the class does not have any static variables to initialize, the initialization completes trivially. The static class definition LazyHolder within it is not initialized until the JVM determines thatLazyHolder must be executed. The static class LazyHolder is only executed when the static method getInstance is invoked on the class Something, and the first time this happens the JVM will load and initialize the LazyHolder class. The initialization of the LazyHolder class results in static variable INSTANCE being initialized by executing the (private) constructor for the outer class Something. Since the class initialization phase is guaranteed by the JLS to be serial, i.e., non-concurrent, no further synchronization is required in the static getInstance method during loading and initialization. And since the initialization phase writes the static variable INSTANCE in a serial operation, all subsequent concurrent invocations of the getInstance will return the same correctly initialized INSTANCE without incurring any additional synchronization overhead. This gives a highly efficient thread-safe “singleton” cache, without synchronization overhead; benchmarking indicates it to be far faster than even uncontended synchronization.[3] However, the idiom is singleton-specific and not extensible to pluralities of objects (e.g. a map-based cache). 本文转载自yweihainan博客，原文连接点这里，版权归原作者所有。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/tags/JAVA/"},{"name":"单例模式","slug":"单例模式","permalink":"http://huermosi.xyz/tags/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"}]},{"title":"JAVA内存模型与volatile","slug":"java/java-memory-model-and-volatile","date":"2017-08-16T12:04:49.000Z","updated":"2021-04-08T08:49:31.608Z","comments":true,"path":"2017/95270316200449/","link":"","permalink":"http://huermosi.xyz/2017/95270316200449/","excerpt":"","text":"一 从单例模式说起在一文中我们详细了解JAVA中单例模式的实现，不了解的可以先阅读之。在该文最后我们给出了双重校验锁来保证既实现线程安全，又能够使性能不受很大的影响的单例模式 代码如下: 123456789101112131415161718public class TwoLockSingleton &#123; private volatile static TwoLockSingleton singleton; private TwoLockSingleton() &#123; &#125; public static TwoLockSingleton getInstance() &#123; //先检查实例是否存在，如果不存在才进入下面的同步块 if (singleton == null) &#123; //同步块，线程安全的创建实例 synchronized (TwoLockSingleton.class) &#123; //再次检查实例是否存在，如果不存在才真正的创建实例 if (singleton == null) &#123; singleton = new TwoLockSingleton(); &#125; &#125; &#125; return singleton; &#125;&#125; “双重检查加锁”机制的实现会使用关键字volatile，它的意思是：被volatile修饰的变量的值，将不会被本地线程缓存，所有对该变量的读写都是直接操作共享内存，从而确保多个线程能正确的处理该变量。 在深入了解volatile之前 我们先了解一下java内存模型和并发编程中的3个问题。 二 JAVA内存模型在进一步讨论volatile前，我们先看Java规范里规定的多线程环境下的内存模型。 大家都知道，计算机在执行程序时，每条指令都是在CPU中执行的，而执行指令过程中，势必涉及到数据的读取和写入。 由于程序运行过程中的临时数据是存放在主存（物理内存）当中的，这时就存在一个问题，由于CPU执行速度很快，而从内存读取数据和向内存写入数据的过程跟CPU执行指令的速度比起来要慢的多，因此如果任何时候对数据的操作都要通过和内存的交互来进行，会大大降低指令执行的速度。 因此在CPU里面就有了高速缓存。也就是，当程序在运行过程中，会将运算需要的数据从主存复制一份到CPU的高速缓存当中，那么CPU进行计算时就可以直接从它的高速缓存读取数据和向其中写入数据，当运算结束之后，再将高速缓存中的数据刷新到主存当中。举个简单的例子，比如下面的这段代码： 1i = i + 1; 当线程执行这个语句时，会先从主存当中读取i的值，然后复制一份到高速缓存当中，然后CPU执行指令对i进行加1操作，然后将数据写入高速缓存，最后将高速缓存中i最新的值刷新到主存当中。 这个代码在单线程中运行是没有任何问题的，但是在多线程中运行就会有问题了。在多核CPU中，每条线程可能运行于不同的CPU中，因此每个线程运行时有自己的高速缓存。 在Java中，一般将共享变量存放的地方叫主存，将各个线程的工作空间，即存放副本的地方叫做本地内存。这种模式被称为共享内存模型：线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信。多个线程之间是不能直接传递数据交互的，它们之间的交互只能通过共享变量来实现. Java语言规范中提到过，JVM中存在一个主存区(Main Memory或Java Heap Memory)，Java中所有变量都是存在主存中的，对于所有线程进行共享，而每个线程又存在自己的工作内存(Working Memory)，工作内存中保存的是主存中某些变量的拷贝，线程对所有变量的操作并非发生在主存区，而是发生在工作内存中，而线程之间是不能直接相互访问，变量在程序中的传递，是依赖主存来完成的。 问题:Java内存模型里面为什么要分主存和本地内存？ 本地内存缓存共享变量副本，是线程的操作区间，线程对所有变量的操作并非发生在主存区，而是发生在工作内存中，而线程之间是不能直接相互访问。 主存是为了存放共享变量，通过本地内存中的副本刷新到贮存，借以实现多线程之间的通信。 这种模式对于多线程环境下共享数据是会带来数据不一致的危险的（不进行并发控制时）。 如：我们希望实现对2个线程完成对a先+1后-1的操作 就可能发生我们不希望的情况 下面是一种： 但有了volatile就会打破这种模式： 被volatile修饰的变量的值，将不会被本地线程缓存，所有对该变量的读写都是直接操作共享内存，从而确保多个线程能正确的处理该变量，从而一个线程对a进行了修改，其他线程就立即可见。 三 并发编程中的三个概念在并发编程中，我们通常会遇到以下三个问题：原子性问题，可见性问题，有序性问题。 1.原子性 原子性：即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。 一个很经典的例子就是银行账户转账问题： 比如从账户A向账户B转1000元，那么必然包括2个操作：从账户A减去1000元，往账户B加上1000元。 试想一下，如果这2个操作不具备原子性，会造成什么样的后果。假如从账户A减去1000元之后，操作突然中止。 然后又从B取出了500元，取出500元之后，再执行 往账户B加上1000元 的操作。这样就会导致账户A虽然减去了1000元，但是账户B没有收到这个转过来的1000元。 所以这2个操作必须要具备原子性才能保证不出现一些意外的问题。 同样地反映到并发编程中会出现什么结果呢？ 举个最简单的例子，大家想一下假如为一个32位的变量赋值过程不具备原子性的话，会发生什么后果？ 1i = 9; 假若一个线程执行到这个语句时，我暂且假设为一个32位的变量赋值包括两个过程：为低16位赋值，为高16位赋值。 那么就可能发生一种情况：当将低16位数值写入之后，突然被中断，而此时又有一个线程去读取i的值，那么读取到的就是错误的数据。 Java里的原子性： 在Java中，对基本数据类型的变量的读取和赋值操作是原子性操作，即这些操作是不可被中断的，要么执行，要么不执行。 上面一句话虽然看起来简单，但是理解起来并不是那么容易。看下面一个例子i： 请分析以下哪些操作是原子性操作： 12341 x = 10; //语句12 y = x; //语句23 x++; //语句34 x = x + 1; //语句4 咋一看，有些朋友可能会说上面的4个语句中的操作都是原子性操作。其实只有语句1是原子性操作，其他三个语句都不是原子性操作。 语句1是直接将数值10赋值给x，也就是说线程执行这个语句的会直接将数值10写入到工作内存中。 语句2实际上包含2个操作，它先要去读取x的值，再将x的值写入工作内存，虽然读取x的值以及 将x的值写入工作内存 这2个操作都是原子性操作，但是合起来就不是原子性操作了。 同样的，x++和 x = x+1包括3个操作：读取x的值，进行加1操作，写入新的值。 所以上面4个语句只有语句1的操作具备原子性。 也就是说，只有简单的读取、赋值（而且必须是将数字赋值给某个变量，变量之间的相互赋值不是原子操作）才是原子操作。 不过这里有一点需要注意：在32位平台下，对64位数据的读取和赋值是需要通过两个操作来完成的，不能保证其原子性。但是好像在最新的JDK中，JVM已经保证对64位数据的读取和赋值也是原子性操作了。 从上面可以看出，Java内存模型只保证了基本读取和赋值是原子性操作，如果要实现更大范围操作的原子性，可以通过synchronized和Lock来实现。 由于synchronized和Lock能够保证任一时刻只有一个线程执行该代码块，那么自然就不存在原子性问题了，从而保证了原子性。 2.可见性 可见性是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。 举个简单的例子，看下面这段代码： 123456//线程1执行的代码int i = 0;i = 10; //线程2执行的代码j = i; 假若执行线程1的是CPU1，执行线程2的是CPU2。由上面的分析可知，当线程1执行 i =10这句时，会先把i的初始值加载到CPU1的高速缓存中， 然后赋值为10，那么在CPU1的高速缓存当中i的值变为10了，却没有立即写入到主存当中。 此时线程2执行 j = i，它会先去主存读取i的值并加载到CPU2的缓存当中，注意此时内存当中i的值还是0，那么就会使得j的值为0，而不是10. 这就是可见性问题，线程1对变量i修改了之后，线程2没有立即看到线程1修改的值。 Java的可见性： 对于可见性，Java提供了volatile关键字来保证可见性。 当一个共享变量被volatile修饰时，它会保证修改的值会立即被更新到主存，当有其他线程需要读取时，它会去内存中读取新值。 而普通的共享变量不能保证可见性，因为普通共享变量被修改之后，什么时候被写入主存是不确定的，当其他线程去读取时，此时内存中可能还是原来的旧值，因此无法保证可见性。 另外，通过synchronized和Lock也能够保证可见性，synchronized和Lock能保证同一时刻只有一个线程获取锁然后执行同步代码，并且在释放锁之前会将对变量的修改刷新到主存当中。因此可以保证可见性。 3.有序性 有序性：即程序执行的顺序按照代码的先后顺序执行。举个简单的例子，看下面这段代码： 1234int i = 0; boolean flag = false;i = 1; //语句1 flag = true; //语句2 上面代码定义了一个int型变量，定义了一个boolean类型变量，然后分别对两个变量进行赋值操作。 从代码顺序上看，语句1是在语句2前面的，那么JVM在真正执行这段代码的时候会保证语句1一定会在语句2前面执行吗？ 不一定，为什么呢？这里可能会发生指令重排序（Instruction Reorder）。 下面解释一下什么是指令重排序，一般来说，处理器为了提高程序运行效率，可能会对输入代码进行优化，它不保证程序中各个语句的执行先后顺序同代码中的顺序一致， 但是它会保证程序最终执行结果和代码顺序执行的结果是一致的。 比如上面的代码中，语句1和语句2谁先执行对最终的程序结果并没有影响，那么就有可能在执行过程中，语句2先执行而语句1后执行。 但是要注意，虽然处理器会对指令进行重排序，但是它会保证程序最终结果会和代码顺序执行结果相同，那么它靠什么保证的呢？再看下面一个例子： 1234int a = 10; //语句1int r = 2; //语句2a = a + 3; //语句3r = a*a; //语句4 这段代码有4个语句，那么可能的一个执行顺序是： 那么可不可能是这个执行顺序呢： 语句2 语句1 语句4 语句3 不可能，因为处理器在进行重排序时是会考虑指令之间的数据依赖性，如果一个指令Instruction 2必须用到Instruction 1的结果，那么处理器会保证Instruction 1会在Instruction 2之前执行。 虽然重排序不会影响单个线程内程序执行的结果，但是多线程呢？下面看一个例子： 123456789//线程1:context = loadContext(); //语句1inited = true; //语句2 //线程2:while(!inited )&#123; sleep()&#125;doSomethingwithconfig(context); 上面代码中，由于语句1和语句2没有数据依赖性，因此可能会被重排序。 假如发生了重排序，在线程1执行过程中先执行语句2，而此是线程2会以为初始化工作已经完成， 那么就会跳出while循环，去执行doSomethingwithconfig(context)方法，而此时context并没有被初始化，就会导致程序出错。 从上面可以看出，指令重排序不会影响单个线程的执行，但是会影响到线程并发执行的正确性。 也就是说，要想并发程序正确地执行，必须要保证原子性、可见性以及有序性。只要有一个没有被保证，就有可能会导致程序运行不正确。 Java的有序性： 在Java内存模型中，允许编译器和处理器对指令进行重排序，但是重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性。 在Java里面，可以通过volatile关键字来保证一定的“有序性”（具体原理在下一节讲述）。 另外可以通过synchronized和Lock来保证有序性，很显然，synchronized和Lock保证每个时刻是有一个线程执行同步代码，相当于是让线程顺序执行同步代码，自然就保证了有序性。 另外，Java内存模型具备一些先天的“有序性”，即不需要通过任何手段就能够得到保证的有序性，这个通常也称为 happens-before 原则。 如果两个操作的执行次序无法从happens-before原则推导出来，那么它们就不能保证它们的有序性，虚拟机可以随意地对它们进行重排序。 下面就来具体介绍下happens-before原则（先行发生原则）： 程序次序规则：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作 锁定规则：一个unLock操作先行发生于后面对同一个锁额lock操作 volatile变量规则：对一个变量的写操作先行发生于后面对这个变量的读操作 传递规则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C 线程启动规则：Thread对象的start()方法先行发生于此线程的每个一个动作 线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生 线程终结规则：线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行 对象终结规则：一个对象的初始化完成先行发生于他的finalize()方法的开始 这8条原则摘自《深入理解Java虚拟机》。 这8条规则中，前4条规则是比较重要的，后4条规则都是显而易见的。 下面我们来解释一下前4条规则：对于程序次序规则来说，我的理解就是一段程序代码的执行在单个线程中看起来是有序的。注意，虽然这条规则中提到“书写在前面的操作先行发生于书写在后面的操作”，这个应该是程序看起来执行的顺序是按照代码顺序执行的，因为虚拟机可能会对程序代码进行指令重排序。虽然进行重排序，但是最终执行的结果是与程序顺序执行的结果一致的，它只会对不存在数据依赖性的指令进行重排序。因此，在单个线程中，程序执行看起来是有序执行的，这一点要注意理解。事实上，这个规则是用来保证程序在单线程中执行结果的正确性，但无法保证程序在多线程中执行的正确性。 第二条规则也比较容易理解，也就是说无论在单线程中还是多线程中，同一个锁如果出于被锁定的状态，那么必须先对锁进行了释放操作，后面才能继续进行lock操作。 第三条规则是一条比较重要的规则，也是后文将要重点讲述的内容。直观地解释就是，如果一个线程先去写一个变量，然后一个线程去进行读取，那么写入操作肯定会先行发生于读操作。 第四条规则实际上就是体现happens-before原则具备传递性。 四 volatile1 语义一旦一个共享变量（类的成员变量、类的静态成员变量）被volatile修饰之后，那么就具备了两层语义： 保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的。 禁止进行指令重排序。 volatile关键字禁止指令重排序有两层意思： 当程序执行到volatile变量的读操作或者写操作时，在其前面的操作的更改肯定全部已经进行，且结果已经对后面的操作可见；在其后面的操作肯定还没有进行； 在进行指令优化时，不能将在对volatile变量访问的语句放在其后面执行，也不能把volatile变量后面的语句放到其前面执行。 先看一段代码，假如线程1先执行，线程2后执行： 12345678//线程1boolean stop = false;while(!stop)&#123; doSomething();&#125; //线程2stop = true; 这段代码是很典型的一段代码，很多人在中断线程时可能都会采用这种标记办法。 但是事实上，这段代码会完全运行正确么？ 即一定会将线程中断么？不一定，也许在大多数时候，这个代码能够把线程中断，但是也有可能会导致无法中断线程（虽然这个可能性很小，但是只要一旦发生这种情况就会造成死循环了）。 下面解释一下这段代码为何有可能导致无法中断线程。在前面已经解释过，每个线程在运行过程中都有自己的工作内存，那么线程1在运行的时候，会将stop变量的值拷贝一份放在自己的工作内存当中。 那么当线程2更改了stop变量的值之后，但是还没来得及写入主存当中，线程2转去做其他事情了，那么线程1由于不知道线程2对stop变量的更改，因此还会一直循环下去。 但是用volatile修饰之后就变得不一样了：第一：使用volatile关键字会强制将修改的值立即写入主存； 第二：使用volatile关键字的话，当线程2进行修改时，会导致线程1的工作内存中缓存变量stop的缓存行无效（反映到硬件层的话，就是CPU的L1或者L2缓存中对应的缓存行无效）； 第三：由于线程1的工作内存中缓存变量stop的缓存行无效，所以线程1再次读取变量stop的值时会去主存读取。 那么在线程2修改stop值时（当然这里包括2个操作，修改线程2工作内存中的值，然后将修改后的值写入内存），会使得线程1的工作内存中缓存变量stop的缓存行无效， 然后线程1读取时，发现自己的缓存行无效，它会等待缓存行对应的主存地址被更新之后，然后去对应的主存读取最新的值。 那么线程1读取到的就是最新的正确的值。 2 volatile无法保证对变量的任何操作都是原子性的从上面知道volatile关键字保证了操作的可见性，但是volatile能保证对变量的操作是原子性吗？ 1234567891011121314151617181920212223public class Test &#123; public volatile int inc = 0; public void increase() &#123; inc++; &#125; public static void main(String[] args) &#123; final Test test = new Test(); for(int i=0;i&lt;10;i++)&#123; new Thread()&#123; public void run() &#123; for(int j=0;j&lt;1000;j++) test.increase(); &#125;; &#125;.start(); &#125; while(Thread.activeCount()&gt;1) //保证前面的线程都执行完 Thread.yield(); System.out.println(test.inc); &#125;&#125; 大家想一下这段程序的输出结果是多少？也许有些朋友认为是10000。但是事实上运行它会发现每次运行结果都不一致，都是一个小于10000的数字。 可能有的朋友就会有疑问，不对啊，上面是对变量inc进行自增操作，由于volatile保证了可见性，那么在每个线程中对inc自增完之后， 在其他线程中都能看到修改后的值啊，所以有10个线程分别进行了1000次操作，那么最终inc的值应该是1000*10=10000。 这里面就有一个误区了，volatile关键字能保证可见性没有错，但是上面的程序错在没能保证原子性。可见性只能保证每次读取的是最新的值，但是volatile没办法保证对变量的操作的原子性。 在前面已经提到过，自增操作是不具备原子性的，它包括读取变量的原始值、进行加1操作、写入工作内存。那么就是说自增操作的三个子操作可能会分割开执行，就有可能导致下面这种情况出现： 假如某个时刻变量inc的值为10， 线程1对变量进行自增操作，线程1先读取了变量inc的原始值，然后线程1被阻塞了； 然后线程2对变量进行自增操作，线程2也去读取变量inc的原始值，由于线程1只是对变量inc进行读取操作，而没有对变量进行修改操作， 所以不会导致线程2的工作内存中缓存变量inc的缓存行无效，所以线程2会直接去主存读取inc的值，发现inc的值时10，然后进行加1操作，并把11写入工作内存，最后写入主存。 然后线程1接着进行加1操作，由于已经读取了inc的值，注意此时在线程1的工作内存中inc的值仍然为10，所以线程1对inc进行加1操作后inc的值为11，然后将11写入工作内存，最后写入主存。 那么两个线程分别进行了一次自增操作后，inc只增加了1。 解释到这里，可能有朋友会有疑问，不对啊，前面不是保证一个变量在修改volatile变量时，会让缓存行无效吗？然后其他线程去读就会读到新的值，对，这个没错。这个就是上面的happens-before规则中的volatile变量规则，但是要注意，线程1对变量进行读取操作之后，被阻塞了的话，并没有对inc值进行修改。然后虽然volatile能保证线程2对变量inc的值读取是从内存中读取的，但是线程1没有进行修改，所以线程2根本就不会看到修改的值。 根源就在这里，自增操作不是原子性操作，而且volatile也无法保证对变量的任何操作都是原子性的。 把上面的代码改成以下任何一种都可以达到效果：采用synchronized 1234567891011121314151617181920212223public class Test &#123; public int inc = 0; public synchronized void increase() &#123; inc++; &#125; public static void main(String[] args) &#123; final Test test = new Test(); for(int i=0;i&lt;10;i++)&#123; new Thread()&#123; public void run() &#123; for(int j=0;j&lt;1000;j++) test.increase(); &#125;; &#125;.start(); &#125; while(Thread.activeCount()&gt;1) //保证前面的线程都执行完 Thread.yield(); System.out.println(test.inc); &#125;&#125; 采用Lock： 1234567891011121314151617181920212223242526272829public class Test &#123; public int inc = 0; Lock lock = new ReentrantLock(); public void increase() &#123; lock.lock(); try &#123; inc++; &#125; finally&#123; lock.unlock(); &#125; &#125; public static void main(String[] args) &#123; final Test test = new Test(); for(int i=0;i&lt;10;i++)&#123; new Thread()&#123; public void run() &#123; for(int j=0;j&lt;1000;j++) test.increase(); &#125;; &#125;.start(); &#125; while(Thread.activeCount()&gt;1) //保证前面的线程都执行完 Thread.yield(); System.out.println(test.inc); &#125;&#125; 采用AtomicInteger：在java 1.5的java.util.concurrent.atomic包下提供了一些原子操作类，即对基本数据类型的 自增（加1操作），自减（减1操作）、以及加法操作（加一个数），减法操作（减一个数）进行了封装，保证这些操作是原子性操作。atomic是利用CAS来实现原子性操作的（Compare And Swap），CAS实际上是利用处理器提供的CMPXCHG指令实现的，而处理器执行CMPXCHG指令是一个原子性操作。 1234567891011121314151617181920212223public class Test &#123; public AtomicInteger inc = new AtomicInteger(); public void increase() &#123; inc.getAndIncrement(); &#125; public static void main(String[] args) &#123; final Test test = new Test(); for(int i=0;i&lt;10;i++)&#123; new Thread()&#123; public void run() &#123; for(int j=0;j&lt;1000;j++) test.increase(); &#125;; &#125;.start(); &#125; while(Thread.activeCount()&gt;1) //保证前面的线程都执行完 Thread.yield(); System.out.println(test.inc); &#125;&#125; 3.volatile能保证有序性吗？在前面提到volatile关键字能禁止指令重排序，所以volatile能在一定程度上保证有序性。 volatile关键字禁止指令重排序有两层意思： 当程序执行到volatile变量的读操作或者写操作时，在其前面的操作的更改肯定全部已经进行，且结果已经对后面的操作可见；在其后面的操作肯定还没有进行； 在进行指令优化时，不能将在对volatile变量访问的语句放在其后面执行，也不能把volatile变量后面的语句放到其前面执行。 可能上面说的比较绕，举个简单的例子： 12345678//x、y为非volatile变量//flag为volatile变量 x = 2; //语句1y = 0; //语句2flag = true; //语句3x = 4; //语句4y = -1; //语句5 由于flag变量为volatile变量，那么在进行指令重排序的过程的时候，不会将语句3放到语句1、语句2前面，也不会讲语句3放到语句4、语句5后面。 但是要注意语句1和语句2的顺序、语句4和语句5的顺序是不作任何保证的。 并且volatile关键字能保证，执行到语句3时，语句1和语句2必定是执行完毕了的，且语句1和语句2的执行结果对语句3、语句4、语句5是可见的。 那么我们回到前面举的一个例子： 123456789//线程1:context = loadContext(); //语句1inited = true; //语句2 //线程2:while(!inited )&#123; sleep()&#125;doSomethingwithconfig(context); 前面举这个例子的时候，提到有可能语句2会在语句1之前执行，那么久可能导致context还没被初始化，而线程2中就使用未初始化的context去进行操作，导致程序出错。 这里如果用volatile关键字对inited变量进行修饰，就不会出现这种问题了，因为当执行到语句2时，必定能保证context已经初始化完毕。 4.volatile的原理和实现机制前面讲述了源于volatile关键字的一些使用，下面我们来探讨一下volatile到底如何保证可见性和禁止指令重排序的。 下面这段话摘自《深入理解Java虚拟机》： 观察加入volatile关键字和没有加入volatile关键字时所生成的汇编代码发现，加入volatile关键字时，会多出一个lock前缀指令 lock前缀指令实际上相当于一个内存屏障（也成内存栅栏），内存屏障会提供3个功能： 它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成； 它会强制将对缓存的修改操作立即写入主存； 如果是写操作，它会导致其他CPU中对应的缓存行无效。 5.使用volatile关键字的场景synchronized关键字是防止多个线程同时执行一段代码，那么就会很影响程序执行效率，而volatile关键字在某些情况下性能要优于synchronized，但是要注意volatile关键字是无法替代synchronized关键字的，因为volatile关键字无法保证操作的原子性。 通常来说，使用volatile必须具备以下2个条件： 对变量的写操作不依赖于当前值 该变量没有包含在具有其他变量的不变式中 实际上，这些条件表明，可以被写入 volatile 变量的这些有效值独立于任何程序的状态，包括变量的当前状态。 事实上，我的理解就是上面的2个条件需要保证操作是原子性操作，才能保证使用volatile关键字的程序在并发时能够正确执行。 下面列举几个Java中使用volatile的几个场景。 1.状态标记量 123456789volatile boolean flag = false; while(!flag)&#123; doSomething();&#125; public void setFlag() &#123; flag = true;&#125; 12345678910volatile boolean inited = false;//线程1:context = loadContext(); inited = true; //线程2:while(!inited )&#123;sleep()&#125;doSomethingwithconfig(context); 2.double check 1234567891011121314151617class Singleton&#123; private volatile static Singleton instance = null; private Singleton() &#123; &#125; public static Singleton getInstance() &#123; if(instance==null) &#123; synchronized (Singleton.class) &#123; if(instance==null) instance = new Singleton(); &#125; &#125; return instance; &#125;&#125; 参考： Java并发编程：volatile关键字解析Java 中的双重检查（Double-Check） 本文转载自yweihainan博客，原文连接点这里，版权归原作者所有。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/categories/JAVA/"},{"name":"JVM","slug":"JAVA/JVM","permalink":"http://huermosi.xyz/categories/JAVA/JVM/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/tags/JAVA/"},{"name":"JMM","slug":"JMM","permalink":"http://huermosi.xyz/tags/JMM/"},{"name":"VOLATILE","slug":"VOLATILE","permalink":"http://huermosi.xyz/tags/VOLATILE/"},{"name":"内存模型","slug":"内存模型","permalink":"http://huermosi.xyz/tags/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/"},{"name":"可见性","slug":"可见性","permalink":"http://huermosi.xyz/tags/%E5%8F%AF%E8%A7%81%E6%80%A7/"}]},{"title":"Java ExecutorService四种线程池的例子与说明","slug":"java/executorservice-02","date":"2017-06-24T07:21:44.000Z","updated":"2021-06-04T07:34:48.716Z","comments":true,"path":"2017/9527888017/","link":"","permalink":"http://huermosi.xyz/2017/9527888017/","excerpt":"","text":"1. new Thread的弊端执行一个异步任务你还只是如下new Thread吗？ 1234567new Thread(new Runnable() &#123; @Override public void run() &#123; // TODO Auto-generated method stub &#125;&#125;).start(); 那你就out太多了，new Thread的弊端如下： 每次new Thread新建对象性能差。 线程缺乏统一管理，可能无限制新建线程，相互之间竞争，及可能占用过多系统资源导致死机或oom。 缺乏更多功能，如定时执行、定期执行、线程中断。 相比new Thread，Java提供的四种线程池的好处在于： 重用存在的线程，减少对象创建、消亡的开销，性能佳。 可有效控制最大并发线程数，提高系统资源的使用率，同时避免过多资源竞争，避免堵塞。 提供定时执行、定期执行、单线程、并发数控制等功能。 2. Java 线程池Java通过Executors提供四种线程池，分别为： newCachedThreadPool 创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 newFixedThreadPool 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 newScheduledThreadPool 创建一个定长线程池，支持定时及周期性任务执行。 newSingleThreadExecutor 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 2.1 newCachedThreadPool创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。示例代码如下： 1234567891011121314151617ExecutorService cachedThreadPool = Executors.newCachedThreadPool();for (int i = 0; i &lt; 10; i++) &#123; final int index = i; try &#123; Thread.sleep(index * 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; cachedThreadPool.execute(new Runnable() &#123; @Override public void run() &#123; System.out.println(index); &#125; &#125;);&#125; 线程池为无限大，当执行第二个任务时第一个任务已经完成，会复用执行第一个任务的线程，而不用每次新建线程。 2.2 newFixedThreadPool创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。示例代码如下： 123456789101112131415161718ExecutorService fixedThreadPool = Executors.newFixedThreadPool(3);for (int i = 0; i &lt; 10; i++) &#123; final int index = i; fixedThreadPool.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; System.out.println(index); Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125;);&#125; 因为线程池大小为3，每个任务输出index后sleep 2秒，所以每两秒打印3个数字。定长线程池的大小最好根据系统资源进行设置。如Runtime.getRuntime().availableProcessors()。可参考PreloadDataCache。 2.3 newScheduledThreadPool创建一个定长线程池，支持定时及周期性任务执行。延迟执行示例代码如下： 12345678ScheduledExecutorService scheduledThreadPool = Executors.newScheduledThreadPool(5);scheduledThreadPool.schedule(new Runnable() &#123; @Override public void run() &#123; System.out.println(\"delay 3 seconds\"); &#125;&#125;, 3, TimeUnit.SECONDS); 表示延迟3秒执行。 定期执行示例代码如下： 1234567scheduledThreadPool.scheduleAtFixedRate(new Runnable() &#123;@Overridepublic void run() &#123;System.out.println(\"delay 1 seconds, and excute every 3 seconds\");&#125;&#125;, 1, 3, TimeUnit.SECONDS); 表示延迟1秒后每3秒执行一次。ScheduledExecutorService比Timer更安全，功能更强大，后面会有一篇单独进行对比。 2.4 newSingleThreadExecutor创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。示例代码如下： 1234567891011121314151617ExecutorService singleThreadExecutor = Executors.newSingleThreadExecutor();for (int i = 0; i &lt; 10; i++) &#123; final int index = i; singleThreadExecutor.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; System.out.println(index); Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125;);&#125; 结果依次输出，相当于顺序执行各个任务。现行大多数GUI程序都是单线程的。Android中单线程可用于数据库操作，文件操作，应用批量安装，应用批量删除等不适合并发但可能IO阻塞性及影响UI线程响应的操作。 本文转载自博客园头条，原文连接点这里，版权归原作者所有，转载目的是为了避免以后找不到。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/categories/JAVA/"},{"name":"多线程","slug":"JAVA/多线程","permalink":"http://huermosi.xyz/categories/JAVA/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/tags/JAVA/"},{"name":"EXECUTORS","slug":"EXECUTORS","permalink":"http://huermosi.xyz/tags/EXECUTORS/"},{"name":"EXECUTOR","slug":"EXECUTOR","permalink":"http://huermosi.xyz/tags/EXECUTOR/"},{"name":"EXECUTORSERVICE","slug":"EXECUTORSERVICE","permalink":"http://huermosi.xyz/tags/EXECUTORSERVICE/"},{"name":"THREAD","slug":"THREAD","permalink":"http://huermosi.xyz/tags/THREAD/"}]},{"title":"Java并发编程：线程间协作的两种方式：wait、notify、notifyAll和Condition","slug":"java/java-wait-notify-notifyall-condition","date":"2017-04-16T14:16:40.000Z","updated":"2021-04-08T08:49:31.636Z","comments":true,"path":"2017/95270316221640/","link":"","permalink":"http://huermosi.xyz/2017/95270316221640/","excerpt":"","text":"本文转载自Matrix海子博客，原文连接点这里，版权归原作者所有。 在前面我们将了很多关于同步的问题，然而在现实中，需要线程之间的协作。比如说最经典的生产者-消费者模型：当队列满时，生产者需要等待队列有空间才能继续往里面放入商品，而在等待的期间内，生产者必须释放对临界资源（即队列）的占用权。因为生产者如果不释放对临界资源的占用权，那么消费者就无法消费队列中的商品，就不会让队列有空间，那么生产者就会一直无限等待下去。因此，一般情况下，当队列满时，会让生产者交出对临界资源的占用权，并进入挂起状态。然后等待消费者消费了商品，然后消费者通知生产者队列有空间了。同样地，当队列空时，消费者也必须等待，等待生产者通知它队列中有商品了。这种互相通信的过程就是线程间的协作。今天我们就来探讨一下Java中线程协作的最常见的两种方式：利用Object.wait()、Object.notify()和使用Condition。 一、wait()、notify()和notifyAll()wait()、notify()和notifyAll()是Object类中的方法： 12345678910111213141516171819202122232425/** * Wakes up a single thread that is waiting on this object's * monitor. If any threads are waiting on this object, one of them * is chosen to be awakened. The choice is arbitrary and occurs at * the discretion of the implementation. A thread waits on an object's * monitor by calling one of the wait methods */public final native void notify(); /** * Wakes up all threads that are waiting on this object's monitor. A * thread waits on an object's monitor by calling one of the * wait methods. */public final native void notifyAll(); /** * Causes the current thread to wait until either another thread invokes the * &#123;@link java.lang.Object#notify()&#125; method or the * &#123;@link java.lang.Object#notifyAll()&#125; method for this object, or a * specified amount of time has elapsed. * &lt;p&gt; * The current thread must own this object's monitor. */public final native void wait(long timeout) throws InterruptedException; 从这三个方法的文字描述可以知道以下几点信息： wait()、notify()和notifyAll()方法是本地方法，并且为final方法，无法被重写。 调用某个对象的wait()方法能让当前线程阻塞，并且当前线程必须拥有此对象的monitor（即锁） 调用某个对象的notify()方法能够唤醒一个正在等待这个对象的monitor的线程，如果有多个线程都在等待这个对象的monitor，则只能唤醒其中一个线程； 调用notifyAll()方法能够唤醒所有正在等待这个对象的monitor的线程； 有朋友可能会有疑问：为何这三个不是Thread类声明中的方法，而是Object类中声明的方法（当然由于Thread类继承了Object类，所以Thread也可以调用者三个方法）？其实这个问题很简单，由于每个对象都拥有monitor（即锁），所以让当前线程等待某个对象的锁，当然应该通过这个对象来操作了。而不是用当前线程来操作，因为当前线程可能会等待多个线程的锁，如果通过线程来操作，就非常复杂了。 上面已经提到，如果调用某个对象的wait()方法，当前线程必须拥有这个对象的monitor（即锁），因此调用wait()方法必须在同步块或者同步方法中进行（synchronized块或者synchronized方法）。 调用某个对象的wait()方法，相当于让当前线程交出此对象的monitor，然后进入等待状态，等待后续再次获得此对象的锁（Thread类中的sleep方法使当前线程暂停执行一段时间，从而让其他线程有机会继续执行，但它并不释放对象锁）； notify()方法能够唤醒一个正在等待该对象的monitor的线程，当有多个线程都在等待该对象的monitor的话，则只能唤醒其中一个线程，具体唤醒哪个线程则不得而知。 同样地，调用某个对象的notify()方法，当前线程也必须拥有这个对象的monitor，因此调用notify()方法必须在同步块或者同步方法中进行（synchronized块或者synchronized方法）。 nofityAll()方法能够唤醒所有正在等待该对象的monitor的线程，这一点与notify()方法是不同的。 这里要注意一点：notify()和notifyAll()方法只是唤醒等待该对象的monitor的线程，并不决定哪个线程能够获取到monitor。 举个简单的例子：假如有三个线程Thread1、Thread2和Thread3都在等待对象objectA的monitor，此时Thread4拥有对象objectA的monitor，当在Thread4中调用objectA.notify()方法之后，Thread1、Thread2和Thread3只有一个能被唤醒。注意，被唤醒不等于立刻就获取了objectA的monitor。假若在Thread4中调用objectA.notifyAll()方法，则Thread1、Thread2和Thread3三个线程都会被唤醒，至于哪个线程接下来能够获取到objectA的monitor就具体依赖于操作系统的调度了。 上面尤其要注意一点，一个线程被唤醒不代表立即获取了对象的monitor，只有等调用完notify()或者notifyAll()并退出synchronized块，释放对象锁后，其余线程才可获得锁执行。 下面看一个例子就明白了： 1234567891011121314151617181920212223242526272829303132333435363738394041public class Test &#123; public static Object object = new Object(); public static void main(String[] args) &#123; Thread1 thread1 = new Thread1(); Thread2 thread2 = new Thread2(); thread1.start(); try &#123; Thread.sleep(200); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; thread2.start(); &#125; static class Thread1 extends Thread&#123; @Override public void run() &#123; synchronized (object) &#123; try &#123; object.wait(); &#125; catch (InterruptedException e) &#123; &#125; System.out.println(\"线程\"+Thread.currentThread().getName()+\"获取到了锁\"); &#125; &#125; &#125; static class Thread2 extends Thread&#123; @Override public void run() &#123; synchronized (object) &#123; object.notify(); System.out.println(\"线程\"+Thread.currentThread().getName()+\"调用了object.notify()\"); &#125; System.out.println(\"线程\"+Thread.currentThread().getName()+\"释放了锁\"); &#125; &#125;&#125; 无论运行多少次，运行结果必定是： 123线程Thread-1调用了object.notify()线程Thread-1释放了锁线程Thread-0获取到了锁 二、ConditionCondition是在java 1.5中才出现的，它用来替代传统的Object的wait()、notify()实现线程间的协作，相比使用Object的wait()、notify()，使用Condition1的await()、signal()这种方式实现线程间协作更加安全和高效。因此通常来说比较推荐使用Condition，在阻塞队列那一篇博文中就讲述到了，阻塞队列实际上是使用了Condition来模拟线程间协作。 Condition是个接口，基本的方法就是await()和signal()方法； Condition依赖于Lock接口，生成一个Condition的基本代码是lock.newCondition() 调用Condition的await()和signal()方法，都必须在lock保护之内，就是说必须在lock.lock()和lock.unlock之间才可以使用 Conditon中的await()对应Object的wait()； Condition中的signal()对应Object的notify()； Condition中的signalAll()对应Object的notifyAll()。 三、生产者-消费者模型的实现1.使用Object的wait()和notify()实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public class Test &#123; private int queueSize = 10; private PriorityQueue&lt;Integer&gt; queue = new PriorityQueue&lt;Integer&gt;(queueSize); public static void main(String[] args) &#123; Test test = new Test(); Producer producer = test.new Producer(); Consumer consumer = test.new Consumer(); producer.start(); consumer.start(); &#125; class Consumer extends Thread&#123; @Override public void run() &#123; consume(); &#125; private void consume() &#123; while(true)&#123; synchronized (queue) &#123; while(queue.size() == 0)&#123; try &#123; System.out.println(\"队列空，等待数据\"); queue.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); queue.notify(); &#125; &#125; queue.poll(); //每次移走队首元素 queue.notify(); System.out.println(\"从队列取走一个元素，队列剩余\"+queue.size()+\"个元素\"); &#125; &#125; &#125; &#125; class Producer extends Thread&#123; @Override public void run() &#123; produce(); &#125; private void produce() &#123; while(true)&#123; synchronized (queue) &#123; while(queue.size() == queueSize)&#123; try &#123; System.out.println(\"队列满，等待有空余空间\"); queue.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); queue.notify(); &#125; &#125; queue.offer(1); //每次插入一个元素 queue.notify(); System.out.println(\"向队列取中插入一个元素，队列剩余空间：\"+(queueSize-queue.size())); &#125; &#125; &#125; &#125;&#125; 2.使用Condition实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public class Test &#123; private int queueSize = 10; private PriorityQueue&lt;Integer&gt; queue = new PriorityQueue&lt;Integer&gt;(queueSize); private Lock lock = new ReentrantLock(); private Condition notFull = lock.newCondition(); private Condition notEmpty = lock.newCondition(); public static void main(String[] args) &#123; Test test = new Test(); Producer producer = test.new Producer(); Consumer consumer = test.new Consumer(); producer.start(); consumer.start(); &#125; class Consumer extends Thread&#123; @Override public void run() &#123; consume(); &#125; private void consume() &#123; while(true)&#123; lock.lock(); try &#123; while(queue.size() == 0)&#123; try &#123; System.out.println(\"队列空，等待数据\"); notEmpty.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; queue.poll(); //每次移走队首元素 notFull.signal(); System.out.println(\"从队列取走一个元素，队列剩余\"+queue.size()+\"个元素\"); &#125; finally&#123; lock.unlock(); &#125; &#125; &#125; &#125; class Producer extends Thread&#123; @Override public void run() &#123; produce(); &#125; private void produce() &#123; while(true)&#123; lock.lock(); try &#123; while(queue.size() == queueSize)&#123; try &#123; System.out.println(\"队列满，等待有空余空间\"); notFull.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; queue.offer(1); //每次插入一个元素 notEmpty.signal(); System.out.println(\"向队列取中插入一个元素，队列剩余空间：\"+(queueSize-queue.size())); &#125; finally&#123; lock.unlock(); &#125; &#125; &#125; &#125;&#125; 参考资料： 《Java编程思想》 http://blog.csdn.net/ns_code/article/details/17225469 http://blog.csdn.net/ghsau/article/details/7481142","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/categories/JAVA/"},{"name":"多线程","slug":"JAVA/多线程","permalink":"http://huermosi.xyz/categories/JAVA/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://huermosi.xyz/tags/JAVA/"},{"name":"并发编程","slug":"并发编程","permalink":"http://huermosi.xyz/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"wait","slug":"wait","permalink":"http://huermosi.xyz/tags/wait/"},{"name":"notify","slug":"notify","permalink":"http://huermosi.xyz/tags/notify/"}]},{"title":"Linux下TCP连接过程总结","slug":"tcp/linux-tcp-connect-procedure","date":"2017-03-14T12:22:19.000Z","updated":"2021-04-26T12:38:34.437Z","comments":true,"path":"2017/95270314202219/","link":"","permalink":"http://huermosi.xyz/2017/95270314202219/","excerpt":"","text":"一、Linux服务器上11种网络连接状态: 通常情况下，一个正常的TCP连接，都会有三个阶段:1、TCP三次握手; 2、数据传送; 3、TCP四次挥手 注:以下说明最好能结合”图:TCP的状态机”来理解。 SYN: (同步序列编号,Synchronize Sequence Numbers)该标志仅在三次握手建立TCP连接时有效。表示一个新的TCP连接请求。 ACK: (确认编号,Acknowledgement Number)是对TCP请求的确认标志,同时提示对端系统已经成功接收所有数据。 FIN: (结束标志,FINish)用来结束一个TCP回话.但对应端口仍处于开放状态,准备接收后续数据。 1)、LISTEN:首先服务端需要打开一个socket进行监听，状态为LISTEN. /* The socket is listening for incoming connections. 侦听来自远方TCP端口的连接请求 /2)、SYN_SENT:客户端通过应用程序调用connect进行active open.于是客户端tcp发送一个SYN以请求建立一个连接.之后状态置为SYN_SENT. /The socket is actively attempting to establish a connection. 在发送连接请求后等待匹配的连接请求 */ 3)、SYN_RECV:服务端应发出ACK确认客户端的SYN,同时自己向客户端发送一个SYN. 之后状态置为SYN_RECV /* A connection request has been received from the network. 在收到和发送一个连接请求后等待对连接请求的确认 */ 4)、ESTABLISHED: 代表一个打开的连接，双方可以进行或已经在数据交互了。/* The socket has an established connection. 代表一个打开的连接，数据可以传送给用户 */ 5)、FIN_WAIT1:主动关闭(active close)端应用程序调用close，于是其TCP发出FIN请求主动关闭连接，之后进入FIN_WAIT1状态./* The socket is closed, and the connection is shutting down. 等待远程TCP的连接中断请求，或先前的连接中断请求的确认 */ 6)、CLOSE_WAIT:被动关闭(passive close)端TCP接到FIN后，就发出ACK以回应FIN请求(它的接收也作为文件结束符传递给上层应用程序),并进入CLOSE_WAIT. /* The remote end has shut down, waiting for the socket to close. 等待从本地用户发来的连接中断请求 */ 7)、FIN_WAIT2:主动关闭端接到ACK后，就进入了FIN-WAIT-2 ./* Connection is closed, and the socket is waiting for a shutdown from the remote end. 从远程TCP等待连接中断请求 */ 8)、LAST_ACK:被动关闭端一段时间后，接收到文件结束符的应用程序将调用CLOSE关闭连接。这导致它的TCP也发送一个 FIN,等待对方的ACK.就进入了LAST-ACK . /* The remote end has shut down, and the socket is closed. Waiting for acknowledgement. 等待原来发向远程TCP的连接中断请求的确认 */ 9)、TIME_WAIT:在主动关闭端接收到FIN后，TCP就发送ACK包，并进入TIME-WAIT状态。/* The socket is waiting after close to handle packets still in the network.等待足够的时间以确保远程TCP接收到连接中断请求的确认 */ 10)、CLOSING: 比较少见./* Both sockets are shut down but we still don’t have all our data sent. 等待远程TCP对连接中断的确认 */ 11)、CLOSED: 被动关闭端在接受到ACK包后，就进入了closed的状态。连接结束./* The socket is not being used. 没有任何连接状态 */ TIME_WAIT状态的形成只发生在主动关闭连接的一方。主动关闭方在接收到被动关闭方的FIN请求后，发送成功给对方一个ACK后,将自己的状态由FIN_WAIT2修改为TIME_WAIT，而必须再等2 倍 的MSL(Maximum Segment Lifetime,MSL是一个数据报在internetwork中能存在的时间)时间之后双方才能把状态都改为CLOSED以关闭连接。目前RHEL里保持TIME_WAIT状态的时间为60秒。当然上述很多TCP状态在系统里都有对应的解释或设置,可见 man tcp 二、关于长连接和短连接:通俗点讲，短连接就是一次TCP请求得到结果后,连接马上结束.而长连接并不马上断开,而一直保持着,直到长连接TIMEOUT(具体程序都有相关参数说明).长连接可以避免不断的进行TCP三次握手和四次挥手。长连接(keepalive)是需要靠双方不断的发送探测包来维持的,keepalive期间服务端和客户端的TCP连接状态是ESTABLISHED.目 前http1.1版本里默认都是keepalive(1.0版本默认是不keepalive的)，ie6/7/8和firefox都默认用的是http 1.1版本了(如何查看当前浏览器用的是哪个版本，这里不再赘述)。一个应用至于到底是该使用短连接还是长连接，应该视具体情况而定。一般的应用应该使用长连接。 TCP四次挥手TCP协议有一个优雅的关闭（graceful close）机制，以保证应用程序在关闭连接时不必担心正在传输的数据会丢失。如第4.5节的压缩示例程序所示，这个机制还设计为允许两个方向的数据传输 相互独立地终止。关闭机制的工作流程是：应用程序通过调用连接套接字的close()方法或shutdownOutput()方法表明数据已经发送完毕。 此刻，底层的TCP实现首先将留存在SendQ队列中的数据传输出去（还要依赖于另一端RecvQ队列的剩余空间），然后向另一端发送一个关闭TCP连接 的握手消息。该关闭握手消息可以看作是流终止标志：它告诉接收端TCP不会再有新的数据传入RecvQ队列了。（注意，关闭握手消息本身并没有传递给接收 端应用程序，而是通过read()方法返回-1来指示其在字节流中的位置。）正在关闭的TCP将等待其关闭握手消息的确认信息，该确认信息表明在连接上传 输的所有数据已经安全地传输到了RecvQ中。只要收到了确认消息，该连接就变成”半关闭（Half closed）”状态。直到连接的另一个方向上收到了对称的握手消息后，连接才完全关闭–也就是说，连接的两端都表明它们再没有数据要发送了。 TCP连接的关闭事件序列可能以两种方式发生：一种方式是先由一个应用程序调用close()方法（或shutdownOutput()方法），并在另 一端调用close()方法之前完成其关闭握手消息；另一种方式是两端同时调用close()方法，它们的关闭握手消息在网络上交叉传输。图6.10展示 了以第一种方式关闭连接时，底层实现中的事件序列。关闭握手消息已经发送，套接字数据结构的状态也已经设置为”Closing”（专业术语称 为”FIN_WAIT_1”），然后close()调用返回。完成这些工作后，将禁止在该Socket上的任何读写操作（会抛出异常）。当收到关闭握手确 认消息后，套接字数据结构的状态则改变为”半关闭”（专业术语称为”FIN_WAIT_2”），这种状态将一直持续，直到接收到另一端的关闭握手消息 关闭TCP连接的最后微妙之处在于对Time-Wait状态的需要。TCP规范要求在终止连接时，两端的关闭握手都完成后，至少要有一个套接字在 Time-Wait状态保持一段时间。这个要求的提出是由于消息在网络中传输时可能延迟。如果在连接两端都完成了关闭握手后，它们都移除了其底层数据结 构，而此时在同样一对套接字地址之间又立即建立了新的连接，那么前一个连接在网络上传输时延迟的消息就可能在新连接建立后到达。由于其包含了相同的源地址 和目的地址，旧消息就会被错误地认为是属于新连接的，其包含的数据就可能被错误地分配到应用程序中。 虽然这种情形可能很少发生，TCP还是使用了包括Time-Wait状态在内的多种机制对其进行防范。Time-Wait状态用于保证每个TCP连接都 在一段平静时间内结束，这期间不会有数据发送。平静时间的长度应该等于分组报文在网络上存留的最长时间的两倍。因此，当一个连接完全结束（即套接字数据结 构离开Time-Wait状态并被删除），并为同样一对地址上的新连接清理道路后，就不会再有旧实例发送的消息还存留在网络中。实际上，平静时间的长度要 依赖于具体实现，因为没有机制能真正限制分组报文在网络上能够延迟的时间。通常使用的时间范围是4分钟减到30秒，或更短。 Time-Wait状态最重要的作用是，只要底层套接字数据结构还存在，就不允许在相同的本地端口上关联其他套接字。尤其是试图使用该端口创建新的Socket实例时，将抛出IOException异常。 TCP三次握手/四次挥手详解1、建立连接协议（三次握手）（1）客户端发送一个带SYN标志的TCP报文到服务器。这是三次握手过程中的报文1。（2） 服务器端回应客户端的，这是三次握手中的第2个报文，这个报文同时带ACK标志和SYN标志。因此它表示对刚才客户端SYN报文的回应；同时又标志SYN给客户端，询问客户端是否准备好进行数据通讯。（3） 客户必须再次回应服务段一个ACK报文，这是报文段3。 2、连接终止协议（四次挥手） 由于TCP连接是全双工的，因此每个方向都必须单独进行关闭。这原则是当一方完成它的数据发送任务后就能发送一个FIN来终止这个方向的连接。收到一个 FIN只意味着这一方向上没有数据流动，一个TCP连接在收到一个FIN后仍能发送数据。首先进行关闭的一方将执行主动关闭，而另一方执行被动关闭。 （1） TCP客户端发送一个FIN，用来关闭客户到服务器的数据传送（报文段4）。 （2） 服务器收到这个FIN，它发回一个ACK，确认序号为收到的序号加1（报文段5）。和SYN一样，一个FIN将占用一个序号。 （3） 服务器关闭客户端的连接，发送一个FIN给客户端（报文段6）。 （4） 客户段发回ACK报文确认，并将确认序号设置为收到序号加1（报文段7）。 CLOSED: 这个没什么好说的了，表示初始状态。 LISTEN: 这个也是非常容易理解的一个状态，表示服务器端的某个SOCKET处于监听状态，可以接受连接了。 SYN_RCVD: 这个状态表示接受到了SYN报文，在正常情况下，这个状态是服务器端的SOCKET在建立TCP连接时的三次握手会话过程中的一个中间状态，很短暂，基本 上用netstat你是很难看到这种状态的，除非你特意写了一个客户端测试程序，故意将三次TCP握手过程中最后一个ACK报文不予发送。因此这种状态 时，当收到客户端的ACK报文后，它会进入到ESTABLISHED状态。 SYN_SENT: 这个状态与SYN_RCVD遥想呼应，当客户端SOCKET执行CONNECT连接时，它首先发送SYN报文，因此也随即它会进入到了SYN_SENT状 态，并等待服务端的发送三次握手中的第2个报文。SYN_SENT状态表示客户端已发送SYN报文。 ESTABLISHED：这个容易理解了，表示连接已经建立了。 FIN_WAIT_1: 这个状态要好好解释一下，其实FIN_WAIT_1和FIN_WAIT_2状态的真正含义都是表示等待对方的FIN报文。而这两种状态的区别 是：FIN_WAIT_1状态实际上是当SOCKET在ESTABLISHED状态时，它想主动关闭连接，向对方发送了FIN报文，此时该SOCKET即 进入到FIN_WAIT_1状态。而当对方回应ACK报文后，则进入到FIN_WAIT_2状态，当然在实际的正常情况下，无论对方何种情况下，都应该马 上回应ACK报文，所以FIN_WAIT_1状态一般是比较难见到的，而FIN_WAIT_2状态还有时常常可以用netstat看到。 FIN_WAIT_2：上面已经详细解释了这种状态，实际上FIN_WAIT_2状态下的SOCKET，表示半连接，也即有一方要求close连接，但另外还告诉对方，我暂时还有点数据需要传送给你，稍后再关闭连接。 TIME_WAIT: 表示收到了对方的FIN报文，并发送出了ACK报文，就等2MSL后即可回到CLOSED可用状态了。如果FIN_WAIT_1状态下，收到了对方同时带 FIN标志和ACK标志的报文时，可以直接进入到TIME_WAIT状态，而无须经过FIN_WAIT_2状态。 CLOSING: 这种状态比较特殊，实际情况中应该是很少见，属于一种比较罕见的例外状态。正常情况下，当你发送FIN报文后，按理来说是应该先收到（或同时收到）对方的 ACK报文，再收到对方的FIN报文。但是CLOSING状态表示你发送FIN报文后，并没有收到对方的ACK报文，反而却也收到了对方的FIN报文。什 么情况下会出现此种情况呢？其实细想一下，也不难得出结论：那就是如果双方几乎在同时close一个SOCKET的话，那么就出现了双方同时发送FIN报 文的情况，也即会出现CLOSING状态，表示双方都正在关闭SOCKET连接。 CLOSE_WAIT: 这种状态的含义其实是表示在等待关闭。怎么理解呢？当对方close一个SOCKET后发送FIN报文给自己，你系统毫无疑问地会回应一个ACK报文给对 方，此时则进入到CLOSE_WAIT状态。接下来呢，实际上你真正需要考虑的事情是察看你是否还有数据发送给对方，如果没有的话，那么你也就可以 close这个SOCKET，发送FIN报文给对方，也即关闭连接。所以你在CLOSE_WAIT状态下，需要完成的事情是等待你去关闭连接。 LAST_ACK: 这个状态还是比较容易好理解的，它是被动关闭一方在发送FIN报文后，最后等待对方的ACK报文。当收到ACK报文后，也即可以进入到CLOSED可用状态了。 最后有2个问题的回答，我自己分析后的结论（不一定保证100%正确）: 1、 为什么建立连接协议是三次握手，而关闭连接却是四次握手呢？这 是因为服务端的LISTEN状态下的SOCKET当收到SYN报文的建连请求后，它可以把ACK和SYN（ACK起应答作用，而SYN起同步作用）放在一 个报文里来发送。但关闭连接时，当收到对方的FIN报文通知时，它仅仅表示对方没有数据发送给你了；但未必你所有的数据都全部发送给对方了，所以你可以未 必会马上会关闭SOCKET,也即你可能还需要发送一些数据给对方之后，再发送FIN报文给对方来表示你同意现在可以关闭连接了，所以它这里的ACK报文 和FIN报文多数情况下都是分开发送的。 2、 为什么TIME_WAIT状态还需要等2MSL后才能返回到CLOSED状态？这 是因为：虽然双方都同意关闭连接了，而且握手的4个报文也都协调和发送完毕，按理可以直接回到CLOSED状态（就好比从SYN_SEND状态到 ESTABLISH状态那样）；但是因为我们必须要假想网络是不可靠的，你无法保证你最后发送的ACK报文会一定被对方收到，因此对方处于 LAST_ACK状态下的SOCKET可能会因为超时未收到ACK报文，而重发FIN报文，所以这个TIME_WAIT状态的作用就是用来重发可能丢失的 ACK报文。 本文转载自猿人谷博客，原文连接点这里，版权归原作者所有。微信公众号：猿人谷如果您认为阅读这篇博客让您有些收获，不妨点击一下右下角的【推荐】如果您希望与我交流互动，欢迎关注微信公众号本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文连接。","categories":[{"name":"更多","slug":"更多","permalink":"http://huermosi.xyz/categories/%E6%9B%B4%E5%A4%9A/"},{"name":"TCP","slug":"更多/TCP","permalink":"http://huermosi.xyz/categories/%E6%9B%B4%E5%A4%9A/TCP/"}],"tags":[{"name":"TCP","slug":"TCP","permalink":"http://huermosi.xyz/tags/TCP/"}]},{"title":"长连接和短连接分析","slug":"tcp/long-short-connect","date":"2017-03-09T13:03:51.000Z","updated":"2021-04-26T12:38:41.237Z","comments":true,"path":"2017/95270314210351/","link":"","permalink":"http://huermosi.xyz/2017/95270314210351/","excerpt":"","text":"1. TCP连接当网络通信时采用TCP协议时，在真正的读写操作之前，server与client之间必须建立一个连接，当读写操作完成后，双方不再需要这个连接 时它们可以释放这个连接，连接的建立是需要三次握手的，而释放则需要4次握手，所以说每个连接的建立都是需要资源消耗和时间消耗的 经典的三次握手示意图： 经典的四次挥手关闭图： 2. TCP短连接我们模拟一下TCP短连接的情况，client向server发起连接请求，server接到请求，然后双方建立连接。client向server 发送消息，server回应client，然后一次读写就完成了，这时候双方任何一个都可以发起close操作，不过一般都是client先发起 close操作。为什么呢，一般的server不会回复完client后立即关闭连接的，当然不排除有特殊的情况。从上面的描述看，短连接一般只会在 client/server间传递一次读写操作 短连接的优点是：管理起来比较简单，存在的连接都是有用的连接，不需要额外的控制手段 3.TCP长连接接下来我们再模拟一下长连接的情况，client向server发起连接，server接受client连接，双方建立连接。Client与server完成一次读写之后，它们之间的连接并不会主动关闭，后续的读写操作会继续使用这个连接。 首先说一下TCP/IP详解上讲到的TCP保活功能，保活功能主要为服务器应用提供，服务器应用希望知道客户主机是否崩溃，从而可以代表客户使用资 源。如果客户已经消失，使得服务器上保留一个半开放的连接，而服务器又在等待来自客户端的数据，则服务器将应远等待客户端的数据，保活功能就是试图在服务 器端检测到这种半开放的连接。 如果一个给定的连接在两小时内没有任何的动作，则服务器就向客户发一个探测报文段，客户主机必须处于以下4个状态之一： 客户主机依然正常运行，并从服务器可达。客户的TCP响应正常，而服务器也知道对方是正常的，服务器在两小时后将保活定时器复位。 客户主机已经崩溃，并且关闭或者正在重新启动。在任何一种情况下，客户的TCP都没有响应。服务端将不能收到对探测的响应，并在75秒后超时。服务器总共发送10个这样的探测 ，每个间隔75秒。如果服务器没有收到一个响应，它就认为客户主机已经关闭并终止连接。 客户主机崩溃并已经重新启动。服务器将收到一个对其保活探测的响应，这个响应是一个复位，使得服务器终止这个连接。 客户机正常运行，但是服务器不可达，这种情况与2类似，TCP能发现的就是没有收到探查的响应。 从上面可以看出，TCP保活功能主要为探测长连接的存活状况，不过这里存在一个问题，存活功能的探测周期太长，还有就是它只是探测TCP连接的存活，属于比较斯文的做法，遇到恶意的连接时，保活功能就不够使了。 在长连接的应用场景下，client端一般不会主动关闭它们之间的连接，Client与server之间的连接如果一直不关闭的话，会存在一个问 题，随着客户端连接越来越多，server早晚有扛不住的时候，这时候server端需要采取一些策略，如关闭一些长时间没有读写事件发生的连接，这样可 以避免一些恶意连接导致server端服务受损；如果条件再允许就可以以客户端机器为颗粒度，限制每个客户端的最大长连接数，这样可以完全避免某个蛋疼的 客户端连累后端服务。 长连接和短连接的产生在于client和server采取的关闭策略，具体的应用场景采用具体的策略，没有十全十美的选择，只有合适的选择。 参考： TCP/IP详解 卷一来源：http://www.cnblogs.com/beifei/archive/2011/06/26/2090611.html 什么是“长连接”和“短连接”？ 解释1所谓长连接指建立SOCKET连接后不管是否使用都保持连接，但安全性较差；所谓短连接指建立SOCKET连接后发送后接收完数据后马上断开连接，一般银行都使用短连接 解释2长连接就是指在基于tcp的通讯中，一直保持连接，不管当前是否发送或者接收数据。而短连接就是只有在有数据传输的时候才进行连接，客户－服务器通信/传输数据完毕就关闭连接。 解释3长连接和短连接这个概念好像只有移动的CMPP协议中提到了，其他的地方没有看到过。通信方式各网元之间共有两种连接方式：长连接和短连接。所谓长连接，指在一个TCP连接上可以连续发送多个数据包，在TCP连接保持期间，如果没有数据包发送，需要双方发检测包以维持此连接。短连接是指通信双方有数据交互时，就建立一个TCP连接，数据发送完成后，则断开此TCP连接，即每次TCP连接只完成一对 CMPP消息的发送。现阶段，要求ISMG之间必须采用长连接的通信方式，建议SP与ISMG之间采用长连接的通信方式。 解释4短连接：比如http的，只是连接、请求、关闭，过程时间较短,服务器若是一段时间内没有收到请求即可关闭连接。长连接：有些服务需要长时间连接到服务器，比如CMPP，一般需要自己做在线维持。 HTTP协议之长、短连接一、长连接与短连接：长连接：client方与server方先建立连接，连接建立后不断开，然后再进行报文发送和接收。这种方式下由于通讯连接一直存在。此种方式常用于P2P通信。短连接：Client方与server每进行一次报文收发交易时才进行通讯连接，交易完毕后立即断开连接。此方式常用于一点对多点通讯。C/S通信。 二、长连接与短连接的操作过程：短连接的操作步骤是：建立连接——数据传输——关闭连接…建立连接——数据传输——关闭连接长连接的操作步骤是：建立连接——数据传输…（保持连接）…数据传输——关闭连接 三、长连接与短连接的使用时机：长连接：长连接多用于操作频繁，点对点的通讯，而且连接数不能太多的情况。每个TCP连接的建立都需要三次握手，每个TCP连接的断开要四次握手。如果每次操作都要建立连接然后再操作的话处理速度会降低，所以每次操作后，下次操作时直接发送数据就可以了，不用再建立TCP连接。例如：数据库的连接用长连接，如果用短连接频繁的通信会造成socket错误，频繁的socket创建也是对资源的浪费。短连接：web网站的http服务一般都用短连接。因为长连接对于服务器来说要耗费一定的资源。像web网站这么频繁的成千上万甚至上亿客户端的连接用短连接更省一些资源。试想如果都用长连接，而且同时用成千上万的用户，每个用户都占有一个连接的话，可想而知服务器的压力有多大。所以并发量大，但是每个用户又不需频繁操作的情况下需要短连接。总之：长连接和短连接的选择要根据需求而定。 四、发送接收方式：1、异步：报文发送和接收是分开的，相互独立，互不影响的。这种方式又分两种情况：异步双工：接收和发送在同一个程序中，有两个不同的子进程分别负责发送和接送。异步单工：接送和发送使用两个不同的程序来完成。2、同步：报文发送和接收是同步进行，即报文发送后等待接送返回报文。同步方式一般需要考虑超时问题，试想我们发送报文以后也不能无限等待啊，所以我们要设定一个等待时候。超过等待时间发送方不再等待读返回报文。直接通知超时返回。 五、报文格式：通信报文格式多样性更多，相应地就必须设计对应的读写报文的接收和发送报文函数。 阻塞与非阻塞方式1、非阻塞方式：读函数不停的进行读动作，如果没有报文接收到，等待一段时间后超时返回，这种情况一般需要指定超时时间。2、阻塞方式：如果没有接收到报文，则读函数一直处于等待状态，知道报文到达。 循环读写方式1、一次直接读写报文：在一次接收或发送报文动作中一次性不加分别地全部读取或全部发送报文字节。2、不指定长度循环读写：这一版发生在短连接进程中，受网络路由等限制，一次较长的报文可能在网络传输过程中被分解成很多个包，一次读取可能不能全部读完一次报文，这就需要循环读取报文，直到读完为止。3、带长度报文头循环读写：这种情况一般在长连接中，由于在长连接中没有条件能够判断循环读写什么时候结束。必须要加长度报文头。读函数先是读取报文头的长度，再根据这个长度去读报文，实际情况中，报头码制格式还经常不一样，如果是非ASCII的报文头，还必须转换成ASCII常见的报文头编制有：1、n个字节的ASCII码。2、n个字节的BCD码。3、n个字节的网络整型码。 以上是几种比较典型的读写报文方式，可以与通信方式模板一起预先提供一些典型的API读写函数。当然在实际问题中，可能还必须编写与对方报文格式配套的读写API. 在实际情况中，往往需要把我们自己的系统与别人的系统进行连接， 有了以上模板与API,可以说连接任何方式的通信程序都不存在问题。 来源：http://www.360doc.com/content/14/0412/16/16726605_368309628.shtml 什么时候用长连接，短连接？ 长连接多用于操作频繁，点对点的通讯，而且连接数不能太多情况，。每个TCP连接都需要三步握手，这需要时间，如果每个操作都是先连接，再操作的话那么处理速度会降低很多，所以每个操作完后都不断开，次处理时直接发送数据包就OK了，不用建立TCP连接。例如：数据库的连接用长连接， 如果用短连接频繁的通信会造成socket错误，而且频繁的socket 创建也是对资源的浪费。 而像WEB网站的http服务一般都用短链接，因为长连接对于服务端来说会耗费一定的资源，而像WEB网站这么频繁的成千上万甚至上亿客户端的 连接用短连接会更省一些资源，如果用长连接，而且同时有成千上万的用户，如果每个用户都占用一个连接的话，那可想而知吧。所以并发量大，但每个用户无需频 繁操作情况下需用短连好。总之，长连接和短连接的选择要视情况而定。 公司的服务器端使用的是resin做中间件，通过客户端每隔几秒发送请求来进行互动。这种就应该是短连接了吧？短连接需要频繁的建立与断开连接，是不是对服务器的资源浪费很大？如果换成长连接呢？长连接的缺点在哪里？ 回答： 从网络技术层面来说：TCP本身是长连接的。当然从业务层面来说：每次连接只处理一笔请求的可以称为短连接；处理业务后不断开连接而是等待处理下一笔可以称为长连接。至于实际场合究竟需要使用短连接还是长连接，主要看实时性要求、数据流向 和 并发量 这三个问题。由于你没有说明请求关于这三个问题上的特点，所以没法给你具体建议。长连接优点：节约TCP握手时间，可以保证高实时性，数据流向可以采用服务器端的主动推模式。长连接缺点：并发量不宜太高，持续占用服务端口（相对消耗资源）。 我有一个基于长连接推模型的聊天室的简单样例，你可以看看：http://blog.csdn.net/ldh911/article/details/7268879 1.现在游戏中的玩家与玩家之间的聊天无法实现实时性，而且系统有邮件或信息时也不能及时的通知玩家—— 如果涉及到聊天的话，一般来说还是用长连接会更合适，否则大量时间浪费到握手上了；—— 但是手机的网络长连接网络质量可能会比较撮，你需要严重考虑容错和重链机制。 2.客户端每隔几秒就会发送一个请求，这样服务器的压力岂不是很大？—— 压力会比较大，关键是聊天往往对时间的要求很高，如果是团战的话，1秒内没看到信息，可能就会觉得完全受不了了；当然也看你聊天的场景如何，是群聊还是单聊，以后会不会发展为语音啥的； NIO没有任何问题，大规模长连接处理的主流都是用NIO；而且也不是Java发明的，本身就是借助了操作系统的网络管理能力。 本文转载自猿人谷博客，原文连接点这里，版权归原作者所有。微信公众号：猿人谷如果您认为阅读这篇博客让您有些收获，不妨点击一下右下角的【推荐】如果您希望与我交流互动，欢迎关注微信公众号本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文连接。","categories":[{"name":"更多","slug":"更多","permalink":"http://huermosi.xyz/categories/%E6%9B%B4%E5%A4%9A/"},{"name":"TCP","slug":"更多/TCP","permalink":"http://huermosi.xyz/categories/%E6%9B%B4%E5%A4%9A/TCP/"}],"tags":[{"name":"TCP","slug":"TCP","permalink":"http://huermosi.xyz/tags/TCP/"},{"name":"长连接","slug":"长连接","permalink":"http://huermosi.xyz/tags/%E9%95%BF%E8%BF%9E%E6%8E%A5/"},{"name":"短连接","slug":"短连接","permalink":"http://huermosi.xyz/tags/%E7%9F%AD%E8%BF%9E%E6%8E%A5/"}]},{"title":"TCP第三次握手失败怎么办","slug":"tcp/tcp-handshake-failed","date":"2017-03-07T13:32:48.000Z","updated":"2021-04-26T12:38:48.720Z","comments":true,"path":"2017/95270314213248/","link":"","permalink":"http://huermosi.xyz/2017/95270314213248/","excerpt":"","text":"笔试题中经常会遇到这个问题：如果tcp建立连接时第三次握手失败，tcp会做何操作？该问题的本质是判断我们对tcp的状态转换是否能有比较深刻的理解。只要理解了下面的状态转换图，很容易回答上述问题。 在此，将《TCP/IP协议族》中每一个状态的转换伪代码整理下： 第58行指明了当第三次握手失败时的处理操作，可以看出当失败时服务器并不会重传ack报文，而是直接发送RST报文段，进入CLOSED状态。这样做的目的是为了防止SYN洪泛攻击。 本文转载自猿人谷博客，原文连接点这里，版权归原作者所有。微信公众号：猿人谷如果您认为阅读这篇博客让您有些收获，不妨点击一下右下角的【推荐】如果您希望与我交流互动，欢迎关注微信公众号本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文连接。","categories":[{"name":"更多","slug":"更多","permalink":"http://huermosi.xyz/categories/%E6%9B%B4%E5%A4%9A/"},{"name":"TCP","slug":"更多/TCP","permalink":"http://huermosi.xyz/categories/%E6%9B%B4%E5%A4%9A/TCP/"}],"tags":[{"name":"TCP","slug":"TCP","permalink":"http://huermosi.xyz/tags/TCP/"},{"name":"三次握手","slug":"三次握手","permalink":"http://huermosi.xyz/tags/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B/"}]},{"title":"TCP协议的三次握手过程","slug":"tcp/tcpip-three-handshake-flow","date":"2017-03-06T12:11:46.000Z","updated":"2021-04-26T12:38:55.416Z","comments":true,"path":"2017/95270314201146/","link":"","permalink":"http://huermosi.xyz/2017/95270314201146/","excerpt":"","text":"TCP(Transmission Control Protocol) 传输控制协议 TCP的连接建立过程又称为TCP三次握手。 首先发送方主机向接收方主机发起一个建立连接的同步（SYN）请求； 接收方主机在收到这个请求后向发送方主机回复一个同步/确认（SYN/ACK）应答； 发送方主机收到此包后再向接收方主机发送一个确认（ACK）。 TCP是主机对主机层的传输控制协议，提供可靠的连接服务，采用三次握手确认建立一个连接: 位码即tcp标志位,有6种标示:SYN(synchronous建立联机) ACK(acknowledgement 确认) PSH(push传送) FIN(finish结束) RST(reset重置) URG(urgent紧急) Sequence number(顺序号码) Acknowledge number(确认号码) 第一次握手：主机A发送位码为syn＝1,随机产生seq number=1234567的数据包到服务器，主机B由SYN=1知道，A要求建立联机； 第二次握手：主机B收到请求后要确认联机信息，向A发送ack number=(主机A的seq+1),syn=1,ack=1,随机产生seq=7654321的包 第三次握手：主机A收到后检查ack number是否正确，即第一次发送的seq number+1,以及位码ack是否为1，若正确，主机A会再发送ack number=(主机B的seq+1),ack=1，主机B收到后确认seq值与ack=1则连接建立成功。 完成三次握手，主机A与主机B开始传送数据。 在TCP/IP协议中，TCP协议提供可靠的连接服务，采用三次握手建立一个连接。第一次握手：建立连接时，客户端发送syn包(syn=j)到服务器，并进入SYN_SEND状态，等待服务器确认；第二次握手：服务器收到syn包，必须确认客户的SYN（ack=j+1），同时自己也发送一个SYN包（syn=k），即SYN+ACK包，此时服务器 进入SYN_RECV状态； 第三次握手：客户端收到服务器的SYN＋ACK包，向服务器发送确认包ACK(ack=k+1)，此包发送完毕，客户端和服务器进入 ESTABLISHED状态，完成三次握手。 完成三次握手，客户端与服务器开始传送数据. 实例: 123IP 192.168.1.116.3337 &gt; 192.168.1.123.7788: S 3626544836:3626544836IP 192.168.1.123.7788 &gt; 192.168.1.116.3337: S 1739326486:1739326486 ack 3626544837IP 192.168.1.116.3337 &gt; 192.168.1.123.7788: ack 1739326487,ack 1 第一次握手：192.168.1.116发送位码syn＝1,随机产生seq number=3626544836的数据包到192.168.1.123,192.168.1.123由SYN=1知道192.168.1.116要求建立联机; 第二次握手：192.168.1.123收到请求后要确认联机信息，向192.168.1.116发送ack number=3626544837,syn=1,ack=1,随机产生seq=1739326486的包; 第三次握手：192.168.1.116收到后检查ack number是否正确，即第一次发送的seq number+1,以及位码ack是否为1，若正确，192.168.1.116会再发送ack number=1739326487,ack=1，192.168.1.123收到后确认seq=seq+1,ack=1则连接建立成功。 图解：一个三次握手的过程（图1，图2） 第一次握手的标志位（图3）我们可以看到标志位里面只有个同步位，也就是在做请求(SYN) 第二次握手的标志位（图4）我们可以看到标志位里面有个确认位和同步位，也就是在做应答(SYN + ACK) 第三次握手的标志位（图5）我们可以看到标志位里面只有个确认位，也就是再做再次确认(ACK) 一个完整的三次握手也就是 请求—应答—再次确认 本文转载自猿人谷博客，原文连接点这里，版权归原作者所有。微信公众号：猿人谷如果您认为阅读这篇博客让您有些收获，不妨点击一下右下角的【推荐】如果您希望与我交流互动，欢迎关注微信公众号本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文连接。","categories":[{"name":"更多","slug":"更多","permalink":"http://huermosi.xyz/categories/%E6%9B%B4%E5%A4%9A/"},{"name":"TCP","slug":"更多/TCP","permalink":"http://huermosi.xyz/categories/%E6%9B%B4%E5%A4%9A/TCP/"}],"tags":[{"name":"TCP","slug":"TCP","permalink":"http://huermosi.xyz/tags/TCP/"},{"name":"三次握手","slug":"三次握手","permalink":"http://huermosi.xyz/tags/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B/"}]},{"title":"面试官，不要再问我三次握手和四次挥手","slug":"tcp/tcpip-three-handshakes","date":"2017-03-03T10:54:58.000Z","updated":"2021-04-26T12:39:04.684Z","comments":true,"path":"2017/95270314185458/","link":"","permalink":"http://huermosi.xyz/2017/95270314185458/","excerpt":"","text":"三次握手和四次挥手是各个公司常见的考点，也具有一定的水平区分度，也被一些面试官作为热身题。很多小伙伴说这个问题刚开始回答的挺好，但是后面越回答越冒冷汗，最后就歇菜了。 见过比较典型的面试场景是这样的: 面试官：请介绍下三次握手求职者：第一次握手就是客户端给服务器端发送一个报文，第二次就是服务器收到报文之后，会应答一个报文给客户端，第三次握手就是客户端收到报文后再给服务器发送一个报文，三次握手就成功了。面试官：然后呢？求职者：这就是三次握手的过程，很简单的。面试官：。。。。。。（番外篇：一首凉凉送给你） 记住猿人谷一句话：面试时越简单的问题，一般就是隐藏着比较大的坑，一般都是需要将问题扩展的。上面求职者的回答不对吗？当然对，但距离面试官的期望可能还有点距离。 希望大家能带着如下问题进行阅读，收获会更大。 请画出三次握手和四次挥手的示意图 为什么连接的时候是三次握手？ 什么是半连接队列？ ISN(Initial Sequence Number)是固定的吗？ 三次握手过程中可以携带数据吗？ 如果第三次握手丢失了，客户端服务端会如何处理？ SYN攻击是什么？ 挥手为什么需要四次？ 四次挥手释放连接时，等待2MSL的意义? 1. 三次握手三次握手（Three-way Handshake）其实就是指建立一个TCP连接时，需要客户端和服务器总共发送3个包。进行三次握手的主要作用就是为了确认双方的接收能力和发送能力是否正常、指定自己的初始化序列号为后面的可靠性传送做准备。实质上其实就是连接服务器指定端口，建立TCP连接，并同步连接双方的序列号和确认号，交换TCP窗口大小信息。 刚开始客户端处于 Closed 的状态，服务端处于 Listen 状态。进行三次握手： 第一次握手：客户端给服务端发一个 SYN 报文，并指明客户端的初始化序列号 ISN。此时客户端处于 SYN_SENT 状态。首部的同步位SYN=1，初始序号seq=x，SYN=1的报文段不能携带数据，但要消耗掉一个序号。 第二次握手：服务器收到客户端的 SYN 报文之后，会以自己的 SYN 报文作为应答，并且也是指定了自己的初始化序列号 ISN(s)。同时会把客户端的 ISN + 1 作为ACK 的值，表示自己已经收到了客户端的 SYN，此时服务器处于 SYN_RCVD 的状态。在确认报文段中SYN=1，ACK=1，确认号ack=x+1，初始序号seq=y。 第三次握手：客户端收到 SYN 报文之后，会发送一个 ACK 报文，当然，也是一样把服务器的 ISN + 1 作为 ACK 的值，表示已经收到了服务端的 SYN 报文，此时客户端处于 ESTABLISHED 状态。服务器收到 ACK 报文之后，也处于 ESTABLISHED 状态，此时，双方已建立起了连接。确认报文段ACK=1，确认号ack=y+1，序号seq=x+1（初始为seq=x，第二个报文段所以要+1），ACK报文段可以携带数据，不携带数据则不消耗序号。 发送第一个SYN的一端将执行主动打开（active open），接收这个SYN并发回下一个SYN的另一端执行被动打开（passive open）。在socket编程中，客户端执行connect()时，将触发三次握手。 1.1 为什么需要三次握手，两次不行吗？弄清这个问题，我们需要先弄明白三次握手的目的是什么，能不能只用两次握手来达到同样的目的。 第一次握手：客户端发送网络包，服务端收到了。这样服务端就能得出结论：客户端的发送能力、服务端的接收能力是正常的。 第二次握手：服务端发包，客户端收到了。这样客户端就能得出结论：服务端的接收、发送能力，客户端的接收、发送能力是正常的。不过此时服务器并不能确认客户端的接收能力是否正常。 第三次握手：客户端发包，服务端收到了。这样服务端就能得出结论：客户端的接收、发送能力正常，服务器自己的发送、接收能力也正常。 因此，需要三次握手才能确认双方的接收与发送能力是否正常。试想如果是用两次握手，则会出现下面这种情况： 如客户端发出连接请求，但因连接请求报文丢失而未收到确认，于是客户端再重传一次连接请求。后来收到了确认，建立了连接。数据传输完毕后，就释放了连接，客户端共发出了两个连接请求报文段，其中第一个丢失，第二个到达了服务端，但是第一个丢失的报文段只是在某些网络结点长时间滞留了，延误到连接释放以后的某个时间才到达服务端，此时服务端误认为客户端又发出一次新的连接请求，于是就向客户端发出确认报文段，同意建立连接，不采用三次握手，只要服务端发出确认，就建立新的连接了，此时客户端忽略服务端发来的确认，也不发送数据，则服务端一致等待客户端发送数据，浪费资源。 1.2 什么是半连接队列？服务器第一次收到客户端的 SYN 之后，就会处于 SYN_RCVD 状态，此时双方还没有完全建立其连接，服务器会把此种状态下请求连接放在一个队列里，我们把这种队列称之为半连接队列。 当然还有一个全连接队列，就是已经完成三次握手，建立起连接的就会放在全连接队列中。如果队列满了就有可能会出现丢包现象。 这里在补充一点关于SYN-ACK 重传次数的问题：服务器发送完SYN-ACK包，如果未收到客户确认包，服务器进行首次重传，等待一段时间仍未收到客户确认包，进行第二次重传。如果重传次数超过系统规定的最大重传次数，系统将该连接信息从半连接队列中删除。注意，每次重传等待的时间不一定相同，一般会是指数增长，例如间隔时间为 1s，2s，4s，8s…… 1.3 ISN(Initial Sequence Number)是固定的吗？当一端为建立连接而发送它的SYN时，它为连接选择一个初始序号。ISN随时间而变化，因此每个连接都将具有不同的ISN。ISN可以看作是一个32比特的计数器，每4ms加1 。这样选择序号的目的在于防止在网络中被延迟的分组在以后又被传送，而导致某个连接的一方对它做错误的解释。 三次握手的其中一个重要功能是客户端和服务端交换 ISN(Initial Sequence Number)，以便让对方知道接下来接收数据的时候如何按序列号组装数据。如果 ISN 是固定的，攻击者很容易猜出后续的确认号，因此 ISN 是动态生成的。 1.4 三次握手过程中可以携带数据吗？其实第三次握手的时候，是可以携带数据的。但是，第一次、第二次握手不可以携带数据 为什么这样呢?大家可以想一个问题，假如第一次握手可以携带数据的话，如果有人要恶意攻击服务器，那他每次都在第一次握手中的 SYN 报文中放入大量的数据。因为攻击者根本就不理服务器的接收、发送能力是否正常，然后疯狂着重复发 SYN 报文的话，这会让服务器花费很多时间、内存空间来接收这些报文。 也就是说，第一次握手不可以放数据，其中一个简单的原因就是会让服务器更加容易受到攻击了。而对于第三次的话，此时客户端已经处于 ESTABLISHED 状态。对于客户端来说，他已经建立起连接了，并且也已经知道服务器的接收、发送能力是正常的了，所以能携带数据也没啥毛病。 1.5 SYN攻击是什么？服务器端的资源分配是在二次握手时分配的，而客户端的资源是在完成三次握手时分配的，所以服务器容易受到SYN洪泛攻击。SYN攻击就是Client在短时间内伪造大量不存在的IP地址，并向Server不断地发送SYN包，Server则回复确认包，并等待Client确认，由于源地址不存在，因此Server需要不断重发直至超时，这些伪造的SYN包将长时间占用未连接队列，导致正常的SYN请求因为队列满而被丢弃，从而引起网络拥塞甚至系统瘫痪。SYN 攻击是一种典型的 DoS/DDoS 攻击。 检测 SYN 攻击非常的方便，当你在服务器上看到大量的半连接状态时，特别是源IP地址是随机的，基本上可以断定这是一次SYN攻击。在 Linux/Unix 上可以使用系统自带的 netstats 命令来检测 SYN 攻击。 1netstat -n -p TCP | grep SYN_RECV 常见的防御 SYN 攻击的方法有如下几种： 缩短超时（SYN Timeout）时间 增加最大半连接数 过滤网关防护 SYN cookies技术 2. 四次挥手建立一个连接需要三次握手，而终止一个连接要经过四次挥手（也有将四次挥手叫做四次握手的）。这由TCP的半关闭（half-close）造成的。所谓的半关闭，其实就是TCP提供了连接的一端在结束它的发送后还能接收来自另一端数据的能力。 TCP 连接的拆除需要发送四个包，因此称为四次挥手(Four-way handshake)，客户端或服务端均可主动发起挥手动作。 刚开始双方都处于ESTABLISHED 状态，假如是客户端先发起关闭请求。四次挥手的过程如下： 第一次挥手：客户端发送一个 FIN 报文，报文中会指定一个序列号。此时客户端处于 FIN_WAIT1 状态。即发出连接释放报文段（FIN=1，序号seq=u），并停止再发送数据，主动关闭TCP连接，进入FIN_WAIT1（终止等待1）状态，等待服务端的确认。 第二次挥手：服务端收到 FIN 之后，会发送 ACK 报文，且把客户端的序列号值 +1 作为 ACK 报文的序列号值，表明已经收到客户端的报文了，此时服务端处于 CLOSE_WAIT 状态。即服务端收到连接释放报文段后即发出确认报文段（ACK=1，确认号ack=u+1，序号seq=v），服务端进入CLOSE_WAIT（关闭等待）状态，此时的TCP处于半关闭状态，客户端到服务端的连接释放。客户端收到服务端的确认后，进入FIN_WAIT2（终止等待2）状态，等待服务端发出的连接释放报文段。 第三次挥手：如果服务端也想断开连接了，和客户端的第一次挥手一样，发给 FIN 报文，且指定一个序列号。此时服务端处于 LAST_ACK 的状态。即服务端没有要向客户端发出的数据，服务端发出连接释放报文段（FIN=1，ACK=1，序号seq=w，确认号ack=u+1），服务端进入LAST_ACK（最后确认）状态，等待客户端的确认。 第四次挥手：客户端收到 FIN 之后，一样发送一个 ACK 报文作为应答，且把服务端的序列号值 +1 作为自己 ACK 报文的序列号值，此时客户端处于 TIME_WAIT 状态。需要过一阵子以确保服务端收到自己的 ACK 报文之后才会进入 CLOSED 状态，服务端收到 ACK 报文之后，就处于关闭连接了，处于 CLOSED 状态。即客户端收到服务端的连接释放报文段后，对此发出确认报文段（ACK=1，seq=u+1，ack=w+1），客户端进入TIME_WAIT（时间等待）状态。此时TCP未释放掉，需要经过时间等待计时器设置的时间2MSL后，客户端才进入CLOSED状态。 收到一个FIN只意味着在这一方向上没有数据流动。客户端执行主动关闭并进入TIME_WAIT是正常的，服务端通常执行被动关闭，不会进入TIME_WAIT状态。 在socket编程中，任何一方执行close()操作即可产生挥手操作。 2.1 挥手为什么需要四次？因为当服务端收到客户端的SYN连接请求报文后，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的。但是关闭连接时，当服务端收到FIN报文时，很可能并不会立即关闭SOCKET，所以只能先回复一个ACK报文，告诉客户端，”你发的FIN报文我收到了”。只有等到我服务端所有的报文都发送完了，我才能发送FIN报文，因此不能一起发送。故需要四次挥手。 2.2 2MSL等待状态TIME_WAIT状态也成为2MSL等待状态。每个具体TCP实现必须选择一个报文段最大生存时间MSL（Maximum Segment Lifetime），它是任何报文段被丢弃前在网络内的最长时间。这个时间是有限的，因为TCP报文段以IP数据报在网络内传输，而IP数据报则有限制其生存时间的TTL字段。 对一个具体实现所给定的MSL值，处理的原则是：当TCP执行一个主动关闭，并发回最后一个ACK，该连接必须在TIME_WAIT状态停留的时间为2倍的MSL。这样可让TCP再次发送最后的ACK以防这个ACK丢失（另一端超时并重发最后的FIN）。 这种2MSL等待的另一个结果是这个TCP连接在2MSL等待期间，定义这个连接的插口（客户的IP地址和端口号，服务器的IP地址和端口号）不能再被使用。这个连接只能在2MSL结束后才能再被使用。 2.3 四次挥手释放连接时，等待2MSL的意义? MSL是Maximum Segment Lifetime的英文缩写，可译为“最长报文段寿命”，它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。 为了保证客户端发送的最后一个ACK报文段能够到达服务器。因为这个ACK有可能丢失，从而导致处在LAST-ACK状态的服务器收不到对FIN-ACK的确认报文。服务器会超时重传这个FIN-ACK，接着客户端再重传一次确认，重新启动时间等待计时器。最后客户端和服务器都能正常的关闭。假设客户端不等待2MSL，而是在发送完ACK之后直接释放关闭，一但这个ACK丢失的话，服务器就无法正常的进入关闭连接状态。 两个理由： 保证客户端发送的最后一个ACK报文段能够到达服务端。这个ACK报文段有可能丢失，使得处于LAST-ACK状态的B收不到对已发送的FIN+ACK报文段的确认，服务端超时重传FIN+ACK报文段，而客户端能在2MSL时间内收到这个重传的FIN+ACK报文段，接着客户端重传一次确认，重新启动2MSL计时器，最后客户端和服务端都进入到CLOSED状态，若客户端在TIME-WAIT状态不等待一段时间，而是发送完ACK报文段后立即释放连接，则无法收到服务端重传的FIN+ACK报文段，所以不会再发送一次确认报文段，则服务端无法正常进入到CLOSED状态。 防止“已失效的连接请求报文段”出现在本连接中。客户端在发送完最后一个ACK报文段后，再经过2MSL，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失，使下一个新的连接中不会出现这种旧的连接请求报文段。 2.4 为什么TIME_WAIT状态需要经过2MSL才能返回到CLOSE状态？理论上，四个报文都发送完毕，就可以直接进入CLOSE状态了，但是可能网络是不可靠的，有可能最后一个ACK丢失。所以TIME_WAIT状态就是用来重发可能丢失的ACK报文。 3. 总结《TCP/IP详解 卷1:协议》有一张TCP状态变迁图，很具有代表性，有助于大家理解三次握手和四次挥手的状态变化。如下图所示，粗的实线箭头表示正常的客户端状态变迁，粗的虚线箭头表示正常的服务器状态变迁。 本文转载自猿人谷博客，原文连接点这里，版权归原作者所有。微信公众号：猿人谷如果您认为阅读这篇博客让您有些收获，不妨点击一下右下角的【推荐】如果您希望与我交流互动，欢迎关注微信公众号本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文连接。","categories":[{"name":"更多","slug":"更多","permalink":"http://huermosi.xyz/categories/%E6%9B%B4%E5%A4%9A/"},{"name":"TCP","slug":"更多/TCP","permalink":"http://huermosi.xyz/categories/%E6%9B%B4%E5%A4%9A/TCP/"}],"tags":[{"name":"TCP","slug":"TCP","permalink":"http://huermosi.xyz/tags/TCP/"},{"name":"三次握手","slug":"三次握手","permalink":"http://huermosi.xyz/tags/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B/"},{"name":"四次挥手","slug":"四次挥手","permalink":"http://huermosi.xyz/tags/%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B/"}]}]}